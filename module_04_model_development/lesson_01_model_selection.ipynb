{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 1: Model Selection Strategies\n",
                "\n",
                "**Module 4: Model Development & Optimization**  \n",
                "**Estimated Time**: 1-2 hours  \n",
                "**Difficulty**: Beginner-Intermediate\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "âœ… Understand how to systematically select the right model architecture  \n",
                "âœ… Learn the \"Occam's Razor\" of ML: Start Simple  \n",
                "âœ… Compare Linear vs. Tree-based vs. Deep Learning models  \n",
                "âœ… Master the trade-offs: Interpretability vs. Accuracy vs. Latency  \n",
                "âœ… Answer interview questions on model selection logic  \n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“š Table of Contents\n",
                "\n",
                "1. [The Hierarchy of Complexity](#1-hierarchy)\n",
                "2. [Trade-off Triangle: Speed, Accuracy, Interpretability](#2-tradeoffs)\n",
                "3. [Hands-On: Benchmarking Candidates](#3-hands-on)\n",
                "4. [Interview Preparation](#4-interview-questions)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Hierarchy of Complexity\n",
                "\n",
                "When faced with a new problem, **never** start with a Transformer or Deep Neural Network. Follow this hierarchy:\n",
                "\n",
                "### Level 1: The Baselines (Sanity Check)\n",
                "- **Mean/Mode**: Predicting the average value or most frequent class. \n",
                "- **Heuristics**: \"If price < $10, buy it\".\n",
                "- **Why?**: If your fancy model can't beat the average, it is useless.\n",
                "\n",
                "### Level 2: The Interpretable Models\n",
                "- **Linear/Logistic Regression**\n",
                "- **Decision Trees** (Depth < 5)\n",
                "- **Why?**: Business stakeholders often need to know *why* a decision was made. Linear weights give you exact feature impact.\n",
                "\n",
                "### Level 3: The Workhorses (Tabular SOTA)\n",
                "- **XGBoost / LightGBM / CatBoost**\n",
                "- **Random Forests**\n",
                "- **Why?**: For structured (tabular) data, Gradient Boosted Trees are typically State-of-the-Art (SOTA). They handle non-linearities and interactions well.\n",
                "\n",
                "### Level 4: The Deep Learners (Unstructured SOTA)\n",
                "- **ResNet / EfficientNet** (Images)\n",
                "- **BERT / GPT** (Text)\n",
                "- **Why?**: Only use these for images, audio, text, or when you have millions of tabular rows and need to squeeze out the last 0.1% accuracy."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Trade-off Triangle\n",
                "\n",
                "You can rarely optimize all three. Pick two:\n",
                "\n",
                "1. **Accuracy**: How well does it predict?\n",
                "2. **Latency (Speed)**: How fast does it predict? (Critical for real-time API)\n",
                "3. **Interpretability**: Can I explain it to a human?\n",
                "\n",
                "| Model | Accuracy | Latency | Interpretability |\n",
                "|-------|----------|---------|------------------|\n",
                "| Logistic Reg | Low | Very Low (<1ms) | High |\n",
                "| XGBoost | High | Low (~10ms) | Medium (SHAP) |\n",
                "| Transformer | Very High | High (>100ms) | Low |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Hands-On: Benchmarking Candidates\n",
                "\n",
                "Let's compare a Logistic Regression vs XGBoost on a classification problem."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "from sklearn.datasets import make_classification\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "# 1. Create Data (10k rows)\n",
                "X, y = make_classification(n_samples=10000, n_features=20, n_informative=10, random_state=42)\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
                "\n",
                "# 2. Candidate 1: Logistic Regression\n",
                "start = time.time()\n",
                "lr = LogisticRegression()\n",
                "lr.fit(X_train, y_train)\n",
                "train_time_lr = time.time() - start\n",
                "\n",
                "start = time.time()\n",
                "acc_lr = accuracy_score(y_test, lr.predict(X_test))\n",
                "infer_time_lr = (time.time() - start) / len(X_test) * 1000 # ms per sample\n",
                "\n",
                "# 3. Candidate 2: Random Forest\n",
                "start = time.time()\n",
                "rf = RandomForestClassifier(n_estimators=100)\n",
                "rf.fit(X_train, y_train)\n",
                "train_time_rf = time.time() - start\n",
                "\n",
                "start = time.time()\n",
                "acc_rf = accuracy_score(y_test, rf.predict(X_test))\n",
                "infer_time_rf = (time.time() - start) / len(X_test) * 1000 # ms per sample\n",
                "\n",
                "print(f\"{'Model':<15} {'Accuracy':<10} {'Train Time(s)':<15} {'Infer Latency(ms)':<20}\")\n",
                "print(\"-\"*60)\n",
                "print(f\"{'Logistic':<15} {acc_lr:<10.3f} {train_time_lr:<15.4f} {infer_time_lr:<20.4f}\")\n",
                "print(f\"{'RandomForest':<15} {acc_rf:<10.3f} {train_time_rf:<15.4f} {infer_time_rf:<20.4f}\")\n",
                "\n",
                "print(\"\\nDecision: Is the extra accuracy of RF worth the increased latency?\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Interview Preparation\n",
                "\n",
                "### Common Questions\n",
                "\n",
                "#### Q1: \"How do you choose a model for a new project?\"\n",
                "**Answer Framework**:\n",
                "1. **Baseline**: \"I always start with a heuristic or simple baseline (Logistic Regression) to establish a performance floor.\"\n",
                "2. **Constraint Check**: \"I check deployment constraints. Do we need <10ms latency? Does it need to run on a phone? This filters out massive models.\"\n",
                "3. **Complexity Ladder**: \"I then move to XGBoost. If that isn't enough, and I have unstructured data/massive scale, I consider Deep Learning.\"\n",
                "\n",
                "#### Q2: \"When would you choose a Decision Tree over a Neural Network?\"\n",
                "**Answer**: \"When Interpretability is key (e.g., Credit Scoring regulation requires explaining rejections), or when data is small/tabular. Neural Networks are overkill for small tabular data and are 'black boxes' by default.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}