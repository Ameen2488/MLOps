{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üèÜ Capstone Project: Production E-Commerce ML Data Pipeline\n",
                "\n",
                "**Module 3: Data & Pipeline Engineering ‚Äî Comprehensive Exercise**\n",
                "\n",
                "---\n",
                "\n",
                "## Project Overview\n",
                "\n",
                "You are a **Senior Data Scientist at ShopStream**, a mid-size e-commerce company.\n",
                "The business wants a **churn prediction model** that runs daily. Your job: build the\n",
                "**entire data pipeline** from raw multi-source data to model-ready features.\n",
                "\n",
                "### What You'll Build\n",
                "\n",
                "```\n",
                " STAGE 1        STAGE 2       STAGE 3        STAGE 4       STAGE 5       STAGE 6\n",
                " ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                " ‚îÇMulti- ‚îÇ‚ñ∂‚îÇValidate‚îÇ‚ñ∂‚îÇLeak-Free‚îÇ‚ñ∂‚îÇFeature ‚îÇ‚ñ∂‚îÇQuality ‚îÇ‚ñ∂‚îÇ Store ‚îÇ\n",
                " ‚îÇSource ‚îÇ  ‚îÇ& Clean ‚îÇ  ‚îÇSampling ‚îÇ  ‚îÇEngineer‚îÇ  ‚îÇ Check  ‚îÇ  ‚îÇParquet‚îÇ\n",
                " ‚îÇIngest ‚îÇ  ‚îÇ        ‚îÇ  ‚îÇ& Split  ‚îÇ  ‚îÇ        ‚îÇ  ‚îÇ        ‚îÇ  ‚îÇ       ‚îÇ\n",
                " ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                " L1: Sources  L2: ETL     L3: Sampling  L5: Feast   L4: Leakage  L1: Parquet\n",
                "              L7: E2E     L4: Leakage   L6: Spark   L7: E2E\n",
                "```\n",
                "\n",
                "### Concepts Exercised\n",
                "\n",
                "| Stage | Lessons Applied |\n",
                "|-------|----------------|\n",
                "| Multi-Source Ingest | L1 (Data Sources & Formats) |\n",
                "| Validate & Clean | L2 (ETL Pipelines), L7 (E2E Pipeline) |\n",
                "| Leak-Free Sampling | L3 (Sampling Strategies), L4 (Data Leakage) |\n",
                "| Feature Engineering | L5 (Feature Stores), L6 (Spark/PySpark) |\n",
                "| Quality Check | L4 (Leakage Detection), L7 (Validation) |\n",
                "| Store as Parquet | L1 (Formats), L7 (Production Storage) |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Setup complete\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# SETUP & IMPORTS\n",
                "# ============================================================\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os, time, json, warnings, shutil\n",
                "from datetime import datetime, timedelta\n",
                "from dataclasses import dataclass, field\n",
                "from typing import Dict, List, Optional, Tuple\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
                "warnings.filterwarnings('ignore')\n",
                "np.random.seed(42)\n",
                "\n",
                "# Pipeline output directory\n",
                "OUTPUT_DIR = 'pipeline_output'\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "print(\"‚úÖ Setup complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Stage 1: Multi-Source Data Ingestion\n",
                "\n",
                "**üìù Concept Revision (Lesson 1 & 2):**\n",
                "\n",
                "In production ML systems, data never comes from a single clean CSV. You're pulling from\n",
                "multiple sources with wildly different reliability, formats, and schemas. Understanding\n",
                "the data source taxonomy is critical for designing robust pipelines:\n",
                "\n",
                "```\n",
                "  DATA SOURCE RELIABILITY SPECTRUM:\n",
                "\n",
                "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "  ‚îÇ Databases   ‚îÇ  ‚îÇ Internal    ‚îÇ  ‚îÇ APIs        ‚îÇ  ‚îÇ Logs/Events ‚îÇ\n",
                "  ‚îÇ (Postgres)  ‚îÇ  ‚îÇ Services    ‚îÇ  ‚îÇ (3rd party) ‚îÇ  ‚îÇ (Clickstr.) ‚îÇ\n",
                "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "  Most reliable ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∑ Least reliable\n",
                "  Structured         Semi-struct.    Variable        Noisy\n",
                "  Schema enforced    Rate-limited    May change      Duplicates\n",
                "```\n",
                "\n",
                "**Key principles:**\n",
                "- **Schema-on-write** (databases): Schema is enforced when data is written. Most reliable.\n",
                "- **Schema-on-read** (logs, APIs): You define schema at read time. Risky ‚Äî schema can change without warning.\n",
                "- Always assume the worst: nulls, duplicates, wrong types, outliers, late-arriving data.\n",
                "- Design your ingestion to handle ALL these issues gracefully.\n",
                "\n",
                "**Data Format Choice matters hugely:** CSV is human-readable but slow and schema-less.\n",
                "Parquet is binary, columnar, compressed, and embeds schema ‚Äî the gold standard for ML workloads.\n",
                "We'll demonstrate the size difference at the end of this pipeline.\n",
                "\n",
                "We simulate 4 realistic sources below: **transactions** (API ‚Äî semi-structured, unreliable),\n",
                "**user profiles** (DB ‚Äî structured, reliable), **product catalog** (internal DB), and\n",
                "**web sessions** (logs ‚Äî noisy with outliers).\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'pd' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# SOURCE 1: Transaction API (semi-structured, unreliable)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m      4\u001b[39m n_txn = \u001b[32m200_000\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m dates = \u001b[43mpd\u001b[49m.date_range(\u001b[33m'\u001b[39m\u001b[33m2023-06-01\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m2024-03-31\u001b[39m\u001b[33m'\u001b[39m, periods=n_txn)\n\u001b[32m      7\u001b[39m transactions = pd.DataFrame({\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtxn_id\u001b[39m\u001b[33m'\u001b[39m: np.arange(n_txn),\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m: np.random.randint(\u001b[32m1\u001b[39m, \u001b[32m10001\u001b[39m, n_txn),\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m: dates,\n\u001b[32m     16\u001b[39m })\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Inject realistic issues: nulls, negatives, duplicates\u001b[39;00m\n",
                        "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# SOURCE 1: Transaction API (semi-structured, unreliable)\n",
                "# ============================================================\n",
                "n_txn = 200_000\n",
                "dates = pd.date_range('2023-06-01', '2024-03-31', periods=n_txn)\n",
                "\n",
                "transactions = pd.DataFrame({\n",
                "    'txn_id': np.arange(n_txn),\n",
                "    'user_id': np.random.randint(1, 10001, n_txn),\n",
                "    'product_id': np.random.randint(1, 2001, n_txn),\n",
                "    'amount': np.round(np.random.exponential(45, n_txn), 2),\n",
                "    'quantity': np.random.randint(1, 8, n_txn),\n",
                "    'payment_method': np.random.choice(['credit_card', 'debit_card', 'upi', 'wallet', 'cod'], n_txn,\n",
                "                                       p=[0.35, 0.25, 0.20, 0.10, 0.10]),\n",
                "    'timestamp': dates,\n",
                "})\n",
                "\n",
                "# Inject realistic issues: nulls, negatives, duplicates\n",
                "transactions.loc[np.random.choice(n_txn, 3000, replace=False), 'amount'] = np.nan\n",
                "transactions.loc[np.random.choice(n_txn, 500, replace=False), 'amount'] *= -1\n",
                "duplicates = transactions.sample(500, random_state=42)\n",
                "transactions = pd.concat([transactions, duplicates]).reset_index(drop=True)\n",
                "\n",
                "print(f\"‚úÖ Source 1 - Transactions: {len(transactions):,} rows | Issues: nulls, negatives, dupes\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# SOURCE 2: User Profiles DB (structured, reliable)\n",
                "# ============================================================\n",
                "n_users = 10_000\n",
                "user_profiles = pd.DataFrame({\n",
                "    'user_id': np.arange(1, n_users + 1),\n",
                "    'signup_date': pd.date_range('2020-01-01', '2024-01-01', periods=n_users),\n",
                "    'age': np.random.randint(18, 72, n_users),\n",
                "    'gender': np.random.choice(['M', 'F', 'Other'], n_users, p=[0.48, 0.48, 0.04]),\n",
                "    'region': np.random.choice(['North', 'South', 'East', 'West', 'Central'], n_users),\n",
                "    'is_premium': np.random.choice([0, 1], n_users, p=[0.75, 0.25]),\n",
                "})\n",
                "\n",
                "# Generate churn labels: ~12% churn rate (imbalanced!)\n",
                "churn_prob = 0.05 + 0.15 * (user_profiles['age'] > 50).astype(float) + \\\n",
                "             0.10 * (1 - user_profiles['is_premium']) + \\\n",
                "             np.random.uniform(-0.05, 0.05, n_users)\n",
                "user_profiles['churned'] = (np.random.random(n_users) < churn_prob.clip(0, 0.5)).astype(int)\n",
                "\n",
                "print(f\"‚úÖ Source 2 - User Profiles: {len(user_profiles):,} rows | Churn rate: {user_profiles['churned'].mean():.1%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# SOURCE 3: Product Catalog (internal DB)\n",
                "# ============================================================\n",
                "n_products = 2000\n",
                "product_catalog = pd.DataFrame({\n",
                "    'product_id': np.arange(1, n_products + 1),\n",
                "    'category': np.random.choice(['Electronics', 'Clothing', 'Home', 'Books', 'Sports',\n",
                "                                  'Beauty', 'Food', 'Toys'], n_products),\n",
                "    'base_price': np.round(np.random.uniform(5, 500, n_products), 2),\n",
                "    'avg_rating': np.round(np.random.uniform(1.5, 5.0, n_products), 1),\n",
                "})\n",
                "\n",
                "print(f\"‚úÖ Source 3 - Product Catalog: {len(product_catalog):,} rows\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# SOURCE 4: Web Session Logs (semi-structured, noisy)\n",
                "# ============================================================\n",
                "n_sessions = 500_000\n",
                "sessions = pd.DataFrame({\n",
                "    'session_id': np.arange(n_sessions),\n",
                "    'user_id': np.random.randint(1, n_users + 1, n_sessions),\n",
                "    'timestamp': pd.date_range('2023-06-01', '2024-03-31', periods=n_sessions),\n",
                "    'pages_viewed': np.random.poisson(5, n_sessions),\n",
                "    'time_on_site_sec': np.random.exponential(180, n_sessions).astype(int),\n",
                "    'device': np.random.choice(['mobile', 'desktop', 'tablet'], n_sessions, p=[0.55, 0.35, 0.10]),\n",
                "    'bounce': np.random.choice([0, 1], n_sessions, p=[0.65, 0.35]),\n",
                "})\n",
                "\n",
                "# Inject noise: some extreme outliers\n",
                "sessions.loc[np.random.choice(n_sessions, 200, replace=False), 'time_on_site_sec'] = \\\n",
                "    np.random.randint(50000, 200000, 200)\n",
                "\n",
                "print(f\"‚úÖ Source 4 - Web Sessions: {len(sessions):,} rows | With outliers injected\")\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"INGESTION SUMMARY\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"  Total data points: {len(transactions)+len(user_profiles)+len(product_catalog)+len(sessions):,}\")\n",
                "print(f\"  Sources: 4 (API, DB, Internal, Logs)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Stage 2: Validation & Cleaning (ETL)\n",
                "\n",
                "**üìù Concept Revision (Lesson 2 & 7):**\n",
                "\n",
                "The ETL (Extract-Transform-Load) pattern is the backbone of every data pipeline.\n",
                "We've already Extracted (Stage 1). Now we Transform (validate + clean). Load comes later.\n",
                "\n",
                "**The cardinal rule: NEVER trust raw data.** Even data from \"reliable\" databases can have:\n",
                "- **Schema changes**: A column was renamed upstream and no one told you\n",
                "- **Null spikes**: A service outage caused 50% of rows to miss a field\n",
                "- **Duplicates**: Retry logic in the API created double-writes\n",
                "- **Out-of-range values**: Negative prices from refunds mixed with sales\n",
                "\n",
                "```\n",
                "  VALIDATION LAYERS (defense in depth):\n",
                "\n",
                "  Layer 1: SCHEMA       ‚Üí  Are the right columns present with correct types?\n",
                "  Layer 2: NULL CHECK   ‚Üí  Are null rates within acceptable thresholds?\n",
                "  Layer 3: RANGE CHECK  ‚Üí  Are values within business-valid ranges?\n",
                "  Layer 4: UNIQUENESS   ‚Üí  Are primary keys actually unique?\n",
                "  Layer 5: STATISTICAL  ‚Üí  Do distributions match historical baselines?\n",
                "\n",
                "  Each layer catches different failure modes.\n",
                "  Validate BEFORE processing to fail fast (don't waste compute on bad data).\n",
                "```\n",
                "\n",
                "**ETL vs ELT:** In traditional ETL, you transform before loading. In ELT (used with\n",
                "cloud data warehouses like BigQuery/Snowflake), you load raw data first, then transform\n",
                "in-place using SQL. The validation step is critical in BOTH patterns.\n",
                "\n",
                "The **DataValidator pattern** below is a production-grade pattern used at companies like\n",
                "Airbnb and Uber. It's essentially a lightweight version of Great Expectations ‚Äî the\n",
                "industry-standard data validation library.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# PRODUCTION DATA VALIDATOR\n",
                "# ============================================================\n",
                "@dataclass\n",
                "class ValidationResult:\n",
                "    check: str\n",
                "    passed: bool\n",
                "    detail: str\n",
                "    severity: str  # 'error', 'warning'\n",
                "\n",
                "class DataValidator:\n",
                "    def __init__(self):\n",
                "        self.results: List[ValidationResult] = []\n",
                "    \n",
                "    def check_schema(self, df, expected_cols):\n",
                "        missing = set(expected_cols) - set(df.columns)\n",
                "        self.results.append(ValidationResult(\n",
                "            'schema', len(missing)==0,\n",
                "            f'Missing: {missing}' if missing else 'OK', 'error' if missing else 'info'))\n",
                "    \n",
                "    def check_nulls(self, df, max_pct=0.05):\n",
                "        violations = {c: f'{v:.1%}' for c, v in df.isnull().mean().items() if v > max_pct}\n",
                "        self.results.append(ValidationResult(\n",
                "            'null_rate', len(violations)==0,\n",
                "            f'High nulls: {violations}' if violations else f'All <{max_pct:.0%}', \n",
                "            'error' if violations else 'info'))\n",
                "    \n",
                "    def check_duplicates(self, df, key_col):\n",
                "        n_dupes = df[key_col].duplicated().sum()\n",
                "        self.results.append(ValidationResult(\n",
                "            f'duplicates({key_col})', n_dupes==0,\n",
                "            f'{n_dupes} duplicates' if n_dupes else 'No duplicates',\n",
                "            'error' if n_dupes else 'info'))\n",
                "    \n",
                "    def check_range(self, df, col, lo, hi):\n",
                "        oob = ((df[col].dropna() < lo) | (df[col].dropna() > hi)).sum()\n",
                "        self.results.append(ValidationResult(\n",
                "            f'range({col})', oob==0,\n",
                "            f'{oob} out of [{lo},{hi}]' if oob else 'In range',\n",
                "            'warning' if oob else 'info'))\n",
                "    \n",
                "    def report(self):\n",
                "        print(f\"{'='*55}\")\n",
                "        print(f\"VALIDATION REPORT  ({datetime.now().strftime('%H:%M:%S')})\")\n",
                "        print(f\"{'='*55}\")\n",
                "        for r in self.results:\n",
                "            icon = '‚úÖ' if r.passed else ('‚ö†Ô∏è' if r.severity=='warning' else '‚ùå')\n",
                "            print(f\"  {icon} {r.check}: {r.detail}\")\n",
                "        ok = all(r.passed or r.severity!='error' for r in self.results)\n",
                "        print(f\"\\n  {'PASSED' if ok else 'FAILED'}\")\n",
                "        self.results = []\n",
                "        return ok\n",
                "\n",
                "print(\"‚úÖ DataValidator defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# VALIDATE RAW TRANSACTIONS\n",
                "# ============================================================\n",
                "v = DataValidator()\n",
                "v.check_schema(transactions, ['txn_id','user_id','product_id','amount','quantity','payment_method','timestamp'])\n",
                "v.check_nulls(transactions)\n",
                "v.check_duplicates(transactions, 'txn_id')\n",
                "v.check_range(transactions, 'amount', 0.01, 5000)\n",
                "v.check_range(transactions, 'quantity', 1, 50)\n",
                "v.report()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CLEAN TRANSACTIONS\n",
                "# ============================================================\n",
                "txn = transactions.copy()\n",
                "before = len(txn)\n",
                "\n",
                "# 1. Deduplicate\n",
                "txn = txn.drop_duplicates(subset='txn_id', keep='first')\n",
                "print(f\"  Deduped: {before - len(txn)} removed\")\n",
                "\n",
                "# 2. Fix negative amounts\n",
                "neg = (txn['amount'] < 0).sum()\n",
                "txn['amount'] = txn['amount'].abs()\n",
                "print(f\"  Fixed {neg} negative amounts\")\n",
                "\n",
                "# 3. Impute nulls with category-aware median (join product info first)\n",
                "txn = txn.merge(product_catalog[['product_id','category']], on='product_id', how='left')\n",
                "null_ct = txn['amount'].isna().sum()\n",
                "txn['amount'] = txn.groupby('category')['amount'].transform(lambda x: x.fillna(x.median()))\n",
                "txn['amount'] = txn['amount'].fillna(txn['amount'].median())  # fallback\n",
                "print(f\"  Imputed {null_ct} null amounts (category-median)\")\n",
                "\n",
                "# 4. Cap outliers at 99th percentile\n",
                "cap = txn['amount'].quantile(0.99)\n",
                "capped = (txn['amount'] > cap).sum()\n",
                "txn['amount'] = txn['amount'].clip(upper=cap)\n",
                "print(f\"  Capped {capped} amounts at p99={cap:.2f}\")\n",
                "\n",
                "# 5. Add derived columns\n",
                "txn['total_value'] = txn['amount'] * txn['quantity']\n",
                "txn['hour'] = txn['timestamp'].dt.hour\n",
                "txn['dow'] = txn['timestamp'].dt.dayofweek\n",
                "txn['month'] = txn['timestamp'].dt.month\n",
                "\n",
                "print(f\"\\n‚úÖ Clean transactions: {len(txn):,} rows, {len(txn.columns)} cols\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CLEAN WEB SESSIONS (cap outlier durations)\n",
                "# ============================================================\n",
                "sess = sessions.copy()\n",
                "p99 = sess['time_on_site_sec'].quantile(0.99)\n",
                "outliers = (sess['time_on_site_sec'] > p99).sum()\n",
                "sess['time_on_site_sec'] = sess['time_on_site_sec'].clip(upper=p99)\n",
                "print(f\"‚úÖ Clean sessions: capped {outliers} outliers at {p99:.0f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Stage 3: Leak-Free Sampling & Temporal Split\n",
                "\n",
                "**üìù Concept Revision (Lesson 3 & 4):**\n",
                "\n",
                "This is where most ML projects silently fail. **Data leakage** is the #1 reason models\n",
                "look amazing in development and crash in production.\n",
                "\n",
                "**Three types of leakage to watch for:**\n",
                "1. **Target leakage**: A feature that directly encodes the label (e.g., 'cancellation_date' for churn)\n",
                "2. **Train-test contamination**: Information from test set leaks into training (e.g., fitting\n",
                "   a scaler on ALL data before splitting)\n",
                "3. **Temporal leakage**: Using future data to predict the past (most insidious with time-series)\n",
                "\n",
                "```\n",
                "  WRONG (random split ‚Äî temporal leakage!):\n",
                "  Jun 2023  Jul  Aug  Sep  Oct  Nov  Dec  Jan 2024  Feb  Mar\n",
                "    [T]  [V]  [T]  [T]  [V]  [T]  [V]  [T]     [V]  [T]\n",
                "    Future data leaks into training! ‚ùå\n",
                "\n",
                "  CORRECT (temporal split ‚Äî no leakage!):\n",
                "  Jun 2023  Jul  Aug  Sep  Oct  Nov  Dec  Jan 2024 | Feb  Mar\n",
                "    [T]  [T]  [T]  [T]  [T]  [T]  [T]  [T]      | [V]  [V]\n",
                "    Only past data in training ‚úÖ                   | Test\n",
                "```\n",
                "\n",
                "**Class imbalance** (Lesson 3): With ~12% churn rate, a naive model predicting \"not churned\" for\n",
                "everyone gets 88% accuracy! That's useless. We handle this with:\n",
                "- `class_weight='balanced'` in the model (adjusts loss function to penalize misses on minority class)\n",
                "- Stratified sampling to preserve class ratios in splits\n",
                "- SMOTE (Synthetic Minority Oversampling) ‚Äî but ONLY on training data, never before splitting!\n",
                "\n",
                "**Why temporal > random for this problem:** Customer behavior is time-dependent. Spending patterns\n",
                "in December (holidays) differ from January. A random split would let December test data inform\n",
                "January training data, creating an unrealistic advantage.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# TEMPORAL SPLIT: train on past, test on future\n",
                "# This prevents temporal leakage (Lesson 4)\n",
                "# ============================================================\n",
                "SPLIT_DATE = pd.Timestamp('2024-02-01')\n",
                "\n",
                "txn_train = txn[txn['timestamp'] < SPLIT_DATE]\n",
                "txn_test = txn[txn['timestamp'] >= SPLIT_DATE]\n",
                "sess_train = sess[sess['timestamp'] < SPLIT_DATE]\n",
                "sess_test = sess[sess['timestamp'] >= SPLIT_DATE]\n",
                "\n",
                "print(f\"Temporal Split at {SPLIT_DATE.date()}:\")\n",
                "print(f\"  Train transactions: {len(txn_train):,}  ({txn_train['timestamp'].min().date()} to {txn_train['timestamp'].max().date()})\")\n",
                "print(f\"  Test  transactions: {len(txn_test):,}  ({txn_test['timestamp'].min().date()} to {txn_test['timestamp'].max().date()})\")\n",
                "print(f\"\\n>>> No future data leaks into training features! (Lesson 4: Temporal Leakage Prevention)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Stage 4: Feature Engineering\n",
                "\n",
                "**üìù Concept Revision (Lesson 5 & 6):**\n",
                "\n",
                "Feature engineering is where the ML value is actually created. Raw data is useless to a model ‚Äî\n",
                "the features you compute from it determine model performance more than algorithm choice.\n",
                "\n",
                "**The Training-Serving Skew Problem (Lesson 5):**\n",
                "If you compute features differently during training vs serving, the model gets confused.\n",
                "Example: during training you compute `avg_order_value` over all historical data, but during\n",
                "serving you only use the last 30 days. The distributions differ ‚Üí model degrades.\n",
                "\n",
                "This is why **feature stores** (like Feast) exist ‚Äî they serve the SAME feature computation\n",
                "logic for both training and inference. In this project, we simulate this by using a\n",
                "**single reusable function** (`build_user_features`) for both train and test.\n",
                "\n",
                "```\n",
                "  FEATURE STORE PATTERN (what we're implementing):\n",
                "\n",
                "  build_user_features()  ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚ñ∂  Training Features  ‚îÄ‚îÄ‚ñ∂ Model.fit()\n",
                "  (single function)        ‚îÇ\n",
                "  Same logic for both!     ‚îî‚îÄ‚îÄ‚ñ∂  Serving Features   ‚îÄ‚îÄ‚ñ∂ Model.predict()\n",
                "                                No skew! ‚úÖ\n",
                "```\n",
                "\n",
                "**Performance (Lesson 6):** We use vectorized Pandas operations (groupby + agg) instead of\n",
                "row-by-row iteration. In Spark, these same operations would distribute across a cluster\n",
                "for TB-scale data. The pattern is identical: think in columns, not rows.\n",
                "\n",
                "**RFM Features:** Recency (days since last purchase), Frequency (purchases per day),\n",
                "Monetary (total spend) ‚Äî the classic customer behavior feature set used by Netflix, Amazon,\n",
                "Spotify, etc. These three alone are often the most predictive features for churn.\n",
                "\n",
                "**Feature categories we build:**\n",
                "- **Transaction features**: spend, frequency, recency, product diversity\n",
                "- **Session features**: engagement, bounce rate, device preference\n",
                "- **Profile features**: demographics, account age, premium status\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# FEATURE ENGINEERING FUNCTION (reusable for train AND test)\n",
                "# This is the \"feature definition\" that a Feature Store would manage\n",
                "# ============================================================\n",
                "def build_user_features(txn_df, sess_df, user_df, product_df, label='_train'):\n",
                "    \"\"\"Build user-level features from transaction and session data.\n",
                "    Uses only vectorized operations (Lesson 6: Spark-style thinking).\n",
                "    \"\"\"\n",
                "    start = time.time()\n",
                "    \n",
                "    # --- Transaction features ---\n",
                "    txn_feats = txn_df.groupby('user_id').agg(\n",
                "        total_transactions=('txn_id', 'count'),\n",
                "        total_spend=('total_value', 'sum'),\n",
                "        avg_order_value=('amount', 'mean'),\n",
                "        median_order_value=('amount', 'median'),\n",
                "        max_order_value=('amount', 'max'),\n",
                "        std_order_value=('amount', 'std'),\n",
                "        avg_quantity=('quantity', 'mean'),\n",
                "        unique_products=('product_id', 'nunique'),\n",
                "        unique_categories=('category', 'nunique'),\n",
                "        unique_payment_methods=('payment_method', 'nunique'),\n",
                "        pct_weekend=('dow', lambda x: (x >= 5).mean()),\n",
                "        pct_evening=('hour', lambda x: ((x >= 18) & (x <= 23)).mean()),\n",
                "        first_txn=('timestamp', 'min'),\n",
                "        last_txn=('timestamp', 'max'),\n",
                "    ).reset_index()\n",
                "    \n",
                "    # Recency & frequency (RFM-style)\n",
                "    ref_date = txn_df['timestamp'].max()\n",
                "    txn_feats['days_since_last_txn'] = (ref_date - txn_feats['last_txn']).dt.days\n",
                "    txn_feats['customer_tenure_days'] = (txn_feats['last_txn'] - txn_feats['first_txn']).dt.days + 1\n",
                "    txn_feats['purchase_frequency'] = txn_feats['total_transactions'] / txn_feats['customer_tenure_days'].clip(lower=1)\n",
                "    txn_feats = txn_feats.drop(columns=['first_txn', 'last_txn'])\n",
                "    txn_feats['std_order_value'] = txn_feats['std_order_value'].fillna(0)\n",
                "    \n",
                "    # Favorite category\n",
                "    fav_cat = txn_df.groupby('user_id')['category'].agg(lambda x: x.mode().iloc[0]).reset_index()\n",
                "    fav_cat.columns = ['user_id', 'fav_category']\n",
                "    txn_feats = txn_feats.merge(fav_cat, on='user_id', how='left')\n",
                "    \n",
                "    # --- Session features ---\n",
                "    sess_feats = sess_df.groupby('user_id').agg(\n",
                "        total_sessions=('session_id', 'count'),\n",
                "        avg_pages_viewed=('pages_viewed', 'mean'),\n",
                "        avg_time_on_site=('time_on_site_sec', 'mean'),\n",
                "        total_time_on_site=('time_on_site_sec', 'sum'),\n",
                "        bounce_rate=('bounce', 'mean'),\n",
                "        pct_mobile=('device', lambda x: (x == 'mobile').mean()),\n",
                "    ).reset_index()\n",
                "    \n",
                "    # --- Merge all features ---\n",
                "    features = user_df[['user_id', 'age', 'gender', 'region', 'is_premium', 'churned']].copy()\n",
                "    features['account_age_days'] = (ref_date - user_df['signup_date']).dt.days\n",
                "    features = features.merge(txn_feats, on='user_id', how='left')\n",
                "    features = features.merge(sess_feats, on='user_id', how='left')\n",
                "    \n",
                "    # Fill NaN for users with no transactions/sessions\n",
                "    numeric_cols = features.select_dtypes(include=[np.number]).columns\n",
                "    features[numeric_cols] = features[numeric_cols].fillna(0)\n",
                "    \n",
                "    elapsed = time.time() - start\n",
                "    print(f\"  ‚úÖ Built {len(features.columns)-2} features for {len(features):,} users ({elapsed:.2f}s) [{label}]\")\n",
                "    return features\n",
                "\n",
                "print(\"‚úÖ Feature engineering function defined (reusable for train/test)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# BUILD FEATURES (separately for train and test to prevent leakage!)\n",
                "# ============================================================\n",
                "print(\"Building features...\")\n",
                "train_features = build_user_features(txn_train, sess_train, user_profiles, product_catalog, 'TRAIN')\n",
                "test_features = build_user_features(txn_test, sess_test, user_profiles, product_catalog, 'TEST')\n",
                "\n",
                "print(f\"\\n  Train features shape: {train_features.shape}\")\n",
                "print(f\"  Test features shape:  {test_features.shape}\")\n",
                "print(f\"  Train churn rate:     {train_features['churned'].mean():.1%}\")\n",
                "print(f\"  Test churn rate:      {test_features['churned'].mean():.1%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Stage 5: Leakage Detection & Quality Check\n",
                "\n",
                "**üìù Concept Revision (Lesson 4 & 7):**\n",
                "\n",
                "Even after careful pipeline design, you need to **verify** that no leakage slipped through.\n",
                "This is your last line of defense before the model sees the data.\n",
                "\n",
                "**Leakage Detection Checklist:**\n",
                "\n",
                "```\n",
                "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "  ‚îÇ              LEAKAGE RED FLAGS                   ‚îÇ\n",
                "  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
                "  ‚îÇ üö© Any feature with >0.9 corr to target        ‚îÇ\n",
                "  ‚îÇ üö© Accuracy >95% on first try (too good!)      ‚îÇ\n",
                "  ‚îÇ üö© Single feature dominates importance (>50%)   ‚îÇ\n",
                "  ‚îÇ üö© Performance drops hugely in production       ‚îÇ\n",
                "  ‚îÇ üö© Future timestamps in feature columns         ‚îÇ\n",
                "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "```\n",
                "\n",
                "We run two checks here:\n",
                "1. **Correlation analysis**: Compute correlation of every feature with the target. Anything >0.9\n",
                "   is almost certainly leakage (no real-world feature predicts churn that perfectly).\n",
                "2. **Output quality validation**: Even if there's no leakage, are the features valid? No nulls?\n",
                "   Values in reasonable ranges? User IDs unique? This is the \"quality gate\" from Lesson 7.\n",
                "\n",
                "**Post-model sanity check:** After training, examine feature importances. If a single feature\n",
                "dominates (>50% importance), it's likely leakage. Real-world churn prediction should rely on\n",
                "a mix of behavioral, demographic, and engagement signals.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# LEAKAGE DETECTION (Lesson 4)\n",
                "# ============================================================\n",
                "print(\"LEAKAGE DETECTION\")\n",
                "print(\"=\"*55)\n",
                "\n",
                "numeric_feats = train_features.select_dtypes(include=[np.number]).columns.drop('churned')\n",
                "correlations = train_features[numeric_feats].corrwith(train_features['churned']).abs().sort_values(ascending=False)\n",
                "\n",
                "print(\"\\nTop 10 feature-target correlations:\")\n",
                "for feat, corr in correlations.head(10).items():\n",
                "    flag = ' üö© SUSPICIOUS!' if corr > 0.5 else ''\n",
                "    print(f\"  {feat:<30} {corr:.3f}{flag}\")\n",
                "\n",
                "leaky = correlations[correlations > 0.9]\n",
                "if len(leaky) > 0:\n",
                "    print(f\"\\n‚ùå LEAKAGE DETECTED in: {list(leaky.index)}\")\n",
                "    print(\"  These features likely encode the target. REMOVE THEM.\")\n",
                "else:\n",
                "    print(f\"\\n‚úÖ No target leakage detected (all correlations < 0.9)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# OUTPUT QUALITY VALIDATION (Lesson 7)\n",
                "# ============================================================\n",
                "v2 = DataValidator()\n",
                "v2.check_nulls(train_features, max_pct=0.0)\n",
                "v2.check_range(train_features, 'total_spend', 0, 500_000)\n",
                "v2.check_range(train_features, 'age', 18, 100)\n",
                "v2.check_range(train_features, 'bounce_rate', 0, 1)\n",
                "v2.check_duplicates(train_features, 'user_id')\n",
                "v2.report()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Stage 6: Model-Ready Pipeline & Storage\n",
                "\n",
                "**üìù Concept Revision (Lesson 1, 3, 4, 7):**\n",
                "\n",
                "**Why sklearn.Pipeline matters (Lesson 4):**\n",
                "The most common source of train-test contamination is fitting a scaler or imputer on the\n",
                "full dataset before splitting. `sklearn.Pipeline` solves this by guaranteeing that `fit()`\n",
                "only touches training data, and `transform()` is applied consistently to test data.\n",
                "\n",
                "```\n",
                "  WRONG (leaky):                    RIGHT (Pipeline):\n",
                "  scaler.fit(ALL_DATA)               pipeline.fit(X_train)\n",
                "  X_train = scaler.transform(train)   ‚îú‚îÄ imputer.fit(X_train)\n",
                "  X_test = scaler.transform(test)     ‚îú‚îÄ scaler.fit(X_train)\n",
                "  ‚ùå test stats leaked into scaler     ‚îú‚îÄ model.fit(X_train)\n",
                "                                     pipeline.predict(X_test)\n",
                "                                     ‚îî‚îÄ uses train-fitted transforms ‚úÖ\n",
                "```\n",
                "\n",
                "**Why Parquet for storage (Lesson 1):**\n",
                "- **Columnar format**: Read only the columns you need (column pruning)\n",
                "- **Schema embedded**: No guessing types, no silent type coercion\n",
                "- **Compression**: Snappy compression gives 3-8x size reduction over CSV\n",
                "- **Predicate pushdown**: Spark/Presto can filter without reading entire files\n",
                "\n",
                "**Class imbalance (Lesson 3):**\n",
                "`class_weight='balanced'` multiplies the loss for minority class samples by\n",
                "`n_samples / (n_classes * n_samples_in_class)`. With 12% churn, the churned class gets\n",
                "~7.3x weight. This forces the model to pay equal attention to both classes instead of\n",
                "learning the lazy \"predict majority class\" shortcut.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# PREPARE FEATURES FOR MODELING\n",
                "# ============================================================\n",
                "# Encode categoricals\n",
                "cat_cols = ['gender', 'region', 'fav_category']\n",
                "\n",
                "# One-hot encode (fit on train, transform both)\n",
                "train_encoded = pd.get_dummies(train_features, columns=cat_cols, drop_first=True)\n",
                "test_encoded = pd.get_dummies(test_features, columns=cat_cols, drop_first=True)\n",
                "\n",
                "# Align columns (test might have missing categories)\n",
                "train_cols = set(train_encoded.columns)\n",
                "test_cols = set(test_encoded.columns)\n",
                "for col in train_cols - test_cols:\n",
                "    test_encoded[col] = 0\n",
                "test_encoded = test_encoded[train_encoded.columns]\n",
                "\n",
                "# Separate X, y\n",
                "drop_cols = ['user_id', 'churned']\n",
                "X_train = train_encoded.drop(columns=drop_cols)\n",
                "y_train = train_encoded['churned']\n",
                "X_test = test_encoded.drop(columns=drop_cols)\n",
                "y_test = test_encoded['churned']\n",
                "\n",
                "print(f\"X_train: {X_train.shape} | X_test: {X_test.shape}\")\n",
                "print(f\"y_train churn rate: {y_train.mean():.1%} | y_test churn rate: {y_test.mean():.1%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# LEAK-FREE SKLEARN PIPELINE (Lesson 4)\n",
                "# fit() happens ONLY on X_train\n",
                "# ============================================================\n",
                "pipeline = Pipeline([\n",
                "    ('imputer', SimpleImputer(strategy='median')),\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('model', RandomForestClassifier(\n",
                "        n_estimators=200,\n",
                "        max_depth=12,\n",
                "        class_weight='balanced',  # Lesson 3: handle imbalance\n",
                "        random_state=42,\n",
                "        n_jobs=-1\n",
                "    ))\n",
                "])\n",
                "\n",
                "# Train\n",
                "start = time.time()\n",
                "pipeline.fit(X_train, y_train)\n",
                "train_time = time.time() - start\n",
                "\n",
                "# Predict\n",
                "y_pred = pipeline.predict(X_test)\n",
                "\n",
                "print(f\"\\nModel trained in {train_time:.2f}s\")\n",
                "print(f\"\\n{'='*55}\")\n",
                "print(\"CLASSIFICATION REPORT (Test Set)\")\n",
                "print(f\"{'='*55}\")\n",
                "print(classification_report(y_test, y_pred, target_names=['Active', 'Churned']))\n",
                "print(f\"Overall F1: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
                "print(f\"Accuracy:   {accuracy_score(y_test, y_pred):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# FEATURE IMPORTANCE (sanity check for leakage)\n",
                "# ============================================================\n",
                "importances = pd.Series(\n",
                "    pipeline.named_steps['model'].feature_importances_,\n",
                "    index=X_train.columns\n",
                ").sort_values(ascending=False)\n",
                "\n",
                "print(\"\\nTop 15 Feature Importances:\")\n",
                "print(\"-\"*40)\n",
                "for feat, imp in importances.head(15).items():\n",
                "    bar = '‚ñà' * int(imp * 100)\n",
                "    print(f\"  {feat:<30} {imp:.3f} {bar}\")\n",
                "\n",
                "# Leakage red flag: single feature dominance\n",
                "if importances.iloc[0] > 0.5:\n",
                "    print(f\"\\n‚ö†Ô∏è WARNING: '{importances.index[0]}' dominates ({importances.iloc[0]:.1%}). Check for leakage!\")\n",
                "else:\n",
                "    print(f\"\\n‚úÖ No single feature dominates. Importance is well-distributed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STORE AS PARQUET (Lesson 1: binary columnar > CSV)\n",
                "# ============================================================\n",
                "print(\"\\nSTORING ARTIFACTS\")\n",
                "print(\"=\"*55)\n",
                "\n",
                "# Save train features\n",
                "train_path = os.path.join(OUTPUT_DIR, 'train_features.parquet')\n",
                "train_encoded.to_parquet(train_path, index=False, compression='snappy')\n",
                "train_size = os.path.getsize(train_path) / 1024\n",
                "\n",
                "# Save test features\n",
                "test_path = os.path.join(OUTPUT_DIR, 'test_features.parquet')\n",
                "test_encoded.to_parquet(test_path, index=False, compression='snappy')\n",
                "test_size = os.path.getsize(test_path) / 1024\n",
                "\n",
                "# Compare with CSV size\n",
                "csv_path = os.path.join(OUTPUT_DIR, 'train_features.csv')\n",
                "train_encoded.to_csv(csv_path, index=False)\n",
                "csv_size = os.path.getsize(csv_path) / 1024\n",
                "\n",
                "print(f\"  Parquet (train): {train_size:.1f} KB\")\n",
                "print(f\"  Parquet (test):  {test_size:.1f} KB\")\n",
                "print(f\"  CSV equivalent:  {csv_size:.1f} KB\")\n",
                "print(f\"  Compression:     {csv_size/train_size:.1f}x smaller with Parquet!\")\n",
                "\n",
                "# Save pipeline metadata\n",
                "metadata = {\n",
                "    'pipeline_run_date': datetime.now().isoformat(),\n",
                "    'split_date': str(SPLIT_DATE.date()),\n",
                "    'train_rows': len(train_encoded),\n",
                "    'test_rows': len(test_encoded),\n",
                "    'n_features': len(X_train.columns),\n",
                "    'churn_rate_train': float(y_train.mean()),\n",
                "    'churn_rate_test': float(y_test.mean()),\n",
                "    'test_f1': float(f1_score(y_test, y_pred, average='weighted')),\n",
                "    'test_accuracy': float(accuracy_score(y_test, y_pred)),\n",
                "    'feature_columns': list(X_train.columns),\n",
                "}\n",
                "with open(os.path.join(OUTPUT_DIR, 'pipeline_metadata.json'), 'w') as f:\n",
                "    json.dump(metadata, f, indent=2)\n",
                "\n",
                "print(f\"\\n‚úÖ All artifacts saved to {OUTPUT_DIR}/\")\n",
                "\n",
                "# Cleanup\n",
                "shutil.rmtree(OUTPUT_DIR)\n",
                "print(\"‚úÖ Cleaned up output directory\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üèÜ Pipeline Summary & Production Readiness Checklist\n",
                "\n",
                "```\n",
                " ‚úÖ Multi-source ingestion       (Lesson 1: 4 data source types)\n",
                " ‚úÖ Schema + null validation      (Lesson 2 & 7: DataValidator pattern)\n",
                " ‚úÖ Data cleaning & dedup         (Lesson 2: ETL Transform)\n",
                " ‚úÖ Temporal train/test split     (Lesson 4: no temporal leakage)\n",
                " ‚úÖ Feature engineering (20+ feats)(Lesson 5 & 6: reusable function)\n",
                " ‚úÖ Target leakage detection      (Lesson 4: correlation check)\n",
                " ‚úÖ Output quality validation     (Lesson 7: post-transform checks)\n",
                " ‚úÖ Leak-free sklearn Pipeline    (Lesson 4: fit only on train)\n",
                " ‚úÖ Class imbalance handling      (Lesson 3: balanced class weights)\n",
                " ‚úÖ Feature importance audit      (Lesson 4: no single-feature dominance)\n",
                " ‚úÖ Parquet storage + metadata    (Lesson 1: binary columnar format)\n",
                "```\n",
                "\n",
                "### What Would Change in Full Production\n",
                "\n",
                "| This Project | Production |\n",
                "|-------------|------------|\n",
                "| Synthetic data | Real DB/API sources |\n",
                "| Pandas | Spark for >10GB data (Lesson 6) |\n",
                "| Manual runs | Airflow/Prefect orchestration (Lesson 7) |\n",
                "| In-memory features | Feast feature store (Lesson 5) |\n",
                "| Print statements | Structured logging + Prometheus metrics |\n",
                "| Local files | S3/GCS data lake with partitioning |\n",
                "\n",
                "---\n",
                "\n",
                "**üéì All 7 lessons of Module 3 applied in one coherent, production-grade pipeline!**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
