{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ›’ Capstone Phase 1: Exploration & Feature Engineering\n",
    "\n",
    "**Goal**: Understand our synthetic retail data and build the \"Industrial\" feature pipeline required for non-time-series models (like LightGBM) and Deep Learning models.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Load & Explore Data\n",
    "\n",
    "We start by loading the `parquet` file generated by our ingestion script. Parquet is column-oriented and much faster/smaller than CSV for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_parquet('../data/raw/m5_lite_synthetic.parquet')\n",
    "\n",
    "# Convert object columns to category for memory efficiency\n",
    "for col in ['store_id', 'item_id']:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizing Demand Patterns\n",
    "\n",
    "Before modeling, we must verify if our synthetic data looks \"real\".\n",
    "We expect to see:\n",
    "1. **Seasonality**: Sales going up/down weekly.\n",
    "2. **Trends**: Long-term increase or decrease.\n",
    "3. **Promo Effects**: Spikes when `is_promo=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one item to plot\n",
    "sample_item = 'item_0'\n",
    "sample_store = 'store_0'\n",
    "\n",
    "subset = df[(df['item_id'] == sample_item) & (df['store_id'] == sample_store)].set_index('date')\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(subset['sales'], label='Sales', alpha=0.7)\n",
    "plt.scatter(subset[subset['is_promo']==1].index, \n",
    "            subset[subset['is_promo']==1]['sales'], \n",
    "            color='red', label='Promo', s=20)\n",
    "\n",
    "plt.title(f\"Sales History: {sample_store} - {sample_item}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering (The \"Secret Sauce\")\n",
    "\n",
    "Machine Learning models (like XGBoost/LightGBM) don't \"know\" time. They treat every row as independent. \n",
    "To tell them about the past, we must explicitly create features representing history.\n",
    "\n",
    "### 3.1 Lag Transform\n",
    "What were the sales 7 days ago? 28 days ago?\n",
    "- `lag_7`: Captures weekly seasonality (e.g., this Saturday is like last Saturday).\n",
    "- `lag_28`: Captures monthly patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(df, lags=[7, 14, 28]):\n",
    "    df = df.sort_values(['store_id', 'item_id', 'date']).copy()\n",
    "    \n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df.groupby(['store_id', 'item_id'])['sales'].shift(lag)\n",
    "        \n",
    "    return df\n",
    "\n",
    "df_lags = create_lag_features(df)\n",
    "df_lags.dropna(inplace=True)\n",
    "df_lags[['date', 'item_id', 'sales', 'lag_7', 'lag_28']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Rolling Window Statistics\n",
    "Instead of just a single point (lag), we look at a summary of a window.\n",
    "- **Rolling Mean**: \"What is the average level of sales recently?\" (Trend)\n",
    "- **Rolling Std**: \"How volatile is this item?\" (Uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_features(df, windows=[7, 28]):\n",
    "    df = df.sort_values(['store_id', 'item_id', 'date']).copy()\n",
    "    \n",
    "    # We shift by 1 first to avoid leakage (including today's sales in today's features)\n",
    "    # But since we use lag_7 as a base for rolling usually in forecasting, let's keep it simple:\n",
    "    # Rolling mean of the last 28 days, shifted by 28 days (to be safe for 4-week forecast horizon)\n",
    "    \n",
    "    for window in windows:\n",
    "        # Group by item, shift by horizon (28), then roll\n",
    "        # For simplicity in this demo, we assume we are predicting day-ahead mostly, \n",
    "        # but for multi-step, we normally roll on the shifted series.\n",
    "        \n",
    "        df[f'rolling_mean_{window}'] = df.groupby(['store_id', 'item_id'])['sales'] \\\n",
    "                                      .transform(lambda x: x.shift(28).rolling(window).mean())\n",
    "                                      \n",
    "        df[f'rolling_std_{window}'] = df.groupby(['store_id', 'item_id'])['sales'] \\\n",
    "                                     .transform(lambda x: x.shift(28).rolling(window).std())\n",
    "            \n",
    "    return df\n",
    "\n",
    "df_feats = create_rolling_features(df_lags)\n",
    "df_feats.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Date Features\n",
    "The model needs to know \"Is it a weekend?\" or \"Is it December?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_features(df):\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek # 0=Monday, 6=Sunday\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # Cyclical Encoding for Month (Dec is close to Jan)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_final = create_date_features(df_feats)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Processed Data\n",
    "\n",
    "We now have a \"Tabular\" dataset ready for training LightGBM or Deep Learning models.\n",
    "Features:\n",
    "- `lag_7`, `lag_14`, `lag_28`\n",
    "- `rolling_mean_7`, `rolling_std_7`\n",
    "- `month`, `day_of_week`, `sell_price`, `is_promo`\n",
    "\n",
    "Target: `sales`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../data/processed/tabular_train_data.parquet'\n",
    "df_final.to_parquet(output_path, index=False)\n",
    "print(f\"Saved processed data to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
