{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 3 Exercises: Data Engineering\n",
                "\n",
                "**Objective**: Master ETL pipelines, Data Leakage detection, and Feature Stores.\n",
                "\n",
                "---\n",
                "\n",
                "## üõ†Ô∏è Setup\n",
                "\n",
                "```bash\n",
                "pip install pandas pyarrow\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üïµÔ∏è Part 1: Leakage Detective\n",
                "\n",
                "**Scenario**: You are predicting **Customer Churn**. You have the following features. Which ones are \"Leaky\" (Future Information) and must be dropped?\n",
                "\n",
                "| Feature | Description | Decision |\n",
                "|---|---|---|\n",
                "| `account_age` | Days since signup | ‚úÖ Keep |\n",
                "| `customer_service_calls` | Calls in last 30 days | ‚úÖ Keep |\n",
                "| `churn_reason` | Text field form exit survey | ‚ùì Wait... |\n",
                "| `last_payment_date` | Date of last bill | ‚ùì Wait... |\n",
                "\n",
                "<details>\n",
                "<summary><b>üîª Click to Reveal Verdict</b></summary>\n",
                "<br>\n",
                "1. <b>churn_reason</b>: üö® <b>LEAK!</b> This only exists IF the user has already churned. Using it gives 100% accuracy in training but 0% in production.\n",
                "2. <b>last_payment_date</b>: ‚ö†Ô∏è <b>Risk!</b> If `last_payment` is > 30 days ago, they might have already churned. Be very careful with dates that update *after* the prediction point.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üöÄ Part 2: Feature Store vs Warehouse\n",
                "\n",
                "**Task**: Fill in the blanks in this system design table.\n",
                "\n",
                "| Requirement | Data Warehouse (Snowflake) | Feature Store (Redis/Feast) |\n",
                "|---|---|---|\n",
                "| **Latency** | High (Seconds/Minutes) | [__________] |\n",
                "| **Use Case** | Analytics / BI reports | [__________] |\n",
                "| **Data State** | Historical / Batch | [__________] |\n",
                "\n",
                "<details>\n",
                "<summary><b>üîª Click for Answer</b></summary>\n",
                "<br>\n",
                "\n",
                "| Requirement | Warehouse | Feature Store |\n",
                "|---|---|---|\n",
                "| Latency | High | <b>Low (Milliseconds)</b> |\n",
                "| Use Case | Analytics | <b>Real-time Inference</b> |\n",
                "| Data State | Historical | <b>Latest Values (Current State)</b> |\n",
                "\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß† Part 3: Design a Pipeline (Interactive)\n",
                "\n",
                "**Scenario**: You have 1 TB of CSV logs on S3. You need to train a model daily.\n",
                "**Constraint**: You cannot fit 1 TB in RAM pandas.\n",
                "\n",
                "Which tool do you choose?\n",
                "A) `pandas.read_csv`\n",
                "B) `Spark` / `Dask`\n",
                "C) Loop through file line-by-line in Python"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def pipeline_quiz():\n",
                "    ans = input(\"Your Choice (A/B/C): \").upper()\n",
                "    if ans == \"B\":\n",
                "        print(\"‚úÖ Correct! Spark/Dask process data in chunks/distributed memory.\")\n",
                "    elif ans == \"C\":\n",
                "        print(\"‚ö†Ô∏è Technically possible, but extremely slow for filtering/aggregating 1TB.\")\n",
                "    else:\n",
                "        print(\"‚ùå Incorrect. Pandas will crash with OOM (Out of Memory).\")\n",
                "\n",
                "# pipeline_quiz()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}