{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 5: Knowledge Distillation\n",
                "\n",
                "**Module 4: Model Development & Optimization**  \n",
                "**Estimated Time**: 1-2 hours  \n",
                "**Difficulty**: Advanced\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "âœ… Understand the Teacher-Student configuration  \n",
                "âœ… Learn why \"Dark Knowledge\" (Soft Tags) helps small models learn  \n",
                "âœ… Implement a custom Distillation Loss function in PyTorch  \n",
                "âœ… Answer interview questions on model compression  \n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“š Table of Contents\n",
                "\n",
                "1. [The Concept: Teacher & Student](#1-concept)\n",
                "2. [The Magic: Soft Targets & Temperature](#2-temperature)\n",
                "3. [Hands-On: Distillation Loss Implementation](#3-hands-on)\n",
                "4. [Interview Preparation](#4-interview-questions)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Concept: Teacher & Student\n",
                "\n",
                "**Goal**: We want the performance of a huge model (e.g., BERT-Large) but the speed of a tiny model (e.g., DistilBERT).\n",
                "\n",
                "**Method**: Train the tiny model (Student) to mimic the huge model (Teacher). \n",
                "The Student learns not just from the ground truth labels (Hard Targets), but from the Teacher's probability distribution (Soft Targets)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. The Magic: Soft Targets & Temperature\n",
                "\n",
                "**Hard Target**: `[0, 1, 0]` (It's a Dog).\n",
                "**Teacher Output**: `[0.05, 0.90, 0.05]` (It's mostly a Dog, but looks 5% like a Cat).\n",
                "\n",
                "That 5% is **Dark Knowledge**. It tells the Student \"Dogs look a bit like Cats sometimes\". This richness helps the Student generalize better than just learning \"Dog\".\n",
                "\n",
                "**Temperature (T)**: \n",
                "To make the distribution softer (reveal more dark knowledge), we divide logits by T > 1 before Softmax."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Hands-On: Distillation Loss Implementation\n",
                "\n",
                "The loss is a combination of:\n",
                "1. **Student vs Ground Truth** (CrossEntropy)\n",
                "2. **Student vs Teacher** (KL Divergence)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "def distillation_loss(student_logits, teacher_logits, labels, T=2.0, alpha=0.5):\n",
                "    \"\"\"\n",
                "    student_logits: Output of student model\n",
                "    teacher_logits: Output of teacher model (frozen)\n",
                "    labels: Ground truth\n",
                "    T: Temperature\n",
                "    alpha: Weight for Soft Target loss\n",
                "    \"\"\"\n",
                "    \n",
                "    # 1. Hard Target Loss (Student vs Label)\n",
                "    hard_loss = F.cross_entropy(student_logits, labels)\n",
                "    \n",
                "    # 2. Soft Target Loss (Student vs Teacher)\n",
                "    # Apply Temperature\n",
                "    soft_student = F.log_softmax(student_logits / T, dim=1)\n",
                "    soft_teacher = F.softmax(teacher_logits / T, dim=1)\n",
                "    \n",
                "    # KL Divergence\n",
                "    distillation_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (T * T)\n",
                "    \n",
                "    # Combine\n",
                "    total_loss = (1. - alpha) * hard_loss + alpha * distillation_loss\n",
                "    return total_loss\n",
                "\n",
                "# Simulation\n",
                "s_logits = torch.tensor([[2.0, 5.0, 1.0]], requires_grad=True) # Learns\n",
                "t_logits = torch.tensor([[2.1, 5.8, 1.2]]) # Fixed\n",
                "labels = torch.tensor([1])\n",
                "\n",
                "loss = distillation_loss(s_logits, t_logits, labels)\n",
                "print(f\"Distillation Loss: {loss.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Interview Preparation\n",
                "\n",
                "### Common Questions\n",
                "\n",
                "#### Q1: \"Why use Temperature > 1?\"\n",
                "**Answer**: \"Standard Softmax pushes probabilities towards 0 and 1 (sharp peaks). High temperature softens the distribution, spreading probability mass to the incorrect classes. This reveals the structural similarity between classes (e.g., Truck is more similar to Car than to Frog), which provides more information to the student.\"\n",
                "\n",
                "#### Q2: \"Can a Student outperform a Teacher?\"\n",
                "**Answer**: \"Rarely in raw accuracy if capacities differ significantly. However, a Student can outperform a Teacher trained **only** on labels, because the Student gets the extra supervision ('Dark Knowledge') from the Teacher's soft targets.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}