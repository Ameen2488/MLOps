{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 2: ETL Pipelines\n",
                "\n",
                "**Module 3: Data & Pipeline Engineering**  \n",
                "**Estimated Time**: 2 hours  \n",
                "**Difficulty**: Intermediate\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "‚úÖ Understand ETL (Extract, Transform, Load) vs ELT  \n",
                "‚úÖ Master **Idempotency** (the most important pipeline concept)  \n",
                "‚úÖ Build a resilient Batch Pipeline in Python  \n",
                "‚úÖ Answer interview questions on pipeline design  \n",
                "\n",
                "---\n",
                "\n",
                "## üìö Table of Contents\n",
                "\n",
                "1. [ETL vs ELT](#1-etl-elt)\n",
                "2. [The Golden Rule: Idempotency](#2-idempotency)\n",
                "3. [Hands-On: Robust Pipeline](#3-hands-on)\n",
                "4. [Interview Preparation](#4-interview-questions)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ETL vs ELT\n",
                "\n",
                "### ETL (Extract, Transform, Load)\n",
                "- **Order**: Read data ‚Üí Process in memory (Python/Spark) ‚Üí Write to DB.\n",
                "- **Use case**: Complex transformations, privacy filtering (PII) before storage.\n",
                "\n",
                "### ELT (Extract, Load, Transform)\n",
                "- **Order**: Dump raw data to DB/Warehouse ‚Üí Transform using SQL (dbt).\n",
                "- **Use case**: Modern Data Stack (Snowflake/BigQuery). Compute is cheap inside the warehouse.\n",
                "\n",
                "**ML Context**: We mostly do **ETL** because complex feature engineering is easier in Python/Spark than SQL."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. The Golden Rule: Idempotency\n",
                "\n",
                "**Definition**: An operation is *idempotent* if running it multiple times yields the **same result** as running it once.\n",
                "\n",
                "**Why it matters**:\n",
                "Pipelines fail. You will need to retry them.\n",
                "\n",
                "### Example: Bad (Not Idempotent)\n",
                "```python\n",
                "# Failing run adds 5 records, then crashes.\n",
                "# Retry adds 10 records.\n",
                "# Total = 15 records. Duplicates!\n",
                "def process_data():\n",
                "    new_data = read()\n",
                "    database.append(new_data) \n",
                "```\n",
                "\n",
                "### Example: Good (Idempotent)\n",
                "```python\n",
                "# Failing run writes partition '2023-01-01', then crashes.\n",
                "# Retry OVERWRITES partition '2023-01-01'.\n",
                "# Result is correct.\n",
                "def process_data(date):\n",
                "    new_data = read(date)\n",
                "    database.overwrite_partition(date, new_data)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Hands-On: Robust Pipeline\n",
                "\n",
                "Let's build a mini-pipeline that handles failures gracefully."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "import shutil\n",
                "\n",
                "# Simulation: Source Data (Daily Logs)\n",
                "source_data = {\n",
                "    '2023-01-01': pd.DataFrame({'id': [1, 2], 'val': [10, 20]}),\n",
                "    '2023-01-02': pd.DataFrame({'id': [3, 4], 'val': [30, 40]})\n",
                "}\n",
                "\n",
                "OUTPUT_DIR = \"data_lake/processed\"\n",
                "\n",
                "def extract(date):\n",
                "    print(f\"[Extract] Reading source for {date}...\")\n",
                "    # Simulate API call or DB read\n",
                "    return source_data.get(date)\n",
                "\n",
                "def transform(df):\n",
                "    if df is None: return None\n",
                "    print(f\"[Transform] Normalizing values...\")\n",
                "    df = df.copy()\n",
                "    df['val_norm'] = df['val'] / 100.0\n",
                "    return df\n",
                "\n",
                "def load(df, date):\n",
                "    if df is None: return\n",
                "    \n",
                "    # IDEMPOTENCY KEY: Partition by Date\n",
                "    # Instead of appending to one big file, we write specific files per day.\n",
                "    # If we re-run this function, we just overwrite the file.\n",
                "    \n",
                "    target_path = f\"{OUTPUT_DIR}/date={date}\"\n",
                "    \n",
                "    # Ensure clean slate for this partition\n",
                "    if os.path.exists(target_path):\n",
                "        shutil.rmtree(target_path)\n",
                "    os.makedirs(target_path)\n",
                "    \n",
                "    file_path = f\"{target_path}/data.parquet\"\n",
                "    print(f\"[Load] Writing to {file_path}...\")\n",
                "    df.to_parquet(file_path)\n",
                "\n",
                "def run_pipeline(date):\n",
                "    try:\n",
                "        print(f\"\\n--- Starting Pipeline for {date} ---\")\n",
                "        raw = extract(date)\n",
                "        processed = transform(raw)\n",
                "        load(processed, date)\n",
                "        print(\"‚úÖ Success!\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Failed: {e}\")\n",
                "\n",
                "# Run for Day 1\n",
                "run_pipeline('2023-01-01')\n",
                "\n",
                "# Run for Day 1 AGAIN (Should be safe!)\n",
                "run_pipeline('2023-01-01')\n",
                "\n",
                "# Run for Day 2\n",
                "run_pipeline('2023-01-02')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Interview Preparation\n",
                "\n",
                "### Common Questions\n",
                "\n",
                "#### Q1: \"How do you handle pipeline failures?\"\n",
                "**Answer**: \n",
                "1. **Idempotency**: Ensure retries don't duplicate data.\n",
                "2. **Atomic Writes**: Write to a temp folder, then swap/rename at the end.\n",
                "3. **Checkpointing**: In streaming, commit offsets only after processing.\n",
                "4. **Alerting**: PagerDuty/Slack alerts on failure.\n",
                "\n",
                "#### Q2: \"What is a DAG?\"\n",
                "**Answer**: \"Directed Acyclic Graph. It represents the workflow logic. Task A (Extract) must finish before Task B (Transform) starts. Airflow uses DAGs to manage dependencies and execution order without creating loops.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}