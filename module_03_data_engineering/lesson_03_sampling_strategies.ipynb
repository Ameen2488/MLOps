{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 3: Sampling Strategies\n",
                "\n",
                "**Module 3: Data & Pipeline Engineering**  \n",
                "**Estimated Time**: 1-2 hours  \n",
                "**Difficulty**: Intermediate\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "âœ… Handle massive Class Imbalance (e.g., Fraud Detection)  \n",
                "âœ… Implement Stratified Splitting  \n",
                "âœ… Use SMOTE (Synthetic Minority Over-sampling Technique)  \n",
                "âœ… Understand when Upsampling is better than Downsampling  \n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“š Table of Contents\n",
                "\n",
                "1. [The Imbalance Problem](#1-imbalance)\n",
                "2. [Strategic Splitting: Stratified Sampling](#2-stratified)\n",
                "3. [Resampling Techniques: Up vs Down](#3-resampling)\n",
                "4. [Advanced: SMOTE](#4-smote)\n",
                "5. [Interview Preparation](#5-interview-questions)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Imbalance Problem\n",
                "\n",
                "**Scenario**: 1000 transactions. 995 are Legit (Class 0), 5 are Fraud (Class 1).\n",
                "\n",
                "**The Trap**: A \"dumb\" model that always predicts \"Legit\" gets **99.5% Accuracy**.\n",
                "\n",
                "**The Solution**: We need to change the data distribution during *training* so the model pays attention to the minority class."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Strategic Splitting: Stratified Sampling\n",
                "\n",
                "Before resampling, you MUST split your data correctly.\n",
                "\n",
                "**Random Split Risk**: If you have 5 fraud cases and do an 80/20 random split, you might end up with ALL 5 fraud cases in the Test set. Your text/train sizes are fine, but your training set has ZERO fraud examples.\n",
                "\n",
                "**Fix**: `stratify=y` ensures the ratio of classes is preserved in both Train and Test sets."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Resampling Techniques: Up vs Down\n",
                "\n",
                "### Downsampling (Undersampling)\n",
                "- **Action**: Delete random rows from Majority class.\n",
                "- **Pro**: Faster training.\n",
                "- **Con**: Loss of information.\n",
                "- **When to use**: You have HUGE data (millions of rows).\n",
                "\n",
                "### Upsampling (Oversampling)\n",
                "- **Action**: Duplicate random rows from Minority class.\n",
                "- **Pro**: Keeps all information.\n",
                "- **Con**: Overfitting (Model memorizes duplicates).\n",
                "- **When to use**: Small dataset."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Advanced: SMOTE\n",
                "\n",
                "**S**ynthetic **M**inority **O**ver-sampling **TE**chnique.\n",
                "\n",
                "Instead of duplicating, it generates **new synthetic examples** by interpolating between existing minority samples.\n",
                "\n",
                "**Library**: `imbalanced-learn` (`pip install imblearn`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.datasets import make_classification\n",
                "from sklearn.model_selection import train_test_split\n",
                "from collections import Counter\n",
                "from imblearn.over_sampling import SMOTE\n",
                "\n",
                "# 1. Generate Imbalanced Data\n",
                "X, y = make_classification(\n",
                "    n_samples=5000, \n",
                "    n_features=10, \n",
                "    weights=[0.99], # 99% Majority\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "print(f\"Original Class Distribution: {Counter(y)}\")\n",
                "\n",
                "# 2. Stratified Split (CRITICAL STEP)\n",
                "# Never resample before splitting! You will leak data.\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, \n",
                "    test_size=0.2, \n",
                "    random_state=42, \n",
                "    stratify=y  # <--- MAGIC KEYWORD\n",
                ")\n",
                "\n",
                "print(f\"Train Distribution: {Counter(y_train)}\")\n",
                "print(f\"Test Distribution: {Counter(y_test)}\")\n",
                "\n",
                "# 3. Apply SMOTE (Only on Training Data)\n",
                "smote = SMOTE(random_state=42)\n",
                "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
                "\n",
                "print(f\"Resampled Train Distribution: {Counter(y_train_resampled)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Interview Preparation\n",
                "\n",
                "### Common Questions\n",
                "\n",
                "#### Q1: \"In fraud detection, accuracy is 99%. Is the model good?\"\n",
                "**Answer**: \"Likely not. If fraud is 1% of data, a dummy classifier gets 99%. I would look at Precision, Recall, and F1-score, specifically focusing on Recall (catching as many frauds as possible).\"\n",
                "\n",
                "#### Q2: \"When would you use Random Undersampling over SMOTE?\"\n",
                "**Answer**: \"If I have massive data (e.g., 100 million negative clicks vs 1 million positive clicks), SMOTE is computationally expensive (O(N^2) or KNN based). Undersampling reduces the data size, making training feasible, and with 1 million positives, I simpler methods work fine.\"\n",
                "\n",
                "#### Q3: \"Should I resample the Test set?\"\n",
                "**Answer**: \"**NEVER**. The test set must reflect the REAL production distribution. If you balance the test set, your metrics (Accuracy/Precision) will be completely wrong relative to real-world performance.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}