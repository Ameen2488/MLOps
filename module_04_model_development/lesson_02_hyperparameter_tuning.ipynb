{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 2: Hyperparameter Tuning\n",
                "\n",
                "**Module 4: Model Development & Optimization**  \n",
                "**Estimated Time**: 2 hours  \n",
                "**Difficulty**: Intermediate\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "âœ… Understand the difference between Parameters vs Hyperparameters  \n",
                "âœ… Know why Grid Search is often a bad idea  \n",
                "âœ… Implement efficient tuning with **Optuna** (Bayesian Optimization)  \n",
                "âœ… Answer interview questions on optimization strategies  \n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“š Table of Contents\n",
                "\n",
                "1. [Parameters vs Hyperparameters](#1-definitions)\n",
                "2. [Search Strategies: Grid vs Random vs Bayesian](#2-strategies)\n",
                "3. [Hands-On: Tuning XGBoost with Optuna](#3-hands-on)\n",
                "4. [Interview Preparation](#4-interview-questions)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Parameters vs Hyperparameters\n",
                "\n",
                "**Parameters**:\n",
                "- Learned **internal** weights of the model.\n",
                "- Example: `Coefficients` in Linear Regression, `Weights` in Neural Net.\n",
                "- You do **NOT** set these manually.\n",
                "\n",
                "**Hyperparameters**:\n",
                "- External configuration set **before** training.\n",
                "- Example: `Learning Rate`, `Tree Depth`, `Number of Layers`.\n",
                "- You **MUST** set these derived from experience or search."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Search Strategies\n",
                "\n",
                "### 1. Grid Search\n",
                "- Try ALL combinations.\n",
                "- `LR = [0.1, 0.01], Depth = [3, 5]` -> 4 runs.\n",
                "- **Pros**: Guaranteed to find the best in the grid.\n",
                "- **Cons**: Exponential cost ($O(N^k)$). Impossible for >3 params.\n",
                "\n",
                "### 2. Random Search\n",
                "- Try random combinations.\n",
                "- **Pros**: Surprisingly effective. Can explore continuous spaces more efficiently.\n",
                "- **Cons**: Dumb. Doesn't learn from past failures.\n",
                "\n",
                "### 3. Bayesian Optimization (Optuna)\n",
                "- **Smart Search**.\n",
                "- Builds a probability model of the objective function.\n",
                "- \"I tried LR=0.01 and it was bad. I won't try LR=0.009 next. I'll try 0.1.\"\n",
                "- **Pros**: Most efficient. Finds better results in fewer runs."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Hands-On: Tuning XGBoost with Optuna\n",
                "\n",
                "Note: Requires `pip install optuna xgboost scikit-learn`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import optuna\n",
                "import xgboost as xgb\n",
                "from sklearn.datasets import load_breast_cancer\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "# 1. Load Data\n",
                "data = load_breast_cancer()\n",
                "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25)\n",
                "\n",
                "# 2. Define Objective Function\n",
                "def objective(trial):\n",
                "    # Suggest hyperparameters\n",
                "    param = {\n",
                "        'objective': 'binary:logistic',\n",
                "        'eval_metric': 'logloss',\n",
                "        'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
                "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
                "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
                "        'max_depth': trial.suggest_int('max_depth', 1, 9),\n",
                "        'eta': trial.suggest_float('eta', 0.01, 1.0, log=True),\n",
                "    }\n",
                "\n",
                "    # Train model\n",
                "    bst = xgb.XGBClassifier(**param)\n",
                "    bst.fit(X_train, y_train)\n",
                "    \n",
                "    # Evaluate\n",
                "    preds = bst.predict(X_test)\n",
                "    accuracy = accuracy_score(y_test, preds)\n",
                "    \n",
                "    return accuracy\n",
                "\n",
                "# 3. Run Optimization\n",
                "study = optuna.create_study(direction=\"maximize\")\n",
                "study.optimize(objective, n_trials=10) # Run 10 experiments\n",
                "\n",
                "print(f\"Best Accuracy: {study.best_value}\")\n",
                "print(f\"Best Params: {study.best_params}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Interview Preparation\n",
                "\n",
                "### Common Questions\n",
                "\n",
                "#### Q1: \"Why is Random Search often better than Grid Search?\"\n",
                "**Answer**: \"In high dimensions, not all parameters are equally important. Grid search wastes time checking all combinations of unimportant parameters. Random search explores the space of important parameters more densely for the same computational budget.\"\n",
                "\n",
                "#### Q2: \"How does Bayesian Optimization work conceptually?\"\n",
                "**Answer**: \"It builds a surrogate model (Gaussian Process) to approximate the relationship between hyperparameters and model performance. It uses an acquisition function to decide where to sample next, balancing **Exploration** (unsampled areas) and **Exploitation** (promising areas).\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}