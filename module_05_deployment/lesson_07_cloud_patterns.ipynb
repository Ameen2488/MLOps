{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 7: Cloud Deployment Patterns\n",
                "\n",
                "**Module 5: Model Deployment**  \n",
                "**Estimated Time**: 1 hour  \n",
                "**Difficulty**: Beginner\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "âœ… Understand the 3 Main Cloud Compute Options  \n",
                "âœ… Compare **VMs**, **Containers**, and **Serverless**  \n",
                "âœ… Decide which one to use for your ML Model  \n",
                "âœ… Answer interview questions on Cloud Architecture  \n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“š Table of Contents\n",
                "\n",
                "1. [Option 1: Virtual Machines (EC2)](#1-vm)\n",
                "2. [Option 2: Container Orchestration (ECS/EKS)](#2-containers)\n",
                "3. [Option 3: Serverless (Lambda)](#3-serverless)\n",
                "4. [Decision Matrix](#4-decision)\n",
                "5. [Interview Preparation](#5-interview-questions)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Option 1: Virtual Machines (EC2)\n",
                "\n",
                "**Analogy**: Renting a computer in the cloud.\n",
                "- **Pros**: Full control, persistent GPU support.\n",
                "- **Cons**: You manage OS updates, security patches, scaling is slower (minutes).\n",
                "- **Use Case**: Training large models (Multi-GPU)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Option 2: Container Orchestration (ECS/EKS)\n",
                "\n",
                "**Analogy**: Renting a fleet of managed Docker hosts.\n",
                "- **Pros**: Standard for microservices, good scaling.\n",
                "- **Cons**: Complex setup (K8s).\n",
                "- **Use Case**: Hosting 50+ different ML models for a company."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Option 3: Serverless (Lambda)\n",
                "\n",
                "**Analogy**: Uber. You pay only for the ride (execution time).\n",
                "- **Pros**: Zero management, scales to zero (cheap).\n",
                "- **Cons**: Cold starts (latency), size limits (hard to fit PyTorch).\n",
                "- **Use Case**: Infrequent traffic, lightweight models (ONNX/Scikit-learn)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Decision Matrix\n",
                "\n",
                "| Feature | Serverless (Lambda) | Containers (K8s) | VM (EC2) |\n",
                "|---|---|---|---|\n",
                "| **Cost (Idle)** | $0 | $$ | $$$ |\n",
                "| **Latency** | High (Cold Start) | Low | Low |\n",
                "| **Max Runtime** | 15 mins | Infinite | Infinite |\n",
                "| **GPU Support** | No (mostly) | Yes | Yes |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Interview Preparation\n",
                "\n",
                "### Common Questions\n",
                "\n",
                "#### Q1: \"How do you deploy a deep learning model on Lambda?\"\n",
                "**Answer**: \"Lambda has a 10GB size limit (docker) and 250MB (zip). Since PyTorch + CUDA is huge, I would:\n",
                "1. Use **CPU-only** versions of libraries.\n",
                "2. Use **ONNX Runtime** (smaller than PyTorch).\n",
                "3. Use **Docker Image** support in Lambda.\"\n",
                "\n",
                "#### Q2: \"When would you choose SageMaker vs EC2?\"\n",
                "**Answer**: \"SageMaker is a managed service (PAAS). I use it if I want built-in features like Model Registry, Endpoints, and Drift Detection without building them myself. I use EC2 if I need something very custom or want to save costs by using Spot Instances manually.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}