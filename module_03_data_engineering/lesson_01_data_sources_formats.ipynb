{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 1: Data Sources & Formats\n",
                "\n",
                "**Module 3: Data & Pipeline Engineering**  \n",
                "**Estimated Time**: 3-4 hours  \n",
                "**Difficulty**: Beginner-Intermediate\n",
                "\n",
                "---\n",
                "\n",
                "## ğŸ¯ Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "âœ… Understand the different types of data sources in production ML systems  \n",
                "âœ… Grasp the critical trade-offs between text vs binary and row vs column-major formats  \n",
                "âœ… Know when to use CSV, Parquet, Avro, and JSON  \n",
                "âœ… Benchmark read/write performance and compression strategies  \n",
                "âœ… Answer 5 interview questions on data sources and formats  \n",
                "\n",
                "---\n",
                "\n",
                "## ğŸ“š Table of Contents\n",
                "\n",
                "1. [The Data Landscape in Production ML](#1-data-landscape)\n",
                "2. [Data Sources in Production Systems](#2-data-sources)\n",
                "3. [Data Formats: Text vs Binary](#3-text-vs-binary)\n",
                "4. [Data Formats: Row-Major vs Column-Major](#4-row-vs-column)\n",
                "5. [Format Comparison Table](#5-format-comparison)\n",
                "6. [Hands-On: Benchmarking Performance](#6-benchmarking)\n",
                "7. [Compression Strategies](#7-compression)\n",
                "8. [Column Pruning Demo](#8-column-pruning)\n",
                "9. [Exercises](#9-exercises)\n",
                "10. [Interview Preparation](#10-interview)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Data Landscape in Production ML <a id='1-data-landscape'></a>\n",
                "\n",
                "In machine learning operations (MLOps), success hinges not just on models but on the **data pipelines** that feed those models.\n",
                "\n",
                "Production machine learning is a different beast entirely. Here, the cleverest model architecture is worthless if itâ€™s fed unreliable data or if its predictions canâ€™t be reproduced.\n",
                "\n",
                "### The Fundamental Truth\n",
                "\n",
                "> **In an enterprise MLOps setting, engineers operate under a fundamental truth: models are often commodities, but the data and the pipelines that process it are the durable, defensible assets that drive business value.**\n",
                "\n",
                "The raw material of ML is data, and the choices we make about data have profound downstream consequences on:\n",
                "- **Performance** â€“ garbage in, garbage out\n",
                "- **Scalability** â€“ can the pipeline handle 10Ã— growth?\n",
                "- **Reliability** â€“ will the system work at 3 AM on a holiday?\n",
                "- **Reproducibility** â€“ can we recreate yesterdayâ€™s training run?\n",
                "\n",
                "### High-Level Data Flow in a Production ML System\n",
                "\n",
                "```\n",
                "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "  â”‚  DATA        â”‚   â”‚ DATA         â”‚   â”‚  FEATURE     â”‚   â”‚  MODEL       â”‚\n",
                "  â”‚  SOURCES     â”‚â”€â–¶â”‚ PIPELINE     â”‚â”€â–¶â”‚  STORE       â”‚â”€â–¶â”‚  TRAINING    â”‚\n",
                "  â”‚              â”‚   â”‚ (ETL/ELT)    â”‚   â”‚              â”‚   â”‚              â”‚\n",
                "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "    â–² User Input       â–² Clean,              â–² Consistent      â–²\n",
                "    â–² Logs            transform,          features for     Uses\n",
                "    â–² Databases       validate            train & serve    Parquet\n",
                "    â–² Third-Party\n",
                "```\n",
                "\n",
                "### Why This Matters for Interviews\n",
                "\n",
                "At companies like **Uber, Airbnb, Netflix** â€” **60-80% of ML engineering work is data engineering**. You cannot claim to be a Senior Data Scientist without understanding the data infrastructure.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Sources in Production Systems <a id='2-data-sources'></a>\n",
                "\n",
                "Data in a production environment is **not** a clean, static CSV file. It is a dynamic, messy, and continuous flow of signals from a multitude of sources, each with its own characteristics and requirements.\n",
                "\n",
                "### Data Sources Overview\n",
                "\n",
                "```\n",
                "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "  â”‚              DATA SOURCES FOR ML SYSTEMS                  â”‚\n",
                "  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
                "  â”‚ USER INPUT  â”‚ SYSTEM LOGS â”‚ INTERNAL DBs â”‚ THIRD-PARTY    â”‚\n",
                "  â”‚             â”‚             â”‚              â”‚                â”‚\n",
                "  â”‚ - Searches  â”‚ - Events    â”‚ - Inventory  â”‚ - Demographics â”‚\n",
                "  â”‚ - Uploads   â”‚ - Metrics   â”‚ - CRM        â”‚ - Social Media â”‚\n",
                "  â”‚ - Forms     â”‚ - Errors    â”‚ - Accounts   â”‚ - Weather      â”‚\n",
                "  â”‚ - Surveys   â”‚ - Latency   â”‚ - Finance    â”‚ - Economics    â”‚\n",
                "  â”‚             â”‚             â”‚              â”‚                â”‚\n",
                "  â”‚ Reliability:â”‚ Reliability:â”‚ Reliability: â”‚ Reliability:   â”‚\n",
                "  â”‚  LOW        â”‚  MEDIUM     â”‚  HIGH        â”‚  VARIABLE      â”‚\n",
                "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "```\n",
                "\n",
                "### 2.1 User Input Data\n",
                "\n",
                "This is data explicitly provided by users â€” text in a search bar, uploaded images, form submissions, etc.\n",
                "\n",
                "**Key characteristics:**\n",
                "- **Notoriously unreliable** â€” users are lazy. If itâ€™s possible to input unformatted and raw data, they will.\n",
                "- Requires **heavy-duty validation** and **robust error handling**\n",
                "- Examples: search queries, uploaded documents, survey responses, user profiles\n",
                "\n",
                "**Real-world example:** An e-commerce search bar receives queries like `\"blue sheos\"`, `\"  NIKE Air max  \"`, and `\"shoes < $50\"`. Your pipeline must handle misspellings, inconsistent casing, extra whitespace, and special characters.\n",
                "\n",
                "### 2.2 System-Generated Data (Logs)\n",
                "\n",
                "Applications and infrastructure generate a **massive volume of logs**. These record:\n",
                "- Significant events\n",
                "- System states (CPU/memory usage)\n",
                "- Service calls\n",
                "- Model predictions and latencies\n",
                "\n",
                "**Key characteristics:**\n",
                "- Often **noisy** but invaluable for debugging and monitoring\n",
                "- Can be processed in **batches** (daily/weekly) or in **real-time** (for alerts)\n",
                "- Usually stored as semi-structured JSON or plain text\n",
                "\n",
                "**Real-world example:** A model serving endpoint logs every prediction: `{\"timestamp\": \"2024-01-15T10:30:00Z\", \"model_version\": \"v2.3\", \"input_features\": {...}, \"prediction\": 0.87, \"latency_ms\": 23}`\n",
                "\n",
                "### 2.3 Internal Databases\n",
                "\n",
                "This is where enterprises typically derive **most value** from. Databases managing:\n",
                "- Inventory\n",
                "- Customer relationships (CRM)\n",
                "- User accounts\n",
                "- Financial transactions\n",
                "\n",
                "**Key characteristics:**\n",
                "- Typically **highly structured** and follows a relational model\n",
                "- Often the most valuable for **feature engineering**\n",
                "- Subject to access controls and privacy regulations\n",
                "\n",
                "**Real-world example:** A recommendation model might process a userâ€™s query, but it must check an internal inventory database to ensure that the recommended products are **actually in stock** before displaying them.\n",
                "\n",
                "### 2.4 Third-Party Data\n",
                "\n",
                "Data acquired from external vendors:\n",
                "- Demographic information\n",
                "- Social media activity\n",
                "- Purchasing habits\n",
                "- Weather data, economic indicators\n",
                "\n",
                "**Key characteristics:**\n",
                "- Powerful for **bootstrapping models** (e.g., recommender systems cold-start)\n",
                "- Availability is increasingly constrained by **privacy regulations** (GDPR, CCPA)\n",
                "- May have different update frequencies and data quality standards\n",
                "\n",
                "### Summary Table\n",
                "\n",
                "| Source | Reliability | Volume | Structure | Example |\n",
                "|--------|-----------|--------|-----------|----------|\n",
                "| User Input | Low | Variable | Unstructured | Search queries, form data |\n",
                "| System Logs | Medium | Very High | Semi-structured | Server logs, model predictions |\n",
                "| Internal DBs | High | High | Structured | CRM, inventory, transactions |\n",
                "| Third-Party | Variable | Variable | Mixed | Demographics, social media |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Formats: Text vs Binary <a id='3-text-vs-binary'></a>\n",
                "\n",
                "The format you choose for storage is a **critical architectural decision** that directly impacts storage costs, access speed, and ease of use.\n",
                "\n",
                "### Format Types Overview\n",
                "\n",
                "```\n",
                "                    DATA FORMATS\n",
                "                        â”‚\n",
                "           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "           â”‚                       â”‚\n",
                "      TEXT (Human)            BINARY (Machine)\n",
                "           â”‚                       â”‚\n",
                "     â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”\n",
                "     â”‚           â”‚         â”‚           â”‚\n",
                "    CSV        JSON      Parquet      Avro\n",
                "  (tabular)  (nested)  (columnar)   (row-based)\n",
                "```\n",
                "\n",
                "### 3.1 Text Formats (JSON, CSV)\n",
                "\n",
                "Text formats are **human-readable**. You can open a JSON or CSV file in a text editor and immediately understand its contents.\n",
                "\n",
                "**Advantages:**\n",
                "- Excellent for **debugging**, configuration, and data interchange\n",
                "- JSON is ubiquitous due to its simplicity and flexibility\n",
                "- Easy to inspect and validate manually\n",
                "\n",
                "**Disadvantages:**\n",
                "- **Verbose** and consume significantly more storage space\n",
                "- Slower to parse (need to convert strings to native types)\n",
                "\n",
                "### The Storage Cost Example\n",
                "\n",
                "Consider storing the number `1000000`:\n",
                "- **As text (CSV/JSON)**: 7 characters = **7 bytes** in ASCII\n",
                "- **As binary (Parquet)**: 32-bit integer = **4 bytes**\n",
                "\n",
                "Thatâ€™s almost **2Ã— the storage** for a single number! Now multiply this across billions of rows.\n",
                "\n",
                "```\n",
                "  Storing the integer 1000000:\n",
                "\n",
                "  TEXT:   ['1'] ['0'] ['0'] ['0'] ['0'] ['0'] ['0']   = 7 bytes\n",
                "  BINARY: [00000000 00001111 01000010 01000000]        = 4 bytes\n",
                "\n",
                "  Saving: ~43%  â†’  At 1 billion rows, thatâ€™s GIGABYTES saved.\n",
                "```\n",
                "\n",
                "### 3.2 Binary Formats (Parquet, Avro)\n",
                "\n",
                "Binary formats are **not human-readable** and are designed for machine consumption.\n",
                "\n",
                "**Advantages:**\n",
                "- Far more **compact** and efficient to process\n",
                "- Schema is embedded (self-describing)\n",
                "- Support efficient compression\n",
                "\n",
                "**Disadvantages:**\n",
                "- Cannot inspect manually (need special tools)\n",
                "- A program must know the exact schema and layout to interpret the bytes\n",
                "\n",
                "### Real-World Size Comparison\n",
                "\n",
                "> A **14 MB CSV file** can be brought down to **6 MB** when converted to the binary Parquet format. For large-scale analytical workloads, binary formats like Parquet are the **industry standard**.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Formats: Row-Major vs Column-Major <a id='4-row-vs-column'></a>\n",
                "\n",
                "This distinction is perhaps the **most critical** for an ML engineer to grasp, as it directly relates to how we typically access data for training and analysis.\n",
                "\n",
                "### Memory Layout Comparison\n",
                "\n",
                "```\n",
                "  Original Table:\n",
                "  â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”\n",
                "  â”‚ ID â”‚ Name â”‚ Age â”‚\n",
                "  â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤\n",
                "  â”‚  1 â”‚ John â”‚  30 â”‚\n",
                "  â”‚  2 â”‚ Jane â”‚  25 â”‚\n",
                "  â”‚  3 â”‚ Bob  â”‚  35 â”‚\n",
                "  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜\n",
                "\n",
                "  ROW-MAJOR (CSV, most databases):\n",
                "  Memory: [1,John,30] [2,Jane,25] [3,Bob,35]\n",
                "          â””â”€â”€ Row 1 â”€â”˜ â””â”€â”€ Row 2 â”€â”˜ â””â”€ Row 3 â”€â”˜\n",
                "  â†’ Good for: Reading entire rows (OLTP, point lookups)\n",
                "\n",
                "  COLUMN-MAJOR (Parquet, Pandas DataFrame):\n",
                "  Memory: [1,2,3] [John,Jane,Bob] [30,25,35]\n",
                "          â””â”€ IDs â”€â”˜ â””â”€â”€â”€ Names â”€â”€â”€â”˜ â””â”€ Ages â”€â”˜\n",
                "  â†’ Good for: Reading specific columns (OLAP, ML, analytics)\n",
                "```\n",
                "\n",
                "### 4.1 Row-Major Format (CSV, most databases)\n",
                "\n",
                "Consecutive elements of a **row** are stored next to each other.\n",
                "\n",
                "**Optimized for:**\n",
                "- **Write-heavy workloads** â€” adding new, complete records\n",
                "- Retrieving **entire samples** (e.g., fetching all data for a specific user ID)\n",
                "- Good for **OLTP** (Online Transaction Processing) systems\n",
                "\n",
                "### 4.2 Column-Major Format (Parquet, Pandas DataFrame)\n",
                "\n",
                "Consecutive elements of a **column** are stored next to each other.\n",
                "\n",
                "**Optimized for:**\n",
                "- **Analytical queries** â€” common in ML (e.g., `SELECT AVG(age) FROM users`)\n",
                "- **Column pruning** â€” read only the columns you need\n",
                "- **Compression** â€” similar values in a column compress better\n",
                "- Good for **OLAP** (Online Analytical Processing) systems\n",
                "\n",
                "### Why Column-Major Matters for ML\n",
                "\n",
                "When training an ML model, you typically:\n",
                "1. Select specific **features** (columns), not entire rows\n",
                "2. Compute **statistics** per column (mean, std for normalization)\n",
                "3. Apply **transformations** column-wise\n",
                "\n",
                "Column-major format allows reading that one column as a **single, contiguous block of memory** â€” extremely efficient and cache-friendly.\n",
                "\n",
                "### The ~20Ã— Performance Difference\n",
                "\n",
                "```\n",
                "  Column-major iteration (Pandas):\n",
                "  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”   Contiguous in memory\n",
                "  â”‚ 30â”‚25â”‚35â”‚28â”‚41â”‚   â†’ CPU cache HITS âš¡\n",
                "  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜   ~2 microseconds per column\n",
                "\n",
                "  Row-major iteration:\n",
                "  â”Œâ”€â”€â”€â”     â”Œâ”€â”€â”€â”     â”Œâ”€â”€â”€â”   Scattered in memory\n",
                "  â”‚ 30â”‚ ... â”‚ 25â”‚ ... â”‚ 35â”‚   â†’ CPU cache MISSES ğŸ¢\n",
                "  â””â”€â”€â”€â”˜     â””â”€â”€â”€â”˜     â””â”€â”€â”€â”˜   ~38 microseconds per row\n",
                "\n",
                "  Result: ~20Ã— slower for row-wise access!\n",
                "```\n",
                "\n",
                "The popular `pandas` library is built around the **column-major DataFrame**. Iterating row-by-row vs column-by-column reveals a dramatic performance gap:\n",
                "\n",
                "- Iterating **32M+ rows by column**: ~2 microseconds\n",
                "- Iterating **32M+ rows by row**: ~38 microseconds\n",
                "- Thatâ€™s a **~20Ã— difference**!\n",
                "\n",
                "This happens because:\n",
                "- Individual columns may be spread across different memory locations\n",
                "- But elements **within each column** are ALWAYS contiguous\n",
                "- Processors are much more efficient with **contiguous blocks of memory** (cache hits vs cache misses)\n",
                "- When iterating rows, the processor must jump from one memory location to another for each row element (cache misses)\n",
                "\n",
                "> This isnâ€™t a flaw in pandas; itâ€™s a **direct consequence** of its underlying column-major data model.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Column-wise sum (all 1,000,000 rows, 4 cols): 0.0213s\n",
                        "Row-wise access (10,000 rows):                  0.1760s\n",
                        "\n",
                        "Row access is ~8x slower (even on 100x fewer iterations!)\n",
                        "\n",
                        ">>> Takeaway: NEVER iterate rows in Pandas. Use vectorized operations.\n"
                    ]
                }
            ],
            "source": [
                "# Demonstration: Row vs Column iteration speed in Pandas\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import time\n",
                "\n",
                "# Create a large DataFrame\n",
                "n_rows = 1_000_000\n",
                "df = pd.DataFrame({\n",
                "    'col_a': np.random.randn(n_rows),\n",
                "    'col_b': np.random.randn(n_rows),\n",
                "    'col_c': np.random.randn(n_rows),\n",
                "    'col_d': np.random.randn(n_rows),\n",
                "})\n",
                "\n",
                "# Column-wise access (fast - contiguous memory)\n",
                "start = time.time()\n",
                "for col in df.columns:\n",
                "    _ = df[col].sum()\n",
                "col_time = time.time() - start\n",
                "\n",
                "# Row-wise access (slow - non-contiguous memory)\n",
                "start = time.time()\n",
                "for idx in range(min(10000, n_rows)):  # Only 10K rows to keep demo fast\n",
                "    _ = df.iloc[idx]\n",
                "row_time = time.time() - start\n",
                "\n",
                "print(f\"Column-wise sum (all {n_rows:,} rows, 4 cols): {col_time:.4f}s\")\n",
                "print(f\"Row-wise access (10,000 rows):                  {row_time:.4f}s\")\n",
                "print(f\"\\nRow access is ~{row_time/col_time:.0f}x slower (even on 100x fewer iterations!)\")\n",
                "print(\"\\n>>> Takeaway: NEVER iterate rows in Pandas. Use vectorized operations.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Format Comparison Table <a id='5-format-comparison'></a>\n",
                "\n",
                "| Feature | CSV | Parquet | Avro | JSON |\n",
                "|---------|-----|---------|------|------|\n",
                "| **Type** | Text (Human Readable) | Binary (Columnar) | Binary (Row-based) | Text (Human Readable) |\n",
                "| **Schema** | None (Inferred) | Embedded in footer | JSON Schema in header | None (Self-describing) |\n",
                "| **Orientation** | Row-major | Column-major | Row-major | Document-oriented |\n",
                "| **Use Case** | Small data, Excel | Analytics, ML training | Streaming (Kafka) | APIs, config, logs |\n",
                "| **Compression** | Poor | Excellent (Snappy/Gzip) | Good | Poor |\n",
                "| **Write Speed** | Slow | Slow (encoding overhead) | Fast | Moderate |\n",
                "| **Read Speed** | Very Slow | Very Fast (column pruning) | Fast | Slow (parsing) |\n",
                "| **Schema Evolution** | None | Limited | Excellent | None |\n",
                "| **Splittable** | Yes | Yes | Yes | Line-delimited only |\n",
                "| **Ecosystem** | Universal | Spark, Pandas, Arrow | Kafka, Hadoop | Universal |\n",
                "\n",
                "### Quick Decision Guide\n",
                "\n",
                "```\n",
                "  What format should I use?\n",
                "\n",
                "  ML Training on tabular data?    â”€â”€â–¶  PARQUET  (always)\n",
                "  Streaming pipeline (Kafka)?      â”€â”€â–¶  AVRO  (schema evolution + row writes)\n",
                "  API data interchange?            â”€â”€â–¶  JSON  (universal, flexible)\n",
                "  Quick exploration / non-tech?    â”€â”€â–¶  CSV   (but convert to Parquet later)\n",
                "```\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Hands-On: Benchmarking Performance <a id='6-benchmarking'></a>\n",
                "\n",
                "Let's generate a large dataset and compare **CSV vs Parquet** on three dimensions:\n",
                "1. **Write speed**\n",
                "2. **Read speed**\n",
                "3. **File size**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating synthetic dataset with 1M rows...\n",
                        "Dataset shape: (1000000, 7)\n",
                        "Memory usage: 88.5 MB\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>user_id</th>\n",
                            "      <th>category</th>\n",
                            "      <th>price</th>\n",
                            "      <th>quantity</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>is_returned</th>\n",
                            "      <th>timestamp</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>0</td>\n",
                            "      <td>books</td>\n",
                            "      <td>103.96</td>\n",
                            "      <td>10</td>\n",
                            "      <td>3.9</td>\n",
                            "      <td>False</td>\n",
                            "      <td>2023-01-01 00:00:00</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>1</td>\n",
                            "      <td>clothing</td>\n",
                            "      <td>326.23</td>\n",
                            "      <td>2</td>\n",
                            "      <td>1.8</td>\n",
                            "      <td>False</td>\n",
                            "      <td>2023-01-01 00:00:01</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>2</td>\n",
                            "      <td>clothing</td>\n",
                            "      <td>124.50</td>\n",
                            "      <td>1</td>\n",
                            "      <td>2.2</td>\n",
                            "      <td>False</td>\n",
                            "      <td>2023-01-01 00:00:02</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>3</td>\n",
                            "      <td>books</td>\n",
                            "      <td>152.62</td>\n",
                            "      <td>2</td>\n",
                            "      <td>2.9</td>\n",
                            "      <td>False</td>\n",
                            "      <td>2023-01-01 00:00:03</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>4</td>\n",
                            "      <td>food</td>\n",
                            "      <td>101.91</td>\n",
                            "      <td>11</td>\n",
                            "      <td>1.7</td>\n",
                            "      <td>False</td>\n",
                            "      <td>2023-01-01 00:00:04</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   user_id  category   price  quantity  rating  is_returned  \\\n",
                            "0        0     books  103.96        10     3.9        False   \n",
                            "1        1  clothing  326.23         2     1.8        False   \n",
                            "2        2  clothing  124.50         1     2.2        False   \n",
                            "3        3     books  152.62         2     2.9        False   \n",
                            "4        4      food  101.91        11     1.7        False   \n",
                            "\n",
                            "            timestamp  \n",
                            "0 2023-01-01 00:00:00  \n",
                            "1 2023-01-01 00:00:01  \n",
                            "2 2023-01-01 00:00:02  \n",
                            "3 2023-01-01 00:00:03  \n",
                            "4 2023-01-01 00:00:04  "
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import time\n",
                "import os\n",
                "\n",
                "# ============================================================\n",
                "# Step 1: Generate a \"Large\" synthetic dataset (1 Million rows)\n",
                "# ============================================================\n",
                "print(\"Generating synthetic dataset with 1M rows...\")\n",
                "n_rows = 1_000_000\n",
                "\n",
                "df = pd.DataFrame({\n",
                "    'user_id': np.arange(n_rows),\n",
                "    'category': np.random.choice(['electronics', 'clothing', 'food', 'books', 'sports'], n_rows),\n",
                "    'price': np.round(np.random.uniform(1, 500, n_rows), 2),\n",
                "    'quantity': np.random.randint(1, 20, n_rows),\n",
                "    'rating': np.round(np.random.uniform(1, 5, n_rows), 1),\n",
                "    'is_returned': np.random.choice([True, False], n_rows, p=[0.05, 0.95]),\n",
                "    'timestamp': pd.date_range('2023-01-01', periods=n_rows, freq='s')\n",
                "})\n",
                "\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / (1024**2):.1f} MB\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Benchmarking CSV...\n",
                        "  Write: 13.07s | Read: 3.58s | Size: 52.3 MB\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Step 2: Benchmark CSV\n",
                "# ============================================================\n",
                "print(\"Benchmarking CSV...\")\n",
                "\n",
                "# Write\n",
                "start = time.time()\n",
                "df.to_csv('benchmark_data.csv', index=False)\n",
                "write_csv = time.time() - start\n",
                "\n",
                "# Read\n",
                "start = time.time()\n",
                "df_csv = pd.read_csv('benchmark_data.csv')\n",
                "read_csv = time.time() - start\n",
                "\n",
                "# File size\n",
                "size_csv = os.path.getsize('benchmark_data.csv') / (1024 * 1024)\n",
                "\n",
                "print(f\"  Write: {write_csv:.2f}s | Read: {read_csv:.2f}s | Size: {size_csv:.1f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Benchmarking Parquet (Snappy)...\n",
                        "  Write: 1.86s | Read: 0.87s | Size: 14.2 MB\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Step 3: Benchmark Parquet (Snappy compression - default)\n",
                "# ============================================================\n",
                "print(\"Benchmarking Parquet (Snappy)...\")\n",
                "\n",
                "# Write\n",
                "start = time.time()\n",
                "df.to_parquet('benchmark_data.parquet', index=False, compression='snappy')\n",
                "write_pq = time.time() - start\n",
                "\n",
                "# Read\n",
                "start = time.time()\n",
                "df_pq = pd.read_parquet('benchmark_data.parquet')\n",
                "read_pq = time.time() - start\n",
                "\n",
                "# File size\n",
                "size_pq = os.path.getsize('benchmark_data.parquet') / (1024 * 1024)\n",
                "\n",
                "print(f\"  Write: {write_pq:.2f}s | Read: {read_pq:.2f}s | Size: {size_pq:.1f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "BENCHMARK RESULTS: CSV vs Parquet (1M rows)\n",
                        "============================================================\n",
                        "Metric               CSV             Parquet         Winner         \n",
                        "------------------------------------------------------------\n",
                        "Write Time (s)       13.07           1.86            Parquet\n",
                        "Read Time (s)        3.58            0.87            Parquet\n",
                        "File Size (MB)       52.3            14.2            Parquet\n",
                        "============================================================\n",
                        "\n",
                        "ğŸ“Š Parquet is 3.7x smaller and 4.1x faster to read!\n",
                        "\n",
                        ">>> For ML workloads with tabular data, ALWAYS prefer Parquet.\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Step 4: Compare Results\n",
                "# ============================================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"BENCHMARK RESULTS: CSV vs Parquet (1M rows)\")\n",
                "print(\"=\"*60)\n",
                "print(f\"{'Metric':<20} {'CSV':<15} {'Parquet':<15} {'Winner':<15}\")\n",
                "print(\"-\"*60)\n",
                "print(f\"{'Write Time (s)':<20} {write_csv:<15.2f} {write_pq:<15.2f} {'Parquet' if write_pq < write_csv else 'CSV'}\")\n",
                "print(f\"{'Read Time (s)':<20} {read_csv:<15.2f} {read_pq:<15.2f} {'Parquet' if read_pq < read_csv else 'CSV'}\")\n",
                "print(f\"{'File Size (MB)':<20} {size_csv:<15.1f} {size_pq:<15.1f} {'Parquet' if size_pq < size_csv else 'CSV'}\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nğŸ“Š Parquet is {size_csv/size_pq:.1f}x smaller and {read_csv/read_pq:.1f}x faster to read!\")\n",
                "print(\"\\n>>> For ML workloads with tabular data, ALWAYS prefer Parquet.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Compression Strategies <a id='7-compression'></a>\n",
                "\n",
                "Parquet supports multiple compression codecs. The two most common are:\n",
                "\n",
                "| Codec | Speed | Compression Ratio | Use Case |\n",
                "|-------|-------|-------------------|----------|\n",
                "| **Snappy** | âš¡ Very Fast | Moderate | Default. Best for interactive/Spark workloads |\n",
                "| **Gzip** | ğŸ¢ Slower | High | Best for archival / cold storage |\n",
                "| **Zstd** | âš¡ Fast | High | Best of both worlds (newer) |\n",
                "| **LZ4** | âš¡âš¡ Fastest | Low-Moderate | When speed is everything |\n",
                "\n",
                "### Rule of Thumb\n",
                "\n",
                "```\n",
                "  Hot data (frequently accessed)  â”€â”€â–¶  Snappy or LZ4\n",
                "  Cold data (archival)            â”€â”€â–¶  Gzip or Zstd\n",
                "  Spark workloads                 â”€â”€â–¶  Snappy (default for good reason)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=================================================================\n",
                        "COMPRESSION COMPARISON (Parquet, 1M rows)\n",
                        "=================================================================\n",
                        "Codec        Write(s)     Read(s)      Size(MB)    \n",
                        "-----------------------------------------------------------------\n",
                        "snappy       0.480        0.162        14.2        \n",
                        "gzip         14.655       0.173        9.3         \n",
                        "none         0.544        0.276        19.9        \n",
                        "\n",
                        ">>> Snappy: best balance. Gzip: smallest files but slower. None: fastest but largest.\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Compression Comparison: Snappy vs Gzip vs None\n",
                "# ============================================================\n",
                "results = {}\n",
                "\n",
                "for codec in ['snappy', 'gzip', None]:\n",
                "    label = codec if codec else 'none'\n",
                "    fname = f'benchmark_{label}.parquet'\n",
                "    \n",
                "    # Write\n",
                "    start = time.time()\n",
                "    df.to_parquet(fname, index=False, compression=codec)\n",
                "    w_time = time.time() - start\n",
                "    \n",
                "    # Read\n",
                "    start = time.time()\n",
                "    pd.read_parquet(fname)\n",
                "    r_time = time.time() - start\n",
                "    \n",
                "    size = os.path.getsize(fname) / (1024 * 1024)\n",
                "    results[label] = {'write': w_time, 'read': r_time, 'size': size}\n",
                "    os.remove(fname)\n",
                "\n",
                "print(\"\\n\" + \"=\"*65)\n",
                "print(\"COMPRESSION COMPARISON (Parquet, 1M rows)\")\n",
                "print(\"=\"*65)\n",
                "print(f\"{'Codec':<12} {'Write(s)':<12} {'Read(s)':<12} {'Size(MB)':<12}\")\n",
                "print(\"-\"*65)\n",
                "for codec, r in results.items():\n",
                "    print(f\"{codec:<12} {r['write']:<12.3f} {r['read']:<12.3f} {r['size']:<12.1f}\")\n",
                "print(\"\\n>>> Snappy: best balance. Gzip: smallest files but slower. None: fastest but largest.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Column Pruning Demo <a id='8-column-pruning'></a>\n",
                "\n",
                "One of Parquetâ€™s killer features is **column pruning**: reading ONLY the columns you need.\n",
                "\n",
                "```\n",
                "  CSV: Must read ALL columns, then filter\n",
                "  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "  â”‚ uid â”‚ cat  â”‚priceâ”‚qty â”‚ratingâ”‚ ret â”‚timestamp â”‚  â† Read ALL (wasteful)\n",
                "  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "\n",
                "  Parquet: Read ONLY requested columns\n",
                "  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”\n",
                "  â”‚priceâ”‚ratingâ”‚  â† Read ONLY 2 columns (fast!)\n",
                "  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\n",
                "  (other 5 columns never touch disk)\n",
                "```\n",
                "\n",
                "In CSV, you must read the **entire file** even if you only need 2 columns out of 100. With Parquet, you read precisely the columns requested â€” the rest are never loaded from disk."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Reading 2 out of 7 columns:\n",
                        "  CSV (usecols):     0.634s\n",
                        "  Parquet (columns): 0.041s\n",
                        "  Speedup:           15.5x faster with Parquet\n",
                        "\n",
                        ">>> This gap grows MASSIVELY with more columns (imagine 500 columns, reading 10)\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Column Pruning: Read only specific columns\n",
                "# ============================================================\n",
                "\n",
                "# Save both formats\n",
                "df.to_csv('benchmark_data.csv', index=False)\n",
                "df.to_parquet('benchmark_data.parquet', index=False)\n",
                "\n",
                "# --- CSV: Must read ALL columns, then select ---\n",
                "start = time.time()\n",
                "df_csv_subset = pd.read_csv('benchmark_data.csv', usecols=['price', 'rating'])\n",
                "csv_prune_time = time.time() - start\n",
                "\n",
                "# --- Parquet: Reads ONLY the requested columns from disk ---\n",
                "start = time.time()\n",
                "df_pq_subset = pd.read_parquet('benchmark_data.parquet', columns=['price', 'rating'])\n",
                "pq_prune_time = time.time() - start\n",
                "\n",
                "print(f\"Reading 2 out of 7 columns:\")\n",
                "print(f\"  CSV (usecols):     {csv_prune_time:.3f}s\")\n",
                "print(f\"  Parquet (columns): {pq_prune_time:.3f}s\")\n",
                "print(f\"  Speedup:           {csv_prune_time/pq_prune_time:.1f}x faster with Parquet\")\n",
                "print(f\"\\n>>> This gap grows MASSIVELY with more columns (imagine 500 columns, reading 10)\")\n",
                "\n",
                "# Cleanup\n",
                "os.remove('benchmark_data.csv')\n",
                "os.remove('benchmark_data.parquet')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Exercises <a id='9-exercises'></a>\n",
                "\n",
                "### Exercise 1: Schema Impact\n",
                "Create a DataFrame with mixed types (int, float, string, datetime). Save it as CSV and read it back. Compare the dtypes before and after. What happens? Why is this a problem for ML pipelines?\n",
                "\n",
                "### Exercise 2: JSON Benchmarking  \n",
                "Extend the benchmarking code to include **JSON Lines** format (`df.to_json('data.jsonl', orient='records', lines=True)`). How does it compare to CSV and Parquet?\n",
                "\n",
                "### Exercise 3: Real-World Scenario\n",
                "Youâ€™re building an ML pipeline that:\n",
                "- Receives 10M user events per day as JSON from an API\n",
                "- Needs to store 6 months of history for training\n",
                "- Training reads only 5 out of 30 columns\n",
                "\n",
                "Write pseudocode for how youâ€™d architect the storage layer. What format for ingestion? What format for storage? Why?\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Interview Preparation <a id='10-interview'></a>\n",
                "\n",
                "### Q1: \"You need to store 1TB of tabular data for ML training. CSV or Parquet? Why?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"Parquet, without question. Four key reasons:\n",
                "1. **Columnar Storage**: ML training typically selects specific features (columns). Parquet allows column pruning â€” reading only the needed columns, saving enormous I/O.\n",
                "2. **Compression**: Parquet with Snappy compression can reduce 1TB to ~200-300GB. Columnar data compresses far better because similar values are stored together.\n",
                "3. **Schema Enforcement**: Parquet embeds the schema (dtypes), preventing â€˜everything is a stringâ€™ issues common in CSV that cause silent ML bugs.\n",
                "4. **Splittable**: Compressed Parquet files can be processed in parallel by Spark executors, enabling distributed training.\n",
                "\n",
                "Iâ€™d use CSV only for small, human-inspectable files or when sharing data with non-technical stakeholders.\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q2: \"Explain the difference between row-major and column-major storage. When would you choose each?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"Row-major stores consecutive elements of each row together (CSV, relational DBs). Column-major stores consecutive elements of each column together (Parquet, Pandas DataFrames).\n",
                "\n",
                "Row-major excels at:\n",
                "- **OLTP workloads**: Inserting/updating individual records\n",
                "- **Point lookups**: Fetching all data for one user\n",
                "\n",
                "Column-major excels at:\n",
                "- **OLAP/ML workloads**: Computing statistics across millions of records\n",
                "- **Feature selection**: Reading only needed columns from disk\n",
                "- **Compression**: Similar values in a column compress very well\n",
                "\n",
                "For ML, column-major is almost always preferred because we read many rows but few columns. The ~20Ã— performance difference in Pandas column vs row iteration demonstrates this concretely.\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q3: \"When would you use Avro instead of Parquet?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"Iâ€™d prioritize Avro for **streaming data pipelines**, especially with Kafka. Key reasons:\n",
                "1. **Row-based**: Efficient for writing records one at a time (append-heavy)\n",
                "2. **Schema evolution**: Avro handles adding/removing fields gracefully, which is crucial for long-running producers/consumers where schemas evolve over time\n",
                "3. **Compact serialization**: Good for message passing between microservices\n",
                "\n",
                "My typical pattern: ingest with Avro (streaming) â†’ land in a data lake â†’ convert to Parquet (analytics/training).\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q4: \"Explain Dictionary Encoding in Parquet and why it matters for ML data.\"\n",
                "\n",
                "**Answer:**  \n",
                "\"For categorical columns with low cardinality (few unique values), Parquet stores a dictionary of values (e.g., `['electronics', 'clothing', 'food']`) and replaces row values with tiny integer indices (e.g., `0, 1, 2`).\n",
                "\n",
                "This provides:\n",
                "1. **Massive compression** for categorical features common in ML datasets\n",
                "2. **Faster filtering** since comparisons happen on small integers\n",
                "3. **Reduced memory footprint** when loading into memory\n",
                "\n",
                "This is especially impactful in ML where categorical features like `country`, `device_type`, `product_category` appear in nearly every dataset.\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q5: \"How would you design a data storage strategy for a system with both real-time serving and batch training needs?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"Iâ€™d use a **Lambda Architecture** approach:\n",
                "\n",
                "1. **Ingestion Layer**: Events arrive as JSON/Avro via Kafka (real-time)\n",
                "2. **Speed Layer** (serving): Store in a low-latency database (Redis, DynamoDB) for real-time feature serving\n",
                "3. **Batch Layer** (training): Land raw events in a data lake (S3/GCS) as Parquet files, partitioned by date\n",
                "4. **Serving Layer**: Pre-computed features for model serving\n",
                "\n",
                "For ML specifically:\n",
                "- Training reads from Parquet in the batch layer (column pruning, compression)\n",
                "- Serving reads from the speed layer (low-latency lookups)\n",
                "- A feature store (like Feast) bridges both, ensuring training-serving consistency.\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ğŸ“ Key Takeaways\n",
                "\n",
                "1. **Data is not a clean CSV** â€” production data comes from user input, logs, databases, and third parties, each with different reliability and structure\n",
                "2. **Binary > Text** for large-scale processing (Parquet over CSV, always)\n",
                "3. **Column-major > Row-major** for ML workloads (feature selection, statistics)\n",
                "4. **Parquet is the gold standard** for ML training data storage\n",
                "5. **Snappy compression** is the default for good reason (fast + decent ratio)\n",
                "6. **Column pruning** provides massive I/O savings when you only need a few features\n",
                "\n",
                "---\n",
                "\n",
                "â¡ï¸ **Next Lesson**: [Lesson 2: ETL/ELT Pipelines](./lesson_02_etl_pipelines.ipynb) â€” Learn how to build production data pipelines."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
