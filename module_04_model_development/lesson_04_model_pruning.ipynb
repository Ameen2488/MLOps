{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 4: Model Pruning\n",
                "\n",
                "**Module 4: Model Development & Optimization**  \n",
                "**Estimated Time**: 1 hour  \n",
                "**Difficulty**: Advanced\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "âœ… Understand the difference between Structured and Unstructured Pruning  \n",
                "âœ… Learn why sparse matrices don't always speed up inference  \n",
                "âœ… Implement Pruning using `torch.nn.utils.prune`  \n",
                "âœ… Answer interview questions on model compression  \n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“š Table of Contents\n",
                "\n",
                "1. [Why Prune? (The Lottery Ticket Hypothesis)](#1-why)\n",
                "2. [Unstructured vs Structured Pruning](#2-types)\n",
                "3. [Hands-On: Pruning a PyTorch Linear Layer](#3-hands-on)\n",
                "4. [Interview Preparation](#4-interview-questions)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Why Prune?\n",
                "\n",
                "**Insight**: Neural Networks are massively over-parameterized. 90% of the weights in a ResNet-50 might not be contributing much to the prediction.\n",
                "\n",
                "**Pruning**: Setting \"unimportant\" weights to Zero.\n",
                "\n",
                "**Goal**: Reduce model size (Memory) and inference time (Latency)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Unstructured vs Structured Pruning\n",
                "\n",
                "### Unstructured Pruning\n",
                "- Set individual weights to 0 (e.g., \"Remove any weight < 0.01\").\n",
                "- **Result**: A random sparse matrix.\n",
                "- **Problem**: GPUs/CPUs rely on dense matrix multiplication. Random zeros don't help speed unless you have specialized hardware.\n",
                "\n",
                "### Structured Pruning\n",
                "- Remove entire **Rows, Columns, or Channels** (e.g., \"Remove kernel 5 in Conv layer 1\").\n",
                "- **Result**: A smaller dense matrix.\n",
                "- **Benefit**: Real speedup on standard hardware.\n",
                "- **Risk**: Higher impact on accuracy (removing a whole channel is aggressive)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Hands-On: Pruning a PyTorch Linear Layer\n",
                "\n",
                "Using PyTorch's built-in APIs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.utils.prune as prune\n",
                "\n",
                "# 1. Create a model\n",
                "module = nn.Linear(5, 5)\n",
                "print(\"Original Weights:\")\n",
                "print(module.weight)\n",
                "\n",
                "# 2. Unstructured Pruning (L1 Unstructured)\n",
                "# Prune 30% of connections with lowest magnitude\n",
                "prune.l1_unstructured(module, name=\"weight\", amount=0.3)\n",
                "\n",
                "print(\"\\nPruned Weights (Note the Mask):\")\n",
                "print(module.weight)\n",
                "\n",
                "# 3. Check Sparsity\n",
                "zeros = torch.sum(module.weight == 0)\n",
                "total = module.weight.nelement()\n",
                "print(f\"\\nSparsity: {zeros}/{total} ({zeros/total:.0%})\")\n",
                "\n",
                "# 4. Making it Permanent\n",
                "# Pruning in PyTorch applies a mask. To save space, we must apply it permanently.\n",
                "prune.remove(module, 'weight')\n",
                "print(\"\\nPruning made permanent (Mask removed, weights are actually zeroed)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Interview Preparation\n",
                "\n",
                "### Common Questions\n",
                "\n",
                "#### Q1: \"If I prune 50% of weights, is my model 2x faster?\"\n",
                "**Answer**: \"Usually NOT, if using unstructured pruning. Standard matrix libraries (BLAS/CuBLAS) still multiply the zeros. You only get speedups with **Structured Pruning** (changing physical shape) or specialized sparse inference engines (e.g., Neural Magic).\"\n",
                "\n",
                "#### Q2: \"What is Iterative Pruning?\"\n",
                "**Answer**: \"Instead of pruning 50% at once (which kills accuracy), prune 10%, retrain (fine-tune) to recover accuracy, prune another 10%, retrain, and repeat. This allows the network to adapt to the loss of capacity.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}