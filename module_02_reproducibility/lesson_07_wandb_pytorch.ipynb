{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 7: W&B with PyTorch\n",
                "\n",
                "**Module 2: Reproducibility & Versioning**  \n",
                "**Estimated Time**: 2-3 hours  \n",
                "**Difficulty**: Intermediate\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "âœ… Track PyTorch gradients and weights with `wandb.watch`  \n",
                "âœ… Log custom media (Images) to debug predictions  \n",
                "âœ… Configure Hyperparameters properly with `wandb.config`  \n",
                "âœ… Answer interview questions on debugging Neural Networks  \n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“š Table of Contents\n",
                "\n",
                "1. [Why Watch Gradients?](#1-gradients)\n",
                "2. [The `wandb.watch` Magic](#2-wandb-watch)\n",
                "3. [Logging Predictions as Images](#3-logging-images)\n",
                "4. [Hands-On: PyTorch Setup](#4-hands-on)\n",
                "5. [Interview Preparation](#5-interview-questions)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Why Watch Gradients?\n",
                "\n",
                "In Deep Learning, **Vanishing Gradients** or **Exploding Gradients** are common silent failures. Your loss curve might look \"okay\" (flat), but your model isn't learning because gradients are zero.\n",
                "\n",
                "Visualizing the distribution of gradients across layers helps you catch this early."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. The `wandb.watch` Magic\n",
                "\n",
                "```python\n",
                "wandb.watch(model, log=\"all\", log_freq=10)\n",
                "```\n",
                "\n",
                "This one line hooks into your model and logs histograms of:\n",
                "- Weights (are they too large?)\n",
                "- Gradients (are they zero?)\n",
                "\n",
                "You call this **before** the training loop."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Logging Predictions as Images\n",
                "\n",
                "Numbers (Accuracy) don't tell the whole story. You want to see **where** the model failed.\n",
                "\n",
                "```python\n",
                "wandb.log({\"examples\": [wandb.Image(x, caption=f\"Pred: {y_pred}\")]})\n",
                "```\n",
                "\n",
                "This creates a gallery in the dashboard."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Hands-On: PyTorch Setup\n",
                "\n",
                "Simulating a CNN training loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import wandb\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "\n",
                "print(\"Ensure 'wandb login' is done.\")\n",
                "\n",
                "# 1. Define Model\n",
                "class SimpleCNN(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
                "        self.fc = nn.Linear(5760, 10)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = torch.relu(self.conv1(x))\n",
                "        x = x.view(x.size(0), -1)\n",
                "        x = self.fc(x)\n",
                "        return x\n",
                "\n",
                "# 2. Training Code\n",
                "def train():\n",
                "    # Standard Config approach\n",
                "    config = {\n",
                "        \"batch_size\": 64,\n",
                "        \"learning_rate\": 0.01,\n",
                "        \"epochs\": 5\n",
                "    }\n",
                "    \n",
                "    with wandb.init(project=\"pytorch-demo\", config=config):\n",
                "        config = wandb.config\n",
                "        \n",
                "        model = SimpleCNN()\n",
                "        optimizer = optim.SGD(model.parameters(), lr=config.learning_rate)\n",
                "        criterion = nn.CrossEntropyLoss()\n",
                "        \n",
                "        # MAGIC LINE: Watch gradients\n",
                "        wandb.watch(model, log=\"all\", log_freq=1)\n",
                "        \n",
                "        # Fake Data (28x28 images)\n",
                "        inputs = torch.randn(64, 1, 28, 28)\n",
                "        labels = torch.randint(0, 10, (64,))\n",
                "        \n",
                "        print(\"Starting training...\")\n",
                "        for epoch in range(config.epochs):\n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(inputs)\n",
                "            loss = criterion(outputs, labels)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            # Log metrics\n",
                "            wandb.log({\"loss\": loss.item(), \"epoch\": epoch})\n",
                "            \n",
                "            # Log sample image every epoch\n",
                "            if epoch % 2 == 0:\n",
                "                # Log the first image of the batch as an example\n",
                "                img_tensor = inputs[0]\n",
                "                wandb.log({\"example_img\": wandb.Image(img_tensor, caption=f\"Epoch {epoch}\")})\n",
                "\n",
                "    print(\"Finished. Check dashboard for Gradients and Images!\")\n",
                "\n",
                "train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Interview Preparation\n",
                "\n",
                "### Common Questions\n",
                "\n",
                "#### Q1: \"How do you debug a neural network that isn't learning?\"\n",
                "**Answer**: \"I look at the gradient distribution. I use `wandb.watch` to visualize histograms of gradients for each layer. If the histograms are concentrated at zero (vanishing) or extremely wide (exploding), I know I need to adjust initialization, activation functions, or learning rate.\"\n",
                "\n",
                "#### Q2: \"How to you monitor data quality during training?\"\n",
                "**Answer**: \"I log sample predictions using `wandb.log({'image': ...})`. By visually inspecting what the model thinks is a 'cat' vs 'dog' during training, I might discover that my preprocessing pipeline is corrupting images (e.g., wrong normalization colors).\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}