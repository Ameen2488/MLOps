{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 8: ONNX Runtime\n",
                "\n",
                "**Module 4: Model Development & Optimization**  \n",
                "**Estimated Time**: 1-2 hours  \n",
                "**Difficulty**: Advanced\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "âœ… Compare PyTorch Inference vs ONNX Runtime (ORT)  \n",
                "âœ… Understand Graph Optimizations (Constant Folding, Fusion)  \n",
                "âœ… Use Execution Providers (CUDA, TensorRT, OpenVINO)  \n",
                "âœ… Answer interview questions on inference acceleration  \n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“š Table of Contents\n",
                "\n",
                "1. [What is ONNX Runtime?](#1-what-is-ort)\n",
                "2. [The Magic: Graph Optimizations](#2-optimizations)\n",
                "3. [Hands-On: Benchmarking PyTorch vs ORT](#3-hands-on)\n",
                "4. [Interview Preparation](#4-interview-questions)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. What is ONNX Runtime?\n",
                "\n",
                "ONNX Runtime (ORT) is a high-performance inference engine developed by Microsoft.\n",
                "\n",
                "**Why use it?**\n",
                "- **Faster**: Often 2x-10x faster than pure PyTorch.\n",
                "- **Hardware Support**: One API for CPU, NVIDIA Models, Intel OpenVINO, Android NNAPI.\n",
                "- **Lightweight**: No training overhead code."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. The Magic: Graph Optimizations\n",
                "\n",
                "ORT applies compiler optimizations to your model graph:\n",
                "\n",
                "1. **Constant Folding**: Pre-calculating static math (e.g., `3 + 5` -> `8`).\n",
                "2. **Operator Fusion**: Combining layers (e.g., `Conv + BatchNormalization + ReLU` -> `FusedConvBNReLU`). This reduces memory access steps.\n",
                "3. **Memory Planning**: Reusing memory buffers efficiently."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Hands-On: Benchmarking PyTorch vs ORT\n",
                "\n",
                "Requires `pip install onnxruntime`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import onnxruntime as ort\n",
                "import numpy as np\n",
                "import time\n",
                "\n",
                "# 1. Setup Data\n",
                "input_data = np.random.randn(1, 10).astype(np.float32)\n",
                "input_tensor = torch.from_numpy(input_data)\n",
                "\n",
                "# 2. Benchmark PyTorch\n",
                "model = torch.load(\"model.pt\") if False else None # Dummy placeholders\n",
                "# Assume model is loaded from Lesson 7 export\n",
                "\n",
                "start = time.time()\n",
                "# PyTorch Inference loop (Simulation)\n",
                "time.sleep(0.01)\n",
                "pt_lat = time.time() - start\n",
                "\n",
                "# 3. Benchmark ONNX Runtime\n",
                "# Load Session\n",
                "session = ort.InferenceSession(\"model.onnx\", providers=['CPUExecutionProvider'])\n",
                "\n",
                "start = time.time()\n",
                "# ORT Inference\n",
                "outputs = session.run(['output'], {'input': input_data})\n",
                "ort_lat = time.time() - start\n",
                "\n",
                "print(f\"PyTorch Time: {pt_lat:.4f}s (Simulated)\")\n",
                "print(f\"ORT Time: {ort_lat:.4f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Interview Preparation\n",
                "\n",
                "### Common Questions\n",
                "\n",
                "#### Q1: \"How do you deploy a model to an NVIDIA GPU?\"\n",
                "**Answer**: \"I would export the model to ONNX, then run it with ONNX Runtime using the `CUDAExecutionProvider` or `TensorRTExecutionProvider`. TensorRT performs aggressive optimizations specifically for NVIDIA hardware, giving maximum throughput.\"\n",
                "\n",
                "#### Q2: \"What is quantization in ONNX Runtime?\"\n",
                "**Answer**: \"ORT supports running quantized INT8 models directly on CPUs using the VNNI instruction set (AVX512), which can give 4x speedups over FP32 on Intel CPUs.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}