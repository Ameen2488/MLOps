{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 6: Spark Fundamentals\n",
                "\n",
                "**Module 3: Data & Pipeline Engineering**  \n",
                "**Estimated Time**: 2-3 hours  \n",
                "**Difficulty**: Intermediate\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "âœ… Understand Distributed Computing concepts  \n",
                "âœ… Master the difference between **Transformations** and **Actions**  \n",
                "âœ… Explain **Lazy Evaluation** in Spark  \n",
                "âœ… Know when to switch from Pandas to PySpark  \n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“š Table of Contents\n",
                "\n",
                "1. [The limit of Pandas](#1-pandas-limit)\n",
                "2. [Spark Architecture](#2-spark-arch)\n",
                "3. [Core Concepts: Lazy Evaluation](#3-lazy-eval)\n",
                "4. [Hands-On: PySpark API Simulation](#4-hands-on)\n",
                "5. [Interview Preparation](#5-interview-questions)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Limit of Pandas\n",
                "\n",
                "**Pandas**:\n",
                "- Runs on 1 machine (Driver).\n",
                "- Loads ALL data into RAM.\n",
                "- If dataset > RAM, it crashes (`MemoryError`).\n",
                "\n",
                "**Spark**:\n",
                "- Runs on N machines (Cluster).\n",
                "- Processes data in chunks (Partitions) on disk/RAM.\n",
                "- Can parse Petabytes of data."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Spark Architecture\n",
                "\n",
                "1. **Driver**: The brain. Runs your `main()` function.\n",
                "2. **Cluster Manager**: Allocates resources (YARN, K8s).\n",
                "3. **Executors**: The workers. They hold data partitions and run tasks.\n",
                "\n",
                "**Key Idea**: You simply write code on the Driver, and Spark automatically sends code to where the data lives (Executors)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Core Concepts: Lazy Evaluation\n",
                "\n",
                "In Pandas, every line executes immediately.\n",
                "In Spark, nothing happens until you ask for a result.\n",
                "\n",
                "### Transformations (Lazy)\n",
                "- `df.filter()`, `df.select()`, `df.groupBy()`\n",
                "- Spark just records the \"Plan\" (DAG).\n",
                "\n",
                "### Actions (Eager)\n",
                "- `df.count()`, `df.show()`, `df.write()`\n",
                "- Spark optimizes the Plan (Catalyst Optimizer) and executes it.\n",
                "\n",
                "**Why?** Optimization. If you filter 1TB data then take top 5 rows, Spark finds the 5 rows without processing the full 1TB."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# NOTE: We use pyspark.sql.SparkSession in real life.\n",
                "# Here we simulate the syntax to learn the API structure.\n",
                "\n",
                "print(\"---- PySpark Simulation ----\")\n",
                "\n",
                "class MockDataFrame:\n",
                "    def __init__(self, plan=[]):\n",
                "        self.plan = plan\n",
                "    \n",
                "    def filter(self, condition):\n",
                "        print(f\"[Transform] Added Filter: {condition}\")\n",
                "        return MockDataFrame(self.plan + [f\"Filter({condition})\"])\n",
                "\n",
                "    def select(self, *cols):\n",
                "        print(f\"[Transform] Added Select: {cols}\")\n",
                "        return MockDataFrame(self.plan + [f\"Select({cols})\"])\n",
                "\n",
                "    def count(self):\n",
                "        print(\"\\n[Action] Triggered Count!\")\n",
                "        print(\"Optimizing Plan...\")\n",
                "        print(f\"Executing: {' -> '.join(self.plan)}\")\n",
                "        return 100\n",
                "\n",
                "# 1. Read Data (Lazy)\n",
                "df = MockDataFrame([\"Read(data.parquet)\"])\n",
                "\n",
                "# 2. Transformations (Lazy - nothing calculates yet)\n",
                "df_filtered = df.filter(\"age > 21\")\n",
                "df_final = df_filtered.select(\"name\", \"age\")\n",
                "\n",
                "print(\"\\nHas any data been touched yet? NO.\")\n",
                "\n",
                "# 3. Action (Eager)\n",
                "result = df_final.count()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Interview Preparation\n",
                "\n",
                "### Common Questions\n",
                "\n",
                "#### Q1: \"What is the difference between `map` and `reduce`?\"\n",
                "**Answer**: \"`map` transforms elements individually (1-to-1). `reduce` aggregates elements (Many-to-1). In Spark, `reduceByKey` is a powerful way to aggregate distributed data.\"\n",
                "\n",
                "#### Q2: \"Explain Wide vs Narrow Dependencies.\"\n",
                "**Answer**: \n",
                "- **Narrow**: Data stays in the same partition (e.g., `filter`, `map`). Fast.\n",
                "- **Wide**: Data must be shuffled across network between executors (e.g., `groupBy`, `join`). Slow. Shuffles are the bottleneck in Spark jobs.\n",
                "\n",
                "#### Q3: \"What is a Broadcast Variable?\"\n",
                "**Answer**: \"If I have a huge table and a tiny dictionary, instead of doing a full shuffle join, I broadcast (send copy of) the tiny dictionary to every executor's RAM. Then map-side joins can happen locally without network traffic.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}