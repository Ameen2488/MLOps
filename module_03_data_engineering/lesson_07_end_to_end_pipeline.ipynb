{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 7: Building End-to-End Data Pipelines\n",
                "\n",
                "**Module 3: Data & Pipeline Engineering** | **Time**: 4-5 hours | **Difficulty**: Advanced\n",
                "\n",
                "---\n",
                "\n",
                "## \ud83c\udfaf Learning Objectives\n",
                "\n",
                "\u2705 Design and build a complete ML data pipeline from raw data to model-ready features  \n",
                "\u2705 Implement data validation, transformation, and quality checks  \n",
                "\u2705 Understand orchestration patterns (Airflow, Prefect)  \n",
                "\u2705 Apply idempotency, observability, and error handling best practices  \n",
                "\u2705 Answer 5 interview questions on production data pipelines  \n",
                "\n",
                "---\n",
                "\n",
                "## \ud83d\udcda Table of Contents\n",
                "\n",
                "1. [What Does \u201cEnd-to-End\u201d Mean?](#1-e2e)\n",
                "2. [Pipeline Architecture Patterns](#2-patterns)\n",
                "3. [Data Validation with Great Expectations](#3-validation)\n",
                "4. [Hands-On: Complete ML Data Pipeline](#4-hands-on)\n",
                "5. [Orchestration: Airflow & Prefect](#5-orchestration)\n",
                "6. [Production Best Practices](#6-best-practices)\n",
                "7. [Exercises](#7-exercises)\n",
                "8. [Interview Preparation](#8-interview)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. What Does \u201cEnd-to-End\u201d Mean? <a id='1-e2e'></a>\n",
                "\n",
                "An end-to-end ML data pipeline covers **everything** from raw data ingestion to model-ready features.\n",
                "\n",
                "### Complete Pipeline Architecture\n",
                "\n",
                "```\n",
                "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "  \u2502 INGEST   \u2502\u25b6\u2502 VALIDATE \u2502\u25b6\u2502TRANSFORM \u2502\u25b6\u2502 QUALITY  \u2502\u25b6\u2502  STORE   \u2502\n",
                "  \u2502          \u2502  \u2502          \u2502  \u2502          \u2502  \u2502  CHECK   \u2502  \u2502          \u2502\n",
                "  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "       \u2502           \u2502           \u2502           \u2502           \u2502\n",
                "  Read from    Schema     Feature     Drift    Write to\n",
                "  sources      checks     engineering  detect   Feature Store\n",
                "  (API, DB,    Null check  Normalize   Anomaly  or Parquet\n",
                "   files)      Type check  Encode      detect\n",
                "```\n",
                "\n",
                "### The 5 Stages of a Production Pipeline\n",
                "\n",
                "| Stage | Purpose | Tools |\n",
                "|-------|---------|-------|\n",
                "| **Ingest** | Pull raw data from sources | APIs, Kafka, databases, file systems |\n",
                "| **Validate** | Ensure data meets expectations | Great Expectations, Pandera |\n",
                "| **Transform** | Feature engineering, cleaning | Pandas, Spark, dbt |\n",
                "| **Quality Check** | Detect drift, anomalies | Custom checks, monitoring |\n",
                "| **Store** | Save model-ready features | Parquet, Feature Store (Feast) |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Pipeline Architecture Patterns <a id='2-patterns'></a>\n",
                "\n",
                "### Pattern 1: Simple Batch Pipeline\n",
                "\n",
                "```\n",
                "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "  \u2502 Cron /  \u2502  \u2500\u25b6  \u2502 Python   \u2502  \u2500\u25b6  \u2502 Parquet \u2502\n",
                "  \u2502 Airflow \u2502      \u2502 Script   \u2502      \u2502 Output  \u2502\n",
                "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "  Trigger          Process          Store\n",
                "  (daily/hourly)   (transform)      (feature store)\n",
                "```\n",
                "\n",
                "**Best for:** Small-medium teams, daily/hourly batch processing\n",
                "\n",
                "### Pattern 2: Lambda Architecture\n",
                "\n",
                "```\n",
                "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "                    \u2502  BATCH LAYER       \u2502\n",
                "  Raw Data \u2500\u2500\u2500\u252c\u2500\u2500\u25b6 \u2502  (Spark + Parquet)  \u2502\u2500\u2500\u2500\u2510\n",
                "               \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "               \u2502                            \u251c\u25b6\u2502 SERVING  \u2502\n",
                "               \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  \u2502  LAYER   \u2502\n",
                "               \u2514\u2500\u2500\u25b6 \u2502  SPEED LAYER       \u2502\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "                    \u2502  (Kafka + Redis)   \u2502\n",
                "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "```\n",
                "\n",
                "**Best for:** Systems needing both historical analysis and real-time features\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Validation with Great Expectations <a id='3-validation'></a>\n",
                "\n",
                "**Never trust raw data.** Every pipeline should validate data at ingestion.\n",
                "\n",
                "### Validation Checks\n",
                "\n",
                "```\n",
                "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "  \u2502             DATA VALIDATION LAYERS               \u2502\n",
                "  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                "  \u2502 SCHEMA       \u2502 VALUE          \u2502 STATISTICAL    \u2502\n",
                "  \u2502              \u2502                \u2502                \u2502\n",
                "  \u2502 \u2022 Correct    \u2502 \u2022 Non-null     \u2502 \u2022 Mean within  \u2502\n",
                "  \u2502   columns    \u2502 \u2022 In range     \u2502   bounds       \u2502\n",
                "  \u2502 \u2022 Correct    \u2502 \u2022 Valid enum   \u2502 \u2022 Distribution \u2502\n",
                "  \u2502   dtypes     \u2502 \u2022 Unique IDs   \u2502   check        \u2502\n",
                "  \u2502 \u2022 No extra   \u2502 \u2022 Positive     \u2502 \u2022 Outlier      \u2502\n",
                "  \u2502   columns    \u2502   values       \u2502   detection    \u2502\n",
                "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "```\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Custom Data Validator (production-grade pattern)\n",
                "# ============================================================\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from dataclasses import dataclass, field\n",
                "from typing import Dict, List, Optional, Any\n",
                "from datetime import datetime\n",
                "\n",
                "@dataclass\n",
                "class ValidationResult:\n",
                "    \"\"\"Result of a single validation check.\"\"\"\n",
                "    check_name: str\n",
                "    passed: bool\n",
                "    details: str\n",
                "    severity: str  # 'error', 'warning', 'info'\n",
                "\n",
                "@dataclass\n",
                "class DataValidator:\n",
                "    \"\"\"Production data validation framework.\"\"\"\n",
                "    results: List[ValidationResult] = field(default_factory=list)\n",
                "    \n",
                "    def check_schema(self, df: pd.DataFrame, expected_columns: List[str]) -> None:\n",
                "        \"\"\"Validate that expected columns exist.\"\"\"\n",
                "        missing = set(expected_columns) - set(df.columns)\n",
                "        extra = set(df.columns) - set(expected_columns)\n",
                "        \n",
                "        self.results.append(ValidationResult(\n",
                "            check_name='schema_check',\n",
                "            passed=len(missing) == 0,\n",
                "            details=f'Missing: {missing}, Extra: {extra}' if missing or extra else 'All columns present',\n",
                "            severity='error' if missing else 'warning' if extra else 'info'\n",
                "        ))\n",
                "    \n",
                "    def check_nulls(self, df: pd.DataFrame, max_null_pct: float = 0.05) -> None:\n",
                "        \"\"\"Check null percentage per column.\"\"\"\n",
                "        null_pcts = df.isnull().mean()\n",
                "        violations = null_pcts[null_pcts > max_null_pct]\n",
                "        \n",
                "        self.results.append(ValidationResult(\n",
                "            check_name='null_check',\n",
                "            passed=len(violations) == 0,\n",
                "            details=f'Columns exceeding {max_null_pct:.0%} nulls: {dict(violations)}' if len(violations) > 0 else 'All columns within null threshold',\n",
                "            severity='error' if len(violations) > 0 else 'info'\n",
                "        ))\n",
                "    \n",
                "    def check_range(self, df: pd.DataFrame, column: str, min_val: float, max_val: float) -> None:\n",
                "        \"\"\"Check values are within expected range.\"\"\"\n",
                "        out_of_range = ((df[column] < min_val) | (df[column] > max_val)).sum()\n",
                "        \n",
                "        self.results.append(ValidationResult(\n",
                "            check_name=f'range_check_{column}',\n",
                "            passed=out_of_range == 0,\n",
                "            details=f'{out_of_range} values outside [{min_val}, {max_val}]' if out_of_range > 0 else f'All values in [{min_val}, {max_val}]',\n",
                "            severity='error' if out_of_range > 0 else 'info'\n",
                "        ))\n",
                "    \n",
                "    def check_uniqueness(self, df: pd.DataFrame, column: str) -> None:\n",
                "        \"\"\"Check that column values are unique (for IDs).\"\"\"\n",
                "        duplicates = df[column].duplicated().sum()\n",
                "        \n",
                "        self.results.append(ValidationResult(\n",
                "            check_name=f'uniqueness_{column}',\n",
                "            passed=duplicates == 0,\n",
                "            details=f'{duplicates} duplicate values found' if duplicates > 0 else 'All values unique',\n",
                "            severity='error' if duplicates > 0 else 'info'\n",
                "        ))\n",
                "    \n",
                "    def report(self) -> bool:\n",
                "        \"\"\"Print validation report and return overall pass/fail.\"\"\"\n",
                "        print(\"\\n\" + \"=\"*60)\n",
                "        print(\"DATA VALIDATION REPORT\")\n",
                "        print(f\"Timestamp: {datetime.now().isoformat()}\")\n",
                "        print(\"=\"*60)\n",
                "        \n",
                "        all_passed = True\n",
                "        for r in self.results:\n",
                "            icon = '\u2705' if r.passed else ('\u26a0\ufe0f' if r.severity == 'warning' else '\u274c')\n",
                "            print(f\"  {icon} {r.check_name}: {r.details}\")\n",
                "            if not r.passed and r.severity == 'error':\n",
                "                all_passed = False\n",
                "        \n",
                "        print(\"\\n\" + \"-\"*60)\n",
                "        status = '\u2705 ALL CHECKS PASSED' if all_passed else '\u274c VALIDATION FAILED'\n",
                "        print(f\"  Overall: {status}\")\n",
                "        print(\"=\"*60)\n",
                "        return all_passed\n",
                "\n",
                "print(\"\u2705 DataValidator class defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Hands-On: Complete ML Data Pipeline <a id='4-hands-on'></a>\n",
                "\n",
                "We\u2019ll build a **complete 6-step pipeline** that takes raw e-commerce data and produces model-ready features.\n",
                "\n",
                "### Pipeline Flow\n",
                "\n",
                "```\n",
                "  Step 1        Step 2       Step 3        Step 4       Step 5       Step 6\n",
                "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "  \u2502Ingest\u2502\u25b6\u2502Validate\u2502\u25b6\u2502Transform\u2502\u25b6\u2502Feature\u2502\u25b6\u2502Quality\u2502\u25b6\u2502 Store \u2502\n",
                "  \u2502      \u2502  \u2502       \u2502  \u2502        \u2502  \u2502 Engg  \u2502  \u2502 Check \u2502  \u2502       \u2502\n",
                "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 1: INGEST - Simulate raw data from multiple sources\n",
                "# ============================================================\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "np.random.seed(42)\n",
                "n = 50_000\n",
                "\n",
                "print(\"STEP 1: INGEST\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Simulate transactions from an API\n",
                "transactions = pd.DataFrame({\n",
                "    'transaction_id': np.arange(n),\n",
                "    'user_id': np.random.randint(1, 5001, n),\n",
                "    'product_id': np.random.randint(1, 1001, n),\n",
                "    'category': np.random.choice(['electronics', 'clothing', 'food', 'books', 'sports'], n),\n",
                "    'amount': np.round(np.random.exponential(50, n), 2),\n",
                "    'quantity': np.random.randint(1, 10, n),\n",
                "    'timestamp': pd.date_range('2024-01-01', periods=n, freq='10min'),\n",
                "})\n",
                "\n",
                "# Inject realistic data quality issues\n",
                "# 1. Missing values (~3%)\n",
                "null_mask = np.random.random(n) < 0.03\n",
                "transactions.loc[null_mask, 'amount'] = np.nan\n",
                "\n",
                "# 2. Negative amounts (data entry errors)\n",
                "neg_mask = np.random.random(n) < 0.005\n",
                "transactions.loc[neg_mask, 'amount'] = -abs(transactions.loc[neg_mask, 'amount'])\n",
                "\n",
                "# 3. Duplicate transactions \n",
                "duplicates = transactions.sample(n=100, random_state=42)\n",
                "transactions = pd.concat([transactions, duplicates]).reset_index(drop=True)\n",
                "\n",
                "# Simulate user profiles from a database\n",
                "user_profiles = pd.DataFrame({\n",
                "    'user_id': np.arange(1, 5001),\n",
                "    'age': np.random.randint(18, 80, 5000),\n",
                "    'region': np.random.choice(['US_East', 'US_West', 'EU', 'Asia', 'Other'], 5000),\n",
                "    'account_age_days': np.random.randint(1, 3650, 5000),\n",
                "    'is_premium': np.random.choice([0, 1], 5000, p=[0.7, 0.3]),\n",
                "})\n",
                "\n",
                "print(f\"  Transactions: {len(transactions):,} rows ({transactions.isna().sum().sum()} nulls)\")\n",
                "print(f\"  User profiles: {len(user_profiles):,} rows\")\n",
                "print(f\"  Injected issues: nulls, negatives, duplicates\")\n",
                "print(\"\u2705 Raw data ingested\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 2: VALIDATE - Check data quality before processing\n",
                "# ============================================================\n",
                "print(\"\\nSTEP 2: VALIDATE\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "validator = DataValidator()\n",
                "\n",
                "# Schema checks\n",
                "validator.check_schema(transactions, \n",
                "    ['transaction_id', 'user_id', 'product_id', 'category', 'amount', 'quantity', 'timestamp'])\n",
                "\n",
                "# Null checks\n",
                "validator.check_nulls(transactions, max_null_pct=0.05)\n",
                "\n",
                "# Range checks\n",
                "validator.check_range(transactions, 'amount', min_val=0.01, max_val=10000)\n",
                "validator.check_range(transactions, 'quantity', min_val=1, max_val=100)\n",
                "\n",
                "# Uniqueness\n",
                "validator.check_uniqueness(transactions, 'transaction_id')\n",
                "\n",
                "is_valid = validator.report()\n",
                "\n",
                "if not is_valid:\n",
                "    print(\"\\n>>> Validation failed! Will clean issues in Transform step.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 3: TRANSFORM - Clean and fix data quality issues\n",
                "# ============================================================\n",
                "print(\"\\nSTEP 3: TRANSFORM (Clean)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "df = transactions.copy()\n",
                "initial_count = len(df)\n",
                "\n",
                "# 1. Remove duplicates\n",
                "df = df.drop_duplicates(subset=['transaction_id'], keep='first')\n",
                "print(f\"  Removed {initial_count - len(df)} duplicates\")\n",
                "\n",
                "# 2. Handle negative amounts (likely data entry errors)\n",
                "neg_count = (df['amount'] < 0).sum()\n",
                "df['amount'] = df['amount'].abs()  # Take absolute value\n",
                "print(f\"  Fixed {neg_count} negative amounts\")\n",
                "\n",
                "# 3. Handle missing values\n",
                "null_count = df['amount'].isna().sum()\n",
                "# Impute with median per category (more accurate than global median)\n",
                "df['amount'] = df.groupby('category')['amount'].transform(\n",
                "    lambda x: x.fillna(x.median())\n",
                ")\n",
                "print(f\"  Imputed {null_count} null amounts with category median\")\n",
                "\n",
                "# 4. Add computed columns\n",
                "df['total_value'] = df['amount'] * df['quantity']\n",
                "df['hour'] = df['timestamp'].dt.hour\n",
                "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
                "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
                "\n",
                "print(f\"  Added computed columns: total_value, hour, day_of_week, is_weekend\")\n",
                "print(f\"  Final shape: {df.shape}\")\n",
                "print(\"\u2705 Data cleaned and transformed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 4: FEATURE ENGINEERING - Create ML-ready features\n",
                "# ============================================================\n",
                "print(\"\\nSTEP 4: FEATURE ENGINEERING\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# --- User-level aggregation features ---\n",
                "user_features = df.groupby('user_id').agg(\n",
                "    total_transactions=('transaction_id', 'count'),\n",
                "    total_spend=('total_value', 'sum'),\n",
                "    avg_order_value=('amount', 'mean'),\n",
                "    max_order_value=('amount', 'max'),\n",
                "    unique_categories=('category', 'nunique'),\n",
                "    avg_quantity=('quantity', 'mean'),\n",
                "    weekend_ratio=('is_weekend', 'mean'),\n",
                "    first_purchase=('timestamp', 'min'),\n",
                "    last_purchase=('timestamp', 'max'),\n",
                ").reset_index()\n",
                "\n",
                "# Recency feature\n",
                "max_date = df['timestamp'].max()\n",
                "user_features['days_since_last_purchase'] = (max_date - user_features['last_purchase']).dt.days\n",
                "\n",
                "# Purchase frequency\n",
                "user_features['purchase_span_days'] = (\n",
                "    (user_features['last_purchase'] - user_features['first_purchase']).dt.days + 1\n",
                ")\n",
                "user_features['purchase_frequency'] = (\n",
                "    user_features['total_transactions'] / user_features['purchase_span_days']\n",
                ")\n",
                "\n",
                "# Drop temporal columns (can't use raw dates as features)\n",
                "user_features = user_features.drop(columns=['first_purchase', 'last_purchase'])\n",
                "\n",
                "# --- Merge with user profiles ---\n",
                "features_df = user_features.merge(user_profiles, on='user_id', how='left')\n",
                "\n",
                "# --- One-hot encode region ---\n",
                "features_df = pd.get_dummies(features_df, columns=['region'], prefix='region')\n",
                "\n",
                "print(f\"  Created {len(features_df.columns) - 1} features for {len(features_df)} users\")\n",
                "print(f\"  Feature list: {list(features_df.columns[:10])}... (+{len(features_df.columns)-10} more)\")\n",
                "print(\"\u2705 Features engineered\")\n",
                "features_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 5: QUALITY CHECK - Validate output features\n",
                "# ============================================================\n",
                "print(\"\\nSTEP 5: QUALITY CHECK (Output Validation)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "output_validator = DataValidator()\n",
                "\n",
                "# Check no nulls in final features\n",
                "output_validator.check_nulls(features_df, max_null_pct=0.0)\n",
                "\n",
                "# Check reasonable ranges\n",
                "output_validator.check_range(features_df, 'total_spend', min_val=0, max_val=1_000_000)\n",
                "output_validator.check_range(features_df, 'avg_order_value', min_val=0, max_val=10_000)\n",
                "output_validator.check_range(features_df, 'age', min_val=18, max_val=100)\n",
                "\n",
                "# Check uniqueness of user_id\n",
                "output_validator.check_uniqueness(features_df, 'user_id')\n",
                "\n",
                "output_valid = output_validator.report()\n",
                "\n",
                "if output_valid:\n",
                "    print(\"\\n>>> Output features pass all quality checks!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 6: STORE - Save as Parquet (production format)\n",
                "# ============================================================\n",
                "print(\"\\nSTEP 6: STORE\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "import time\n",
                "\n",
                "os.makedirs('pipeline_output', exist_ok=True)\n",
                "\n",
                "# Save as Parquet with metadata\n",
                "output_path = 'pipeline_output/user_features.parquet'\n",
                "features_df.to_parquet(output_path, index=False, compression='snappy')\n",
                "\n",
                "file_size = os.path.getsize(output_path) / 1024\n",
                "\n",
                "print(f\"  Output: {output_path}\")\n",
                "print(f\"  Size: {file_size:.1f} KB\")\n",
                "print(f\"  Rows: {len(features_df):,}\")\n",
                "print(f\"  Columns: {len(features_df.columns)}\")\n",
                "print(f\"  Compression: snappy\")\n",
                "print(\"\u2705 Features saved to Parquet!\")\n",
                "\n",
                "# Verify we can read it back\n",
                "verify_df = pd.read_parquet(output_path)\n",
                "assert verify_df.shape == features_df.shape\n",
                "print(\"\u2705 Read-back verification passed!\")\n",
                "\n",
                "# Cleanup\n",
                "import shutil\n",
                "shutil.rmtree('pipeline_output', ignore_errors=True)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"\ud83c\udf89 END-TO-END PIPELINE COMPLETE!\")\n",
                "print(\"=\"*60)\n",
                "print(\"  Ingested \u2192 Validated \u2192 Transformed \u2192 Feature Engineered \u2192 Quality Checked \u2192 Stored\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Orchestration: Airflow & Prefect <a id='5-orchestration'></a>\n",
                "\n",
                "In production, pipelines need to be **scheduled, monitored, and managed**.\n",
                "\n",
                "### Orchestration Visual\n",
                "\n",
                "```\n",
                "  AIRFLOW DAG:\n",
                "\n",
                "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "  \u2502 ingest \u2502\u2500\u2500\u25b6\u2502 validate\u2502\u2500\u2500\u25b6\u2502transform\u2502\u2500\u2500\u25b6\u2502  store   \u2502\n",
                "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "    @daily       on_success    on_success      on_success\n",
                "                                                  \u2502\n",
                "                                           \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "                                           \u2502   notify    \u2502\n",
                "                                           \u2502   (Slack)   \u2502\n",
                "                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "```\n",
                "\n",
                "### Airflow vs Prefect Comparison\n",
                "\n",
                "| Feature | Apache Airflow | Prefect |\n",
                "|---------|---------------|----------|\n",
                "| **Setup** | Complex (needs Postgres, Redis) | Simple (pip install + cloud) |\n",
                "| **Language** | Python DAG definitions | Python w/ decorators |\n",
                "| **Testing** | Hard to test locally | Easy local testing |\n",
                "| **Dynamic tasks** | Limited | First-class support |\n",
                "| **Community** | Massive, mature | Growing fast |\n",
                "| **Best for** | Enterprise, complex DAGs | Smaller teams, rapid iteration |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Conceptual: Airflow DAG Definition\n",
                "# ============================================================\n",
                "print(\"Conceptual Airflow DAG definition:\")\n",
                "print(\"\"\"  \n",
                "from airflow import DAG\n",
                "from airflow.operators.python import PythonOperator\n",
                "from datetime import datetime, timedelta\n",
                "\n",
                "default_args = {\n",
                "    'owner': 'ml_team',\n",
                "    'depends_on_past': False,\n",
                "    'retries': 3,\n",
                "    'retry_delay': timedelta(minutes=5),\n",
                "}\n",
                "\n",
                "dag = DAG(\n",
                "    'ml_feature_pipeline',\n",
                "    default_args=default_args,\n",
                "    description='Daily ML feature engineering pipeline',\n",
                "    schedule_interval='@daily',\n",
                "    start_date=datetime(2024, 1, 1),\n",
                "    catchup=False,\n",
                ")\n",
                "\n",
                "ingest_task   = PythonOperator(task_id='ingest',   python_callable=ingest_data,   dag=dag)\n",
                "validate_task = PythonOperator(task_id='validate', python_callable=validate_data, dag=dag)\n",
                "transform_task= PythonOperator(task_id='transform',python_callable=transform_data,dag=dag)\n",
                "store_task    = PythonOperator(task_id='store',    python_callable=store_features,dag=dag)\n",
                "\n",
                "ingest_task >> validate_task >> transform_task >> store_task\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Production Best Practices <a id='6-best-practices'></a>\n",
                "\n",
                "### The DOs and DON\u2019Ts\n",
                "\n",
                "| Practice | DO | DON\u2019T |\n",
                "|----------|-----|-------|\n",
                "| **Idempotency** | Make pipelines re-runnable safely | Assume one-time execution |\n",
                "| **Error handling** | Fail loudly, retry gracefully | Silently swallow errors |\n",
                "| **Schema enforcement** | Validate schema at ingestion | Trust input data blindly |\n",
                "| **Logging** | Log every step with metrics | Run pipelines silently |\n",
                "| **Testing** | Unit test each transform | Test only end-to-end |\n",
                "| **Monitoring** | Alert on failures and drift | Check only when users complain |\n",
                "\n",
                "### Idempotency Explained\n",
                "\n",
                "```\n",
                "  IDEMPOTENT Pipeline (safe to re-run):\n",
                "  Run 1: Produces output A   \u2713\n",
                "  Run 2: Produces output A   \u2713  (same result!)\n",
                "  Run 3: Produces output A   \u2713  (same result!)\n",
                "\n",
                "  NON-IDEMPOTENT Pipeline (dangerous):\n",
                "  Run 1: Produces output A   \u2713\n",
                "  Run 2: Produces output A+A \u274c  (duplicated!)\n",
                "  Run 3: Produces output A+A+A \u274c  (triple duplicated!)\n",
                "\n",
                "  How to achieve idempotency:\n",
                "  \u2022 Use INSERT OVERWRITE instead of INSERT INTO\n",
                "  \u2022 Write to dated partitions: output/date=2024-01-15/\n",
                "  \u2022 Delete before write: delete partition, then write\n",
                "  \u2022 Use upserts with unique keys\n",
                "```\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Exercises <a id='7-exercises'></a>\n",
                "\n",
                "### Exercise 1: Add Alerting\n",
                "Extend the DataValidator to send a Slack/email alert when validation fails. Use a simple webhook simulation.\n",
                "\n",
                "### Exercise 2: Incremental Pipeline\n",
                "Modify the pipeline to process only **new data since the last run** (incremental processing) instead of the full dataset.\n",
                "\n",
                "### Exercise 3: Data Lineage\n",
                "Add metadata tracking to the pipeline: record which source files were used, what transformations were applied, and output statistics. Save as a lineage JSON file alongside the output.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Interview Preparation <a id='8-interview'></a>\n",
                "\n",
                "### Q1: \"Walk me through designing a production data pipeline for an ML model.\"\n",
                "\n",
                "**Answer:**  \n",
                "\"My pipeline would have 5 stages:\n",
                "1. **Ingest**: Read from source systems (APIs, databases, event streams). Store raw data immutably.\n",
                "2. **Validate**: Schema checks, null checks, range checks. Fail early if data quality is unacceptable.\n",
                "3. **Transform**: Clean (dedup, handle nulls, fix types), then feature engineering (aggregations, encoding, scaling).\n",
                "4. **Quality Check**: Validate output features match expectations. Check for drift vs training distribution.\n",
                "5. **Store**: Write to Parquet (offline) and feature store (online). Partition by date for incremental processing.\n",
                "\n",
                "Key properties: idempotent (safe to re-run), observable (logging + alerting), and testable (unit tests per transform).\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q2: \"What is idempotency and why is it critical for data pipelines?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"An idempotent pipeline produces the same output regardless of how many times it\u2019s run. This matters because pipelines fail and get retried \u2014 without idempotency, retries can duplicate data.\n",
                "\n",
                "I achieve idempotency by: writing to dated partitions (overwrite, don\u2019t append), using upserts with unique keys for databases, and ensuring transforms are deterministic (set random seeds, sort before processing).\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q3: \"How do you handle data quality issues in production?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"Three layers of defense:\n",
                "1. **Prevention**: Schema validation at ingestion. Reject bad data early.\n",
                "2. **Detection**: Statistical monitors \u2014 track null rates, value distributions, row counts. Set thresholds and alert.\n",
                "3. **Remediation**: Automated fixes for known issues (dedup, type casting). Manual review for unknowns. Dead-letter queue for data that can\u2019t be processed.\n",
                "\n",
                "Tools: Great Expectations for validation, custom monitors for drift, PagerDuty/Slack for alerts.\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q4: \"Compare Airflow and Prefect. When would you choose each?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"**Airflow**: Enterprise standard. Choose when you have complex DAGs with many dependencies, need robust scheduling, and have a platform team to maintain it. Drawbacks: heavy infrastructure (Postgres, Redis, webserver), hard to test locally.\n",
                "\n",
                "**Prefect**: Modern alternative. Choose for smaller teams, rapid iteration, and Pythonic workflows. Easy local development, built-in observability. Drawbacks: smaller community, managed cloud may be required for production.\n",
                "\n",
                "My choice: Airflow for large organizations with existing infra. Prefect for startups and new projects.\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q5: \"A pipeline that ran perfectly for months suddenly starts producing bad features. How do you debug it?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"Systematic debugging approach:\n",
                "1. **Check inputs first**: Has the source data changed? Schema change? New categories? Volume spike?\n",
                "2. **Check each pipeline stage**: Run validation at each step to find where quality degrades\n",
                "3. **Check infrastructure**: Did a dependency update? Memory issues? Timeout changes?\n",
                "4. **Compare distributions**: Compare current output features vs historical baselines. Which features drifted?\n",
                "5. **Root cause**: Most common causes are upstream schema changes, data source outages, and silent categorical changes (new enum values).\n",
                "\n",
                "Prevention: add data contracts with upstream teams, monitor feature distributions over time, and version everything.\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83c\udf93 Key Takeaways\n",
                "\n",
                "1. **End-to-end pipelines** have 5 stages: Ingest \u2192 Validate \u2192 Transform \u2192 Quality Check \u2192 Store\n",
                "2. **Never trust raw data** \u2014 validate at ingestion and again at output\n",
                "3. **Idempotency** is non-negotiable for production pipelines\n",
                "4. **Feature engineering** is where the ML value is created\n",
                "5. **Orchestration** (Airflow/Prefect) handles scheduling, retries, and monitoring\n",
                "6. **Observability** \u2014 if you can\u2019t see it, you can\u2019t fix it\n",
                "\n",
                "---\n",
                "\n",
                "\ud83c\udf89 **Module 3 Complete!** You now have a comprehensive understanding of Data & Pipeline Engineering for MLOps."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}