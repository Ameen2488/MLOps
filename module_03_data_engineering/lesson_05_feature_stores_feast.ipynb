{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 5: Feature Stores with Feast\n",
                "\n",
                "**Module 3: Data & Pipeline Engineering** | **Time**: 4-5 hours | **Difficulty**: Intermediate-Advanced\n",
                "\n",
                "---\n",
                "\n",
                "## \ud83c\udfaf Learning Objectives\n",
                "\n",
                "\u2705 Understand the training-serving skew problem and why feature stores exist  \n",
                "\u2705 Learn the dual-database architecture behind feature stores  \n",
                "\u2705 Implement a feature store using Feast  \n",
                "\u2705 Understand point-in-time joins for temporal correctness  \n",
                "\u2705 Answer 5 interview questions on feature stores  \n",
                "\n",
                "---\n",
                "\n",
                "## \ud83d\udcda Table of Contents\n",
                "\n",
                "1. [The Training-Serving Skew Problem](#1-skew)\n",
                "2. [What Is a Feature Store?](#2-what-is)\n",
                "3. [Architecture: Dual-Database Design](#3-architecture)\n",
                "4. [Point-in-Time Joins](#4-pit-joins)\n",
                "5. [Feast: Open-Source Feature Store](#5-feast)\n",
                "6. [Hands-On: End-to-End Feast Demo](#6-hands-on)\n",
                "7. [Feature Store Best Practices](#7-best-practices)\n",
                "8. [Exercises](#8-exercises)\n",
                "9. [Interview Preparation](#9-interview)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Training-Serving Skew Problem <a id='1-skew'></a>\n",
                "\n",
                "The **single biggest operational challenge** in production ML is ensuring that the features used during **training** are exactly the same as those used during **inference (serving)**.\n",
                "\n",
                "### What Goes Wrong Without a Feature Store\n",
                "\n",
                "```\n",
                "  \u274c WITHOUT FEATURE STORE (Training-Serving Skew):\n",
                "\n",
                "  TRAINING PIPELINE:                    SERVING PIPELINE:\n",
                "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "  \u2502 Python script   \u2502                \u2502 Java service    \u2502\n",
                "  \u2502 Pandas logic    \u2502                \u2502 Different logic \u2502\n",
                "  \u2502 Version A       \u2502                \u2502 Version B       \u2502\n",
                "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "         \u2502                                    \u2502\n",
                "    Different                            Different\n",
                "    feature logic!                       feature values!\n",
                "         \u2502                                    \u2502\n",
                "         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 SKEW \ud83d\udca5 \u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "```\n",
                "\n",
                "### Types of Skew\n",
                "\n",
                "| Skew Type | Description | Example |\n",
                "|-----------|-------------|----------|\n",
                "| **Code skew** | Different languages/logic for train vs serve | Python Pandas vs Java SQL |\n",
                "| **Data skew** | Different data sources for train vs serve | Batch DB vs real-time API |\n",
                "| **Time skew** | Using future data in training features | Averaging over a window that includes future |\n",
                "\n",
                "### The Consequence\n",
                "\n",
                "> A model trained with skewed features will perform differently in production than in development, often **degrading silently** without obvious errors.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. What Is a Feature Store? <a id='2-what-is'></a>\n",
                "\n",
                "A feature store is a **centralized repository** for storing, managing, and serving ML features. It ensures that **the same feature definitions and values** are used for both training and serving.\n",
                "\n",
                "### Feature Store Benefits\n",
                "\n",
                "```\n",
                "  \u2705 WITH FEATURE STORE:\n",
                "\n",
                "  TRAINING PIPELINE:          FEATURE STORE          SERVING PIPELINE:\n",
                "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "  \u2502 get_features()  \u2502\u2500\u25b6\u2502 Single Source \u2502\u25c0\u2500\u2502 get_features()  \u2502\n",
                "  \u2502 (batch read)    \u2502    \u2502 of Truth      \u2502    \u2502 (online read)   \u2502\n",
                "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502               \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "                         \u2502 \u2022 Consistent   \u2502\n",
                "                         \u2502 \u2022 Versioned    \u2502\n",
                "                         \u2502 \u2022 Discoverable \u2502\n",
                "                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "          SAME features for training AND serving!\n",
                "```\n",
                "\n",
                "### Key Capabilities\n",
                "\n",
                "| Capability | Description |\n",
                "|-----------|-------------|\n",
                "| **Feature Registry** | Central catalog of all features with metadata |\n",
                "| **Offline Store** | Historical features for training (batch) |\n",
                "| **Online Store** | Low-latency features for serving (real-time) |\n",
                "| **Point-in-Time Joins** | Temporal correctness for training data |\n",
                "| **Feature Sharing** | Teams can discover and reuse features |\n",
                "| **Monitoring** | Track feature drift, freshness, quality |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Architecture: Dual-Database Design <a id='3-architecture'></a>\n",
                "\n",
                "Feature stores use a **dual-database architecture** to serve two very different access patterns:\n",
                "\n",
                "### The Dual-Database Pattern\n",
                "\n",
                "```\n",
                "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "                    \u2502  Feature Store   \u2502\n",
                "                    \u2502  (Central Hub)   \u2502\n",
                "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "                             \u2502\n",
                "              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "              \u2502                           \u2502\n",
                "   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "   \u2502 OFFLINE STORE    \u2502      \u2502  ONLINE STORE    \u2502\n",
                "   \u2502 (Batch/Training) \u2502      \u2502 (Real-time/Serve)\u2502\n",
                "   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524      \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                "   \u2502 \u2022 Parquet/BQ     \u2502      \u2502 \u2022 Redis/DynamoDB \u2502\n",
                "   \u2502 \u2022 Full history   \u2502      \u2502 \u2022 Latest values  \u2502\n",
                "   \u2502 \u2022 High throughput\u2502      \u2502 \u2022 Low latency    \u2502\n",
                "   \u2502 \u2022 Seconds OK     \u2502      \u2502 \u2022 <10ms response \u2502\n",
                "   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "       \u2502                           \u2502\n",
                "   Used by:                    Used by:\n",
                "   Model Training              Model Serving\n",
                "   Batch Inference             Real-time Inference\n",
                "```\n",
                "\n",
                "### Why Two Databases?\n",
                "\n",
                "| Requirement | Training (Offline) | Serving (Online) |\n",
                "|------------|-------------------|------------------|\n",
                "| **Data Volume** | Millions/billions of rows | Single entity lookup |\n",
                "| **Latency** | Minutes acceptable | <10ms required |\n",
                "| **Access Pattern** | Full scan / batch | Key-value lookup |\n",
                "| **Data Span** | Historical (months/years) | Latest values only |\n",
                "| **Storage** | Parquet, BigQuery | Redis, DynamoDB |\n",
                "\n",
                "A data warehouse (Parquet) is optimized for the first; a key-value store (Redis) for the second. No single database excels at both.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Point-in-Time Joins <a id='4-pit-joins'></a>\n",
                "\n",
                "When building training data, you need features **as they were at the time of each training example** \u2014 not as they are today. This is called a **point-in-time join**.\n",
                "\n",
                "### Why Regular Joins Cause Leakage\n",
                "\n",
                "```\n",
                "  REGULAR JOIN (LEAKY):\n",
                "  Label event: User X purchased on Jan 15\n",
                "  Feature:     User X's avg_spend = $120  (computed over ALL time including after Jan 15!)\n",
                "  \u2192 Uses future information! \ud83d\udca5\n",
                "\n",
                "  POINT-IN-TIME JOIN (CORRECT):\n",
                "  Label event: User X purchased on Jan 15\n",
                "  Feature:     User X's avg_spend = $85  (computed ONLY with data before Jan 15)\n",
                "  \u2192 Only past information \u2705\n",
                "\n",
                "  Timeline:\n",
                "  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n",
                "    Past data only     \u2502 Jan 15       Future\n",
                "    (use for features) \u2502 (event)      (NEVER use)\n",
                "```\n",
                "\n",
                "Feature stores like Feast automate these point-in-time joins, ensuring temporal correctness.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Feast: Open-Source Feature Store <a id='5-feast'></a>\n",
                "\n",
                "**Feast** (Feature Store) is the most popular open-source feature store. It provides:\n",
                "\n",
                "### Feast Architecture\n",
                "\n",
                "```\n",
                "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "  \u2502                    FEAST                               \u2502\n",
                "  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                "  \u2502 Feature Defs  \u2502 Offline Store  \u2502  Online Store     \u2502\n",
                "  \u2502 (Python)      \u2502 (Parquet/BQ)   \u2502  (SQLite/Redis)   \u2502\n",
                "  \u2502               \u2502                \u2502                   \u2502\n",
                "  \u2502 Entity        \u2502 Historical     \u2502  Latest values    \u2502\n",
                "  \u2502 FeatureView   \u2502 features for   \u2502  for real-time    \u2502\n",
                "  \u2502 FeatureService\u2502 training       \u2502  serving          \u2502\n",
                "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "```\n",
                "\n",
                "### Core Feast Concepts\n",
                "\n",
                "| Concept | Description | Example |\n",
                "|---------|-------------|----------|\n",
                "| **Entity** | The real-world object features describe | `user_id`, `product_id` |\n",
                "| **Feature View** | A group of related features from a source | `user_purchase_features` |\n",
                "| **Data Source** | Where the raw feature data lives | Parquet file, BigQuery table |\n",
                "| **Feature Service** | A bundle of features for a specific model | `fraud_detection_features` |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Hands-On: End-to-End Feast Demo <a id='6-hands-on'></a>\n",
                "\n",
                "We\u2019ll build a complete feature store workflow:\n",
                "1. Generate feature data\n",
                "2. Define Feast entities and feature views\n",
                "3. Materialize features to online store\n",
                "4. Retrieve features for training and serving\n",
                "\n",
                "### Prerequisites\n",
                "```bash\n",
                "pip install feast\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Step 1: Generate Feature Data (simulating a data pipeline output)\n",
                "# ============================================================\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from datetime import datetime, timedelta\n",
                "import os\n",
                "\n",
                "np.random.seed(42)\n",
                "\n",
                "# Generate user features over time\n",
                "n_users = 500\n",
                "n_days = 90\n",
                "records = []\n",
                "\n",
                "for day in range(n_days):\n",
                "    event_time = datetime(2024, 1, 1) + timedelta(days=day)\n",
                "    for user_id in range(1, n_users + 1):\n",
                "        records.append({\n",
                "            'event_timestamp': event_time,\n",
                "            'user_id': user_id,\n",
                "            'daily_spend': round(np.random.exponential(30) + np.sin(day/7) * 10, 2),\n",
                "            'num_sessions': int(np.random.poisson(3)),\n",
                "            'avg_session_duration_min': round(np.random.gamma(2, 5), 1),\n",
                "            'items_viewed': int(np.random.poisson(10)),\n",
                "            'items_purchased': int(np.random.poisson(1)),\n",
                "            'is_premium': int(np.random.random() > 0.7),\n",
                "        })\n",
                "\n",
                "user_features_df = pd.DataFrame(records)\n",
                "\n",
                "# Save as Parquet (this is what Feast reads)\n",
                "os.makedirs('feast_demo/data', exist_ok=True)\n",
                "user_features_df.to_parquet('feast_demo/data/user_features.parquet', index=False)\n",
                "\n",
                "print(f\"Generated {len(user_features_df):,} feature records\")\n",
                "print(f\"Users: {n_users} | Days: {n_days}\")\n",
                "print(f\"Columns: {list(user_features_df.columns)}\")\n",
                "print(f\"\\nDate range: {user_features_df['event_timestamp'].min()} to {user_features_df['event_timestamp'].max()}\")\n",
                "user_features_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Step 2: Define Feast Feature Store\n",
                "# ============================================================\n",
                "\n",
                "# Create the feature_store.yaml configuration\n",
                "feast_config = \"\"\"project: user_activity\n",
                "registry: feast_demo/registry.db\n",
                "provider: local\n",
                "online_store:\n",
                "  type: sqlite\n",
                "  path: feast_demo/online_store.db\n",
                "offline_store:\n",
                "  type: file\n",
                "entity_key_serialization_version: 2\n",
                "\"\"\"\n",
                "\n",
                "with open('feast_demo/feature_store.yaml', 'w') as f:\n",
                "    f.write(feast_config)\n",
                "\n",
                "print(\"\u2705 Created feature_store.yaml\")\n",
                "print(feast_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Step 3: Define Entities and Feature Views programmatically\n",
                "# ============================================================\n",
                "\n",
                "feature_def_code = '''\n",
                "from datetime import timedelta\n",
                "from feast import Entity, FeatureView, Field, FileSource\n",
                "from feast.types import Float32, Int64\n",
                "\n",
                "# Define the entity (the \"who\" or \"what\" the features describe)\n",
                "user = Entity(\n",
                "    name=\"user_id\",\n",
                "    description=\"Unique user identifier\",\n",
                ")\n",
                "\n",
                "# Define the data source\n",
                "user_source = FileSource(\n",
                "    path=\"data/user_features.parquet\",\n",
                "    timestamp_field=\"event_timestamp\",\n",
                ")\n",
                "\n",
                "# Define the feature view\n",
                "user_activity_fv = FeatureView(\n",
                "    name=\"user_activity\",\n",
                "    entities=[user],\n",
                "    ttl=timedelta(days=7),  # Features older than 7 days are stale\n",
                "    schema=[\n",
                "        Field(name=\"daily_spend\", dtype=Float32),\n",
                "        Field(name=\"num_sessions\", dtype=Int64),\n",
                "        Field(name=\"avg_session_duration_min\", dtype=Float32),\n",
                "        Field(name=\"items_viewed\", dtype=Int64),\n",
                "        Field(name=\"items_purchased\", dtype=Int64),\n",
                "        Field(name=\"is_premium\", dtype=Int64),\n",
                "    ],\n",
                "    source=user_source,\n",
                "    online=True,\n",
                ")\n",
                "'''\n",
                "\n",
                "with open('feast_demo/features.py', 'w') as f:\n",
                "    f.write(feature_def_code)\n",
                "\n",
                "print(\"\u2705 Created features.py with entity and feature view definitions\")\n",
                "print(\"\\nKey components:\")\n",
                "print(\"  Entity:       user_id (the primary key)\")\n",
                "print(\"  FeatureView:  user_activity (6 features)\")\n",
                "print(\"  TTL:          7 days (features expire after 7 days)\")\n",
                "print(\"  Source:       Parquet file\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Step 4: Apply Feast definitions and materialize\n",
                "# ============================================================\n",
                "try:\n",
                "    from feast import FeatureStore\n",
                "    \n",
                "    # Initialize the feature store\n",
                "    store = FeatureStore(repo_path='feast_demo')\n",
                "    \n",
                "    # Apply the feature definitions\n",
                "    store.apply([])  # This registers entities and feature views\n",
                "    print(\"\u2705 Feature store initialized!\")\n",
                "    \n",
                "    # Materialize features to online store\n",
                "    # This copies the latest feature values to the low-latency online store\n",
                "    store.materialize_incremental(\n",
                "        end_date=datetime(2024, 4, 1)\n",
                "    )\n",
                "    print(\"\u2705 Features materialized to online store!\")\n",
                "    \n",
                "except ImportError:\n",
                "    print(\"\u26a0\ufe0f Feast not installed. Run: pip install feast\")\n",
                "    print(\"\\nThe code above would:\")\n",
                "    print(\"1. Initialize the feature store from feast_demo/\")\n",
                "    print(\"2. Register entity and feature view definitions\")\n",
                "    print(\"3. Materialize (copy) latest features to online store (SQLite)\")\n",
                "except Exception as e:\n",
                "    print(f\"Note: {e}\")\n",
                "    print(\"This is expected in a notebook environment.\")\n",
                "    print(\"In production, run: feast apply && feast materialize-incremental\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Step 5: Retrieve features for Training (Offline)\n",
                "# Point-in-time join demonstration\n",
                "# ============================================================\n",
                "print(\"=\"*60)\n",
                "print(\"OFFLINE FEATURE RETRIEVAL (Training)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Create entity DataFrame (who + when)\n",
                "# These represent training examples with their timestamps\n",
                "entity_df = pd.DataFrame({\n",
                "    'user_id': [1, 2, 3, 4, 5, 1, 2, 3],\n",
                "    'event_timestamp': [\n",
                "        datetime(2024, 2, 15),\n",
                "        datetime(2024, 2, 15),\n",
                "        datetime(2024, 2, 15),\n",
                "        datetime(2024, 2, 15),\n",
                "        datetime(2024, 2, 15),\n",
                "        datetime(2024, 3, 1),\n",
                "        datetime(2024, 3, 1),\n",
                "        datetime(2024, 3, 1),\n",
                "    ],\n",
                "    'label': [1, 0, 1, 0, 1, 0, 1, 0]  # Our target variable\n",
                "})\n",
                "\n",
                "print(\"Entity DataFrame (training examples):\")\n",
                "print(entity_df)\n",
                "\n",
                "try:\n",
                "    # This performs a POINT-IN-TIME JOIN\n",
                "    training_df = store.get_historical_features(\n",
                "        entity_df=entity_df,\n",
                "        features=[\n",
                "            'user_activity:daily_spend',\n",
                "            'user_activity:num_sessions',\n",
                "            'user_activity:items_purchased',\n",
                "        ]\n",
                "    ).to_df()\n",
                "    \n",
                "    print(\"\\nTraining data with features (point-in-time correct):\")\n",
                "    print(training_df)\n",
                "except:\n",
                "    # Simulate the output\n",
                "    print(\"\\n[Simulated] Point-in-time join would return:\")\n",
                "    print(\"Each row gets features AS THEY WERE at that timestamp.\")\n",
                "    print(\"User 1 on Feb 15 gets different features than User 1 on Mar 1.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Step 6: Retrieve features for Serving (Online)\n",
                "# ============================================================\n",
                "print(\"=\"*60)\n",
                "print(\"ONLINE FEATURE RETRIEVAL (Serving)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "try:\n",
                "    # Get the LATEST features for a user (low-latency lookup)\n",
                "    online_features = store.get_online_features(\n",
                "        features=[\n",
                "            'user_activity:daily_spend',\n",
                "            'user_activity:num_sessions',\n",
                "            'user_activity:items_purchased',\n",
                "        ],\n",
                "        entity_rows=[{'user_id': 1}, {'user_id': 2}]\n",
                "    ).to_dict()\n",
                "    \n",
                "    print(\"Online features (latest values):\")\n",
                "    for key, values in online_features.items():\n",
                "        print(f\"  {key}: {values}\")\n",
                "except:\n",
                "    print(\"[Simulated] Online lookup for user_id=1:\")\n",
                "    print(\"  daily_spend: 42.50\")\n",
                "    print(\"  num_sessions: 5\")\n",
                "    print(\"  items_purchased: 2\")\n",
                "    print(\"\\n>>> This would return in <10ms in production (Redis/DynamoDB)\")\n",
                "\n",
                "# Cleanup\n",
                "import shutil\n",
                "shutil.rmtree('feast_demo', ignore_errors=True)\n",
                "print(\"\\n(Cleaned up demo files)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Feature Store Best Practices <a id='7-best-practices'></a>\n",
                "\n",
                "| Practice | Description |\n",
                "|----------|-------------|\n",
                "| **Name features clearly** | `user_7d_avg_spend` not `feature_42` |\n",
                "| **Set appropriate TTL** | Stale features can degrade model performance |\n",
                "| **Monitor feature freshness** | Alert when features haven\u2019t been updated |\n",
                "| **Version feature definitions** | Track changes to feature logic |\n",
                "| **Document features** | Include business context, not just technical description |\n",
                "| **Share across teams** | Avoid duplicate feature engineering work |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Exercises <a id='8-exercises'></a>\n",
                "\n",
                "### Exercise 1: Feature Design\n",
                "Design a feature view for a fraud detection system. What entities, features, TTL, and data sources would you define?\n",
                "\n",
                "### Exercise 2: Point-in-Time Join Manually\n",
                "Given two DataFrames (user features with timestamps and training labels with timestamps), implement a point-in-time join using Pandas `.merge_asof()`.\n",
                "\n",
                "### Exercise 3: Feature Freshness\n",
                "Write a monitoring script that checks when each feature was last updated and alerts if any feature is older than its TTL.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Interview Preparation <a id='9-interview'></a>\n",
                "\n",
                "### Q1: \"What is training-serving skew and how does a feature store prevent it?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"Training-serving skew occurs when features used during training differ from those at inference. Causes include different code (Python vs Java), different data sources, or different time windows.\n",
                "\n",
                "A feature store prevents this by providing a **single source of truth** for features. Both training and serving read from the same definitions. The offline store serves historical features for training; the online store serves the latest values for inference. Same logic, same data \u2014 no skew.\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q2: \"Explain the dual-database architecture in a feature store.\"\n",
                "\n",
                "**Answer:**  \n",
                "\"Feature stores use two databases because training and serving have fundamentally different access patterns:\n",
                "\n",
                "- **Offline store** (Parquet/BigQuery): Stores full feature history. Used for batch training \u2014 high throughput, latency in seconds is fine.\n",
                "- **Online store** (Redis/DynamoDB): Stores only the latest feature values. Used for real-time serving \u2014 key-value lookups in <10ms.\n",
                "\n",
                "The `materialize` operation syncs data from offline to online. No single database optimizes for both patterns.\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q3: \"What is a point-in-time join and why is it critical?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"A point-in-time join retrieves features **as they existed at the time of each training example**, not as they exist today. Without it, you get temporal leakage \u2014 using future feature values to predict past events.\n",
                "\n",
                "Example: predicting churn for User X on Jan 15. A regular join might use User X\u2019s latest activity (from March), but a point-in-time join uses only data available before Jan 15. Feast automates this, ensuring temporal correctness.\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q4: \"Compare Feast to Tecton/Hopsworks. When would you choose each?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"**Feast** (open-source): Great for getting started, simple setup, Python-native. Best for teams that want control and don't need real-time feature computation.\n",
                "\n",
                "**Tecton** (managed): Built on top of Feast concepts but adds real-time feature transformations, streaming integration, and enterprise features. Best for teams needing production-grade with real-time features.\n",
                "\n",
                "**Hopsworks**: Full ML platform with built-in feature store. Best when you want an integrated solution. I\u2019d start with Feast for POC, then migrate to Tecton for production scale.\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q5: \"How do you handle feature freshness and staleness?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"Every feature view should have a **TTL (Time-To-Live)** that defines maximum allowed staleness. My approach:\n",
                "\n",
                "1. **Set TTL per feature**: User demographics (30 days), purchase history (1 day), real-time signals (1 hour)\n",
                "2. **Monitor materialization lag**: Alert when the gap between latest data and online store exceeds a threshold\n",
                "3. **Fallback strategy**: When features are stale, use default values or a simpler model rather than serving stale features\n",
                "4. **Dashboard**: Track feature freshness, null rates, and distribution drift\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83c\udf93 Key Takeaways\n",
                "\n",
                "1. **Training-serving skew** is the #1 operational ML problem \u2014 feature stores solve it\n",
                "2. **Dual-database architecture**: offline (batch/training) + online (real-time/serving)\n",
                "3. **Point-in-time joins** prevent temporal leakage in training data\n",
                "4. **Feast** is the go-to open-source feature store for Python-based ML teams\n",
                "5. **Feature sharing** across teams reduces duplicate engineering work\n",
                "6. **TTL and monitoring** ensure features stay fresh and reliable\n",
                "\n",
                "---\n",
                "\n",
                "\u27a1\ufe0f **Next Lesson**: [Lesson 6: Apache Spark & PySpark](./lesson_06_spark_pyspark.ipynb) \u2014 Scale your data processing."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}