{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 6: Quantization\n",
                "\n",
                "**Module 4: Model Development & Optimization**  \n",
                "**Estimated Time**: 1-2 hours  \n",
                "**Difficulty**: Advanced\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "âœ… Understand Floating Point (FP32) vs Integer (INT8) representation  \n",
                "âœ… Learn the trade-off: 4x Smaller Size, potential Accuracy drop  \n",
                "âœ… Implement Post-Training Quantization (PTQ) in PyTorch  \n",
                "âœ… Answer interview questions on Edge AI optimizations  \n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“š Table of Contents\n",
                "\n",
                "1. [The Math: FP32 to INT8](#1-math)\n",
                "2. [PTQ vs QAT](#2-types)\n",
                "3. [Hands-On: Quantizing ResNet](#3-hands-on)\n",
                "4. [Interview Preparation](#4-interview-questions)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Math: FP32 to INT8\n",
                "\n",
                "**Standard Model**: Weights are 32-bit floats (FP32).\n",
                "- Range: $\\pm 3.4 \\times 10^{38}$\n",
                "- Size: 4 bytes per weight.\n",
                "\n",
                "**Quantized Model**: Weights are 8-bit integers (INT8).\n",
                "- Range: $[-128, 127]$\n",
                "- Size: 1 byte per weight.\n",
                "\n",
                "**Benefit**: \n",
                "1. **4x Smaller Model Size** (100MB -> 25MB).\n",
                "2. **Faster Inference**: Integer math is cheap on CPU/DSP.\n",
                "\n",
                "**Mapping**: We map the min/max of the float range to -128/127.\n",
                "$Q(x) = \\text{round}(x / S + Z)$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. PTQ vs QAT\n",
                "\n",
                "### Post-Training Quantization (PTQ)\n",
                "- Train model in FP32 usually.\n",
                "- Convert to INT8 **after** training.\n",
                "- Needs a \"Calibration\" step (run a few images to find min/max).\n",
                "- **Pros**: Easy. **Cons**: Accuracy drop often significant.\n",
                "\n",
                "### Quantization-Aware Training (QAT)\n",
                "- Train model while **simulating** quantization errors.\n",
                "- The model learns to be robust to low precision.\n",
                "- **Pros**: Best accuracy. **Cons**: Complex setup."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Hands-On: Quantizing ResNet (PTQ)\n",
                "\n",
                "Using PyTorch's Eager Mode Quantization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from torchvision import models\n",
                "\n",
                "# 1. Load FP32 Model\n",
                "model_fp32 = models.resnet18(pretrained=True)\n",
                "model_fp32.eval()\n",
                "\n",
                "# 2. Prepare for Quantization\n",
                "# Fuse layers (Conv+BN+ReLU) for speed\n",
                "model_fp32.fuse_model()\n",
                "\n",
                "# 3. Configure Layout\n",
                "model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
                "torch.quantization.prepare(model_fp32, inplace=True)\n",
                "\n",
                "# 4. Calibration (Run dummy data)\n",
                "print(\"Calibrating...\")\n",
                "with torch.no_grad():\n",
                "    model_fp32(torch.randn(1, 3, 224, 224))\n",
                "\n",
                "# 5. Convert to INT8\n",
                "model_int8 = torch.quantization.convert(model_fp32, inplace=True)\n",
                "\n",
                "print(\"Quantization Complete.\")\n",
                "print(model_int8.conv1) # Look for QuantizedConv2d"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Interview Preparation\n",
                "\n",
                "### Common Questions\n",
                "\n",
                "#### Q1: \"Why do we need Calibration?\"\n",
                "**Answer**: \"To map FP32 values to INT8, we need to know the dynamic range (min/max) of activations. Calibration runs a small dataset through the model to observe these ranges so the mapping scale factor ($S$) is optimal.\"\n",
                "\n",
                "#### Q2: \"When to use QAT?\"\n",
                "**Answer**: \"If PTQ degrades accuracy too much (>1% drop). This often happens with small models (MobileNet) where every bit of precision counts.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}