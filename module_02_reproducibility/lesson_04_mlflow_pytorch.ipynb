{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 4: MLflow with PyTorch\n",
                "\n",
                "**Module 2: Reproducibility & Versioning**  \n",
                "**Estimated Time**: 2-3 hours  \n",
                "**Difficulty**: Intermediate\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "âœ… Learn how to track Deep Learning training loops  \n",
                "âœ… Implement `mlflow.pytorch.autolog()` vs manual logging  \n",
                "âœ… Track loss curves over epochs  \n",
                "âœ… Save and load PyTorch models with MLflow  \n",
                "âœ… Answer interview questions on DL experiment tracking  \n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“š Table of Contents\n",
                "\n",
                "1. [Deep Learning Tracking Challenges](#1-dl-challenges)\n",
                "2. [The Easy Way: Autologging](#2-autolog)\n",
                "3. [The Custom Way: Manual Training Loop](#3-manual-loop)\n",
                "4. [Hands-On: PyTorch MNIST Example](#4-hands-on)\n",
                "5. [Interview Preparation](#5-interview-questions)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Deep Learning Tracking Challenges\n",
                "\n",
                "Comparing sklearn (Lesson 3) vs PyTorch:\n",
                "\n",
                "**Sklearn**:\n",
                "- 1 `fit()` call.\n",
                "- Metrics are usually calculated once at the end (Accuracy, RMSE).\n",
                "\n",
                "**PyTorch / Deep Learning**:\n",
                "- Iterative `for` loops (Epochs, Batches).\n",
                "- Metrics change constantly (Loss decreases, Accuracy increases).\n",
                "- Need to visualize **curves**, not just final numbers.\n",
                " \n",
                "**Goal**: We want to see a chart of Training Loss vs Validation Loss in MLflow."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. The Easy Way: Autologging\n",
                "\n",
                "MLflow supports automatic logging for PyTorch Lightning/Ignite (and raw PyTorch to some extent).\n",
                "\n",
                "```python\n",
                "import mlflow.pytorch\n",
                "\n",
                "mlflow.pytorch.autolog()\n",
                "\n",
                "# ... Training code ...\n",
                "```\n",
                "\n",
                "**Pros**: Zero code changes. captures common metrics.\n",
                "**Cons**: Less control over exactly what/when to log. Not perfect for raw PyTorch loops."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. The Custom Way: Manual Training Loop\n",
                "\n",
                "This is the standard for raw PyTorch. You inject `mlflow.log_metric()` inside your loop.\n",
                "\n",
                "```python\n",
                "with mlflow.start_run():\n",
                "    for epoch in range(epochs):\n",
                "        loss = train_one_epoch()\n",
                "        # The key: pass the 'step' argument!\n",
                "        mlflow.log_metric(\"train_loss\", loss, step=epoch)\n",
                "```\n",
                "\n",
                "Using `step=epoch` allows MLflow UI to draw the line chart."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Hands-On: PyTorch MNIST Example\n",
                "\n",
                "Let's simulate a simplified training loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import mlflow\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "\n",
                "# 1. Define a Simple Model\n",
                "class SimpleNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.fc = nn.Linear(10, 1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.fc(x)\n",
                "\n",
                "# 2. Training Function with MLflow\n",
                "def train_model(lr=0.01, epochs=5):\n",
                "    mlflow.set_experiment(\"pytorch_demo\")\n",
                "    \n",
                "    with mlflow.start_run():\n",
                "        # Log Params\n",
                "        mlflow.log_param(\"lr\", lr)\n",
                "        mlflow.log_param(\"epochs\", epochs)\n",
                "        \n",
                "        model = SimpleNet()\n",
                "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
                "        criterion = nn.MSELoss()\n",
                "        \n",
                "        # Dummy Data\n",
                "        inputs = torch.randn(5, 10)\n",
                "        targets = torch.randn(5, 1)\n",
                "        \n",
                "        print(f\"Starting training with lr={lr}...\")\n",
                "        for epoch in range(epochs):\n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(inputs)\n",
                "            loss = criterion(outputs, targets)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            # Log Metric PER EPOCH\n",
                "            current_loss = loss.item()\n",
                "            mlflow.log_metric(\"loss\", current_loss, step=epoch)\n",
                "            print(f\"Epoch {epoch}: Loss={current_loss:.4f}\")\n",
                "        \n",
                "        # Log Final Model\n",
                "        # MLflow handles saving the state_dict and wrapping it\n",
                "        mlflow.pytorch.log_model(model, \"model\")\n",
                "        print(\"Run Complete. Model saved to MLflow.\")\n",
                "\n",
                "# 3. Run It\n",
                "train_model(lr=0.1, epochs=10)\n",
                "train_model(lr=0.01, epochs=10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Interview Preparation\n",
                "\n",
                "### Common Questions\n",
                "\n",
                "#### Q1: \"How do you visualize overfitting in MLflow?\"\n",
                "**Answer**: I log both `train_loss` and `val_loss` metrics at each epoch. In the MLflow UI, I can plot both lines on the same chart. If `train_loss` keeps going down while `val_loss` starts going up, that divergence indicates overfitting.\n",
                "\n",
                "#### Q2: \"When saving a PyTorch model in MLflow, what actually gets saved?\"\n",
                "**Answer**: `mlflow.pytorch.log_model` typically saves the model using `torch.save` (pickle format) or `state_dict`. It also saves a `MLmodel` configuration file and a `conda.yaml` defining the environment (python version, torch version) needed to run it. This ensures reproducibility.\n",
                "\n",
                "#### Q3: \"Can I log images (like generated GAN outputs) to MLflow?\"\n",
                "**Answer**: Yes, using `mlflow.log_artifact()`. For example, at the end of every 10 epochs, I can save a generated image to a local file `output_epoch_10.png` and then log it. It will appear in the artifact viewer in the UI."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}