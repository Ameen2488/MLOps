{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Project Step 3: Optimization (ONNX + Quantization)\n",
                "\n",
                "**Objective**: Deploy the model efficiently by converting it to **ONNX** and **Quantizing** it.\n",
                "\n",
                "**Why?**\n",
                "- **ONNX**: Open Standard. Runs on C++, C#, Java, JavaScript. No Python dependency in production.\n",
                "- **Quantization**: FP32 -> INT8. Reduces size by 75% and speeds up inference on CPU.\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“š Steps\n",
                "1. **Export**: Convert YOLOv8 to ONNX.\n",
                "2. **Benchmark FP32**: Measure speed of the ONNX model.\n",
                "3. **Quantize**: Convert to INT8.\n",
                "4. **Benchmark INT8**: Compare size and speed."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Export to ONNX\n",
                "\n",
                "Ultralytics has a built-in export function that simplifies this."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ultralytics import YOLO\n",
                "import os\n",
                "\n",
                "# Load the model we trained in Step 2\n",
                "# If it doesn't exist, we load the pre-trained n-model\n",
                "model_path = \"../models/yolo_baseline/weights/best.pt\"\n",
                "if not os.path.exists(model_path):\n",
                "    print(\"Custom model not found, using standard yolov8n.pt\")\n",
                "    model_path = \"yolov8n.pt\"\n",
                "\n",
                "model = YOLO(model_path)\n",
                "\n",
                "# Export to ONNX\n",
                "print(\"Exporting to ONNX...\")\n",
                "success = model.export(format=\"onnx\", dynamic=False, imgsz=640)\n",
                "\n",
                "# The file is saved alongside the .pt file\n",
                "onnx_path = model_path.replace(\".pt\", \".onnx\")\n",
                "print(f\"Exported to: {onnx_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Benchmark FP32 (ONNX Runtime)\n",
                "\n",
                "We use `onnxruntime` to run inference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import onnxruntime as ort\n",
                "import numpy as np\n",
                "import time\n",
                "\n",
                "# Create Session\n",
                "session = ort.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\n",
                "\n",
                "# Get Input Name\n",
                "input_name = session.get_inputs()[0].name\n",
                "\n",
                "# Create Dummy Input (Batch 1, 3 Channels, 640x640)\n",
                "dummy_input = np.random.randn(1, 3, 640, 640).astype(np.float32)\n",
                "\n",
                "# Warmup\n",
                "for _ in range(5):\n",
                "    session.run(None, {input_name: dummy_input})\n",
                "\n",
                "# Benchmark\n",
                "start = time.time()\n",
                "for _ in range(50):\n",
                "    session.run(None, {input_name: dummy_input})\n",
                "end = time.time()\n",
                "\n",
                "fps_fp32 = 50 / (end - start)\n",
                "print(f\"FP32 Inference Speed: {fps_fp32:.2f} FPS\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Quantize to INT8\n",
                "\n",
                "We use `onnxruntime.quantization` to apply dynamic quantization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
                "\n",
                "int8_path = onnx_path.replace(\".onnx\", \"_int8.onnx\")\n",
                "\n",
                "print(\"Quantizing...\")\n",
                "quantize_dynamic(\n",
                "    onnx_path,\n",
                "    int8_path,\n",
                "    weight_type=QuantType.QUInt8\n",
                ")\n",
                "print(f\"Quantized Model saved to: {int8_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Compare Size & Speed\n",
                "\n",
                "Let's see the benefits."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare Size\n",
                "size_fp32 = os.path.getsize(onnx_path) / (1024 * 1024)\n",
                "size_int8 = os.path.getsize(int8_path) / (1024 * 1024)\n",
                "\n",
                "print(f\"FP32 Size: {size_fp32:.2f} MB\")\n",
                "print(f\"INT8 Size: {size_int8:.2f} MB\")\n",
                "print(f\"Reduction: {(1 - size_int8/size_fp32):.0%}\")\n",
                "\n",
                "# Benchmark INT8\n",
                "session_int8 = ort.InferenceSession(int8_path, providers=['CPUExecutionProvider'])\n",
                "\n",
                "# Warmup\n",
                "for _ in range(5):\n",
                "    session_int8.run(None, {input_name: dummy_input})\n",
                "\n",
                "start = time.time()\n",
                "for _ in range(50):\n",
                "    session_int8.run(None, {input_name: dummy_input})\n",
                "end = time.time()\n",
                "\n",
                "fps_int8 = 50 / (end - start)\n",
                "print(f\"INT8 Inference Speed: {fps_int8:.2f} FPS\")\n",
                "print(f\"Speedup: {fps_int8/fps_fp32:.2f}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "You have successfully:\n",
                "1. Downloaded & Versioned data (DVC).\n",
                "2. Configured & Tracked training (Hydra + W&B).\n",
                "3. Optimized the model for deployment (ONNX + Quantization).\n",
                "\n",
                "This represents a complete **MLOps Model Development Lifecycle**."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}