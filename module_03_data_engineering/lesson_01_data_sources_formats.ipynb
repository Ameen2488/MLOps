{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 1: Data Sources & Formats\n",
                "\n",
                "**Module 3: Data & Pipeline Engineering**  \n",
                "**Estimated Time**: 1-2 hours  \n",
                "**Difficulty**: Beginner-Intermediate\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "âœ… Understand the trade-offs between CSV, Parquet, and Avro  \n",
                "âœ… Learn why Columnar Storage is critical for analytics and ML  \n",
                "âœ… Benchmark read/write speeds of different formats  \n",
                "âœ… Answer interview questions on Big Data formats  \n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“š Table of Contents\n",
                "\n",
                "1. [The Big Three: CSV, Parquet, Avro](#1-formats)\n",
                "2. [Deep Dive: Row vs Columnar Storage](#2-storage)\n",
                "3. [Hands-On: Benchmarking Performance](#3-hands-on)\n",
                "4. [Compression Strategies](#4-compression)\n",
                "5. [Interview Preparation](#5-interview-questions)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Big Three: CSV, Parquet, Avro\n",
                "\n",
                "| Feature | CSV | Parquet | Avro |\n",
                "|---------|-----|---------|------|\n",
                "| **Type** | Text (Human Readable) | Binary (Columnar) | Binary (Row-based) |\n",
                "| **Schema** | None (Inferred) | Embedded in footer | JSON Schema header |\n",
                "| **Use Case** | Excel, small data | Analytics, ML training | Streaming (Kafka) |\n",
                "| **Compression** | Poor | Excellent (Snappy/Gzip) | Good |\n",
                "| **Write Speed** | Slow | Slow (Encoding overhead) | Fast |\n",
                "| **Read Speed** | Very Slow | Very Fast (Column pruning) | Fast |\n",
                "\n",
                "**Key Insight**: For training ML models on tabular data, **Parquet** is almost always the best choice."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Deep Dive: Row vs Columnar Storage\n",
                "\n",
                "### Row-Oriented (CSV, Database, Avro)\n",
                "Stores data record by record:  \n",
                "`[ID:1, Name:John, Age:30], [ID:2, Name:Jane, Age:25]`\n",
                "\n",
                "**Good for**: Transactional (OLTP) systems. writing ONE new user.\n",
                "\n",
                "### Column-Oriented (Parquet)\n",
                "Stores data column by column:  \n",
                "`IDs:[1, 2], Names:[John, Jane], Ages:[30, 25]`\n",
                "\n",
                "**Good for**: Analytical (OLAP) queries. \"Calculate average Age\".\n",
                "\n",
                "**Why for ML?**\n",
                "When training, you often select specific features (columns). Parquet allows you to read ONLY the columns you need, ignoring the rest. This drastically reduces I/O."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Hands-On: Benchmarking Performance\n",
                "\n",
                "Simulate a large dataset and compare formats."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import time\n",
                "import os\n",
                "\n",
                "# 1. Create a \"Large\" Dataset (1 Million rows)\n",
                "print(\"Generating data...\")\n",
                "n_rows = 1_000_000\n",
                "df = pd.DataFrame({\n",
                "    'id': np.arange(n_rows),\n",
                "    'category': np.random.choice(['A', 'B', 'C'], n_rows),\n",
                "    'value1': np.random.randn(n_rows),\n",
                "    'value2': np.random.randn(n_rows) \n",
                "})\n",
                "\n",
                "# 2. Benchmark CSV\n",
                "start = time.time()\n",
                "df.to_csv('data.csv', index=False)\n",
                "write_csv = time.time() - start\n",
                "\n",
                "start = time.time()\n",
                "pd.read_csv('data.csv')\n",
                "read_csv = time.time() - start\n",
                "size_csv = os.path.getsize('data.csv') / (1024 * 1024)\n",
                "\n",
                "# 3. Benchmark Parquet (Snappy compression default)\n",
                "start = time.time()\n",
                "df.to_parquet('data.parquet', index=False)\n",
                "write_pq = time.time() - start\n",
                "\n",
                "start = time.time()\n",
                "pd.read_parquet('data.parquet')\n",
                "read_pq = time.time() - start\n",
                "size_pq = os.path.getsize('data.parquet') / (1024 * 1024)\n",
                "\n",
                "# 4. Results\n",
                "print(f\"\\n{'Format':<10} {'Write(s)':<10} {'Read(s)':<10} {'Size(MB)':<10}\")\n",
                "print(\"-\"*40)\n",
                "print(f\"{'CSV':<10} {write_csv:<10.2f} {read_csv:<10.2f} {size_csv:<10.2f}\")\n",
                "print(f\"{'Parquet':<10} {write_pq:<10.2f} {read_pq:<10.2f} {size_pq:<10.2f}\")\n",
                "\n",
                "print(f\"\\nParquet is {size_csv/size_pq:.1f}x smaller and {read_csv/read_pq:.1f}x faster to read!\")\n",
                "\n",
                "# Cleanup\n",
                "os.remove('data.csv')\n",
                "os.remove('data.parquet')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Interview Preparation\n",
                "\n",
                "### Common Questions\n",
                "\n",
                "#### Q1: \"Why is Parquet preferred over CSV for S3 data lakes?\"\n",
                "**Answer**: \n",
                "1. **Columnar Storage**: Allows scanning only required columns (saving I/O and cost with Athena/Spark).\n",
                "2. **Schema Enforcement**: Stores data types, preventing \"everything is a string\" issues common in CSV.\n",
                "3. **Compression**: Significantly smaller file sizes (saving storage cost).\n",
                "4. **Splittable**: Compression blocks can be processed in parallel by Spark executors.\n",
                "\n",
                "#### Q2: \"When would you use Avro?\"\n",
                "**Answer**: \"I prioritize Avro for **streaming data pipelines** (e.g., Kafka). It is row-based, making it efficient for writing records one by one. It also handles schema evolution (adding fields) very gracefully, which is crucial for long-running producers/consumers.\"\n",
                "\n",
                "#### Q3: \"Explain Dictionary Encoding in Parquet.\"\n",
                "**Answer**: \"For categorical columns with low cardinality (few unique values), Parquet stores a dictionary of values (e.g., ['A', 'B']) and stores row values as tiny integers (indexes). This provides massive compression for categorical data.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}