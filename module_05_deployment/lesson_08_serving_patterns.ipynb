{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 8: Serving Patterns\n",
                "\n",
                "**Module 5: Model Deployment**  \n",
                "**Estimated Time**: 1 hour  \n",
                "**Difficulty**: Advanced\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "âœ… Understand **Online**, **Batch**, and **Streaming** inference  \n",
                "âœ… Design data flows for each pattern  \n",
                "âœ… Answer **System Design** interview questions  \n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“š Table of Contents\n",
                "\n",
                "1. [Pattern 1: Online Inference (RPC)](#1-online)\n",
                "2. [Pattern 2: Batch Inference](#2-batch)\n",
                "3. [Pattern 3: Streaming Inference](#3-streaming)\n",
                "4. [Interview Preparation](#4-interview-questions)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Pattern 1: Online Inference (RPC)\n",
                "\n",
                "**Scenario**: User clicks \"Upload Photo\". App shows \"Cat\" immediately.\n",
                "- **Flow**: Client -> HTTP Request -> Model -> HTTP Response.\n",
                "- **Metric**: Latency (ms).\n",
                "- **Tech**: FastAPI, KServe, Triton."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Pattern 2: Batch Inference\n",
                "\n",
                "**Scenario**: Every night, predict churn probability for all 1M users.\n",
                "- **Flow**: Database Table -> Spark/Airflow Job -> Model -> Result Table.\n",
                "- **Metric**: Throughput (Records per second).\n",
                "- **Tech**: Airflow, Spark ML."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Pattern 3: Streaming Inference\n",
                "\n",
                "**Scenario**: Credit Card Fraud Detection. Check transaction as it happens, but asynchronously.\n",
                "- **Flow**: App -> Kafka Topic (`transactions`) -> Consumer (Model) -> Kafka Topic (`fraud_alerts`).\n",
                "- **Metric**: Lag (seconds).\n",
                "- **Tech**: Kafka, Flink."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Interview Preparation\n",
                "\n",
                "### Common Questions\n",
                "\n",
                "#### Q1: \"Design a Fraud Detection System.\"\n",
                "**Answer**: \n",
                "- \"Fraud needs to be detected quickly, so Batch is out.\"\n",
                "- \"Blocking the transaction (Online) is risky; if the model is slow, the user's card fails.\"\n",
                "- \"Best approach: **Async/Streaming**. Let the payment go through (or hold for 200ms), push event to Kafka. Model scores it. If Fraud > 0.9, trigger an alert/block subsequent transactions. This decouples the payment gateway from the heavy ML model.\"\n",
                "\n",
                "#### Q2: \"How to serve a 100GB LLM?\"\n",
                "**Answer**: \"You cannot put this in a standard API container easily. Use specific Model Serving tools like **vLLM** or **TGI** (Text Generation Inference) which handle issues like KV Cache, Continuous Batching, and Tensor Parallelism across multiple GPUs.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}