{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 6: Apache Spark & PySpark for ML Data Processing\n",
                "\n",
                "**Module 3: Data & Pipeline Engineering** | **Time**: 4-5 hours | **Difficulty**: Intermediate-Advanced\n",
                "\n",
                "---\n",
                "\n",
                "## \ud83c\udfaf Learning Objectives\n",
                "\n",
                "\u2705 Understand why single-machine processing fails at scale  \n",
                "\u2705 Learn Spark\u2019s distributed architecture (Driver, Executors, DAG)  \n",
                "\u2705 Master PySpark DataFrame operations for ML data prep  \n",
                "\u2705 Understand partitioning, caching, and optimization  \n",
                "\u2705 Answer 5 interview questions on Spark and distributed computing  \n",
                "\n",
                "---\n",
                "\n",
                "## \ud83d\udcda Table of Contents\n",
                "\n",
                "1. [Why Spark? The Scale Problem](#1-why-spark)\n",
                "2. [Spark Architecture](#2-architecture)\n",
                "3. [Spark Execution Model: Lazy Evaluation + DAG](#3-execution)\n",
                "4. [PySpark DataFrame Basics](#4-pyspark-basics)\n",
                "5. [Data Transformations for ML](#5-transformations)\n",
                "6. [Partitioning & Optimization](#6-partitioning)\n",
                "7. [Spark vs Pandas Comparison](#7-spark-vs-pandas)\n",
                "8. [Exercises](#8-exercises)\n",
                "9. [Interview Preparation](#9-interview)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Why Spark? The Scale Problem <a id='1-why-spark'></a>\n",
                "\n",
                "Pandas is brilliant for datasets that fit in memory on a single machine. But what happens when they don\u2019t?\n",
                "\n",
                "### The Scale Wall\n",
                "\n",
                "```\n",
                "  Dataset Size vs Processing Approach:\n",
                "\n",
                "  < 1 GB     \u2500\u2500\u25b6  Pandas on laptop         \u2705 Easy\n",
                "  1-10 GB    \u2500\u2500\u25b6  Pandas with chunking     \u26a0\ufe0f Painful\n",
                "  10-100 GB  \u2500\u2500\u25b6  Pandas fails             \u274c Out of memory\n",
                "  100+ GB    \u2500\u2500\u25b6  Need distributed system  \ud83d\ude80 Spark!\n",
                "  1+ TB      \u2500\u2500\u25b6  Spark required           \ud83d\ude80\ud83d\ude80 Spark!\n",
                "```\n",
                "\n",
                "### Pandas vs Spark Under the Hood\n",
                "\n",
                "```\n",
                "  PANDAS (Single Machine):\n",
                "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "  \u2502      Your Laptop (32 GB)      \u2502\n",
                "  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n",
                "  \u2502  \u2502 ENTIRE DataFrame in   \u2502  \u2502\n",
                "  \u2502  \u2502 RAM at once           \u2502  \u2502\n",
                "  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n",
                "  \u2502  100 GB dataset? OOM! \ud83d\udca5      \u2502\n",
                "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "\n",
                "  SPARK (Distributed Cluster):\n",
                "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "  \u2502 Worker1 \u2502 \u2502 Worker2 \u2502 \u2502 Worker3 \u2502 \u2502 Worker4 \u2502\n",
                "  \u2502 25 GB   \u2502 \u2502 25 GB   \u2502 \u2502 25 GB   \u2502 \u2502 25 GB   \u2502\n",
                "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "       100 GB split across 4 workers \u2705\n",
                "       Each processes its chunk in parallel!\n",
                "```\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Spark Architecture <a id='2-architecture'></a>\n",
                "\n",
                "### Core Components\n",
                "\n",
                "```\n",
                "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "  \u2502                    SPARK APPLICATION                     \u2502\n",
                "  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                "  \u2502  DRIVER (your program)                                   \u2502\n",
                "  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n",
                "  \u2502  \u2502 SparkContext \u2192 Creates DAG \u2192 Submits Jobs           \u2502  \u2502\n",
                "  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n",
                "  \u2502                         \u2502                                \u2502\n",
                "  \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n",
                "  \u2502  CLUSTER    \u2502  CLUSTER MANAGER   \u2502                  \u2502\n",
                "  \u2502  MANAGER    \u2502  (YARN/Mesos/K8s)  \u2502                  \u2502\n",
                "  \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n",
                "  \u2502                         \u2502                                \u2502\n",
                "  \u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n",
                "  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n",
                "  \u2502  \u2502 Executor \u2502  \u2502 Executor \u2502  \u2502 Executor \u2502      \u2502\n",
                "  \u2502  \u2502 [Task]   \u2502  \u2502 [Task]   \u2502  \u2502 [Task]   \u2502      \u2502\n",
                "  \u2502  \u2502 [Task]   \u2502  \u2502 [Task]   \u2502  \u2502 [Task]   \u2502      \u2502\n",
                "  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n",
                "  \u2502     Worker 1       Worker 2       Worker 3       \u2502\n",
                "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "```\n",
                "\n",
                "| Component | Role |\n",
                "|-----------|------|\n",
                "| **Driver** | Your PySpark program. Creates the SparkContext, builds the DAG of operations, submits jobs |\n",
                "| **Cluster Manager** | Allocates resources (YARN, Mesos, Kubernetes, or Standalone) |\n",
                "| **Executors** | JVM processes on worker nodes. Execute tasks and store data in memory/disk |\n",
                "| **Tasks** | Individual unit of work. One task per partition per stage |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Spark Execution Model: Lazy Evaluation + DAG <a id='3-execution'></a>\n",
                "\n",
                "Spark uses **lazy evaluation**: transformations are NOT executed immediately. Instead, Spark builds a **DAG (Directed Acyclic Graph)** of operations. Execution only happens when an **action** is called.\n",
                "\n",
                "### Transformations vs Actions\n",
                "\n",
                "```\n",
                "  TRANSFORMATIONS (Lazy - build the plan):\n",
                "  .filter()  .select()  .groupBy()  .join()  .withColumn()\n",
                "        \u2502         \u2502          \u2502         \u2502          \u2502\n",
                "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "                        \u2502\n",
                "                   Build DAG\n",
                "                   (no execution yet!)\n",
                "                        \u2502\n",
                "  ACTIONS (Trigger execution):\n",
                "  .show()  .collect()  .count()  .write()  .toPandas()\n",
                "        \u2502\n",
                "    NOW the DAG executes!\n",
                "    Spark optimizes the entire plan first.\n",
                "```\n",
                "\n",
                "### Why Lazy Evaluation?\n",
                "\n",
                "```\n",
                "  Example: df.filter(col('age') > 30).select('name', 'age')\n",
                "\n",
                "  Eager (like Pandas):              Lazy (Spark):\n",
                "  1. Load ALL columns               1. Build plan\n",
                "  2. Filter rows                    2. Optimize: push filter down\n",
                "  3. Select 2 columns               3. Execute: read only 2 columns\n",
                "  4. Wasted: loaded unused columns      + filter in one pass\n",
                "\n",
                "  Lazy is SMARTER \u2014 it sees the whole plan and optimizes!\n",
                "```\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. PySpark DataFrame Basics <a id='4-pyspark-basics'></a>\n",
                "\n",
                "PySpark DataFrames look similar to Pandas but run on a distributed cluster."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# PySpark Setup (local mode for learning)\n",
                "# In production, Spark runs on a cluster (YARN, K8s, Databricks)\n",
                "# ============================================================\n",
                "try:\n",
                "    from pyspark.sql import SparkSession\n",
                "    from pyspark.sql import functions as F\n",
                "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
                "    \n",
                "    spark = SparkSession.builder \\\n",
                "        .master('local[*]') \\\n",
                "        .appName('Module3_Lesson6') \\\n",
                "        .getOrCreate()\n",
                "    \n",
                "    spark.sparkContext.setLogLevel('ERROR')\n",
                "    print(f\"\u2705 Spark {spark.version} initialized (local mode)\")\n",
                "    print(f\"   Cores: {spark.sparkContext.defaultParallelism}\")\n",
                "    SPARK_AVAILABLE = True\n",
                "except ImportError:\n",
                "    print(\"\u26a0\ufe0f PySpark not installed. Run: pip install pyspark\")\n",
                "    print(\"   The code examples below show what would execute.\")\n",
                "    SPARK_AVAILABLE = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Creating DataFrames in PySpark\n",
                "# ============================================================\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Generate synthetic ML dataset\n",
                "np.random.seed(42)\n",
                "n = 100_000\n",
                "\n",
                "pandas_df = pd.DataFrame({\n",
                "    'user_id': np.arange(n),\n",
                "    'category': np.random.choice(['electronics', 'clothing', 'food', 'books'], n),\n",
                "    'price': np.round(np.random.uniform(5, 500, n), 2),\n",
                "    'quantity': np.random.randint(1, 10, n),\n",
                "    'rating': np.round(np.random.uniform(1, 5, n), 1),\n",
                "})\n",
                "\n",
                "if SPARK_AVAILABLE:\n",
                "    # Convert Pandas to Spark DataFrame\n",
                "    sdf = spark.createDataFrame(pandas_df)\n",
                "    \n",
                "    # Basic operations\n",
                "    print(\"Schema:\")\n",
                "    sdf.printSchema()\n",
                "    \n",
                "    print(f\"\\nPartitions: {sdf.rdd.getNumPartitions()}\")\n",
                "    print(f\"Row count: {sdf.count():,}\")\n",
                "    \n",
                "    sdf.show(5)\n",
                "else:\n",
                "    print(\"[Without Spark] Pandas DataFrame:\")\n",
                "    print(pandas_df.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Common PySpark Operations (Pandas equivalents shown)\n",
                "# ============================================================\n",
                "if SPARK_AVAILABLE:\n",
                "    # SELECT columns\n",
                "    print(\"1. SELECT columns:\")\n",
                "    sdf.select('category', 'price').show(3)\n",
                "    \n",
                "    # FILTER rows\n",
                "    print(\"2. FILTER rows:\")\n",
                "    sdf.filter(F.col('price') > 400).show(3)\n",
                "    \n",
                "    # ADD new column\n",
                "    print(\"3. ADD computed column:\")\n",
                "    sdf.withColumn('total', F.col('price') * F.col('quantity')).show(3)\n",
                "    \n",
                "    # GROUP BY + aggregate\n",
                "    print(\"4. GROUP BY + AGGREGATE:\")\n",
                "    sdf.groupBy('category').agg(\n",
                "        F.avg('price').alias('avg_price'),\n",
                "        F.sum('quantity').alias('total_qty'),\n",
                "        F.count('*').alias('count')\n",
                "    ).show()\n",
                "else:\n",
                "    print(\"PySpark equivalents of common Pandas operations:\")\n",
                "    print(\"\"\"  \n",
                "    | Pandas                          | PySpark                                  |\n",
                "    |---------------------------------|------------------------------------------|\n",
                "    | df[['cat', 'price']]            | sdf.select('cat', 'price')               |\n",
                "    | df[df.price > 400]              | sdf.filter(F.col('price') > 400)         |\n",
                "    | df['total'] = df.price * df.qty | sdf.withColumn('total', col*col)         |\n",
                "    | df.groupby('cat').agg(...)       | sdf.groupBy('cat').agg(F.avg('price'))   |\n",
                "    \"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Data Transformations for ML <a id='5-transformations'></a>\n",
                "\n",
                "### Common ML Feature Engineering in PySpark\n",
                "\n",
                "```\n",
                "  Raw Data \u2500\u25b6 Feature Engineering Pipeline in Spark:\n",
                "\n",
                "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "  \u2502 Handle   \u2502\u25b6\u2502 Encode   \u2502\u25b6\u2502 Window   \u2502\u25b6\u2502 Scale    \u2502\u25b6\u2502 Save as \u2502\n",
                "  \u2502 Nulls    \u2502  \u2502 Categor. \u2502  \u2502 Features \u2502  \u2502 Numeric  \u2502  \u2502 Parquet \u2502\n",
                "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# ML Feature Engineering in PySpark\n",
                "# ============================================================\n",
                "if SPARK_AVAILABLE:\n",
                "    from pyspark.sql.window import Window\n",
                "    \n",
                "    # 1. Handle nulls\n",
                "    sdf_clean = sdf.fillna({'rating': 3.0, 'price': 0})\n",
                "    \n",
                "    # 2. One-hot encode categoricals (StringIndexer + OneHotEncoder)\n",
                "    from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
                "    indexer = StringIndexer(inputCol='category', outputCol='category_idx')\n",
                "    sdf_indexed = indexer.fit(sdf_clean).transform(sdf_clean)\n",
                "    \n",
                "    # 3. Window features (rolling aggregates)\n",
                "    window = Window.partitionBy('category').orderBy('user_id').rowsBetween(-10, 0)\n",
                "    sdf_features = sdf_indexed.withColumn(\n",
                "        'rolling_avg_price', F.avg('price').over(window)\n",
                "    )\n",
                "    \n",
                "    # 4. Interaction features\n",
                "    sdf_features = sdf_features.withColumn(\n",
                "        'price_x_qty', F.col('price') * F.col('quantity')\n",
                "    )\n",
                "    \n",
                "    # 5. Binning\n",
                "    sdf_features = sdf_features.withColumn(\n",
                "        'price_bucket',\n",
                "        F.when(F.col('price') < 50, 'low')\n",
                "         .when(F.col('price') < 200, 'medium')\n",
                "         .otherwise('high')\n",
                "    )\n",
                "    \n",
                "    print(\"Engineered features:\")\n",
                "    sdf_features.select('category', 'price', 'category_idx', \n",
                "                        'rolling_avg_price', 'price_x_qty', 'price_bucket').show(5)\n",
                "else:\n",
                "    print(\"PySpark ML feature engineering patterns:\")\n",
                "    print(\"1. fillna() - Handle missing values\")\n",
                "    print(\"2. StringIndexer + OneHotEncoder - Encode categoricals\")\n",
                "    print(\"3. Window functions - Rolling aggregates\")\n",
                "    print(\"4. withColumn() - Interaction features\")\n",
                "    print(\"5. when/otherwise - Binning\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Partitioning & Optimization <a id='6-partitioning'></a>\n",
                "\n",
                "### What Is a Partition?\n",
                "\n",
                "```\n",
                "  A Spark DataFrame is split into PARTITIONS:\n",
                "  Each partition is processed by ONE task on ONE core.\n",
                "\n",
                "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                "  \u2502              DataFrame (100M rows)            \u2502\n",
                "  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                "  \u2502 Part 0   \u2502 Part 1   \u2502 Part 2   \u2502 Part 3   \u2502\n",
                "  \u2502 25M rows \u2502 25M rows \u2502 25M rows \u2502 25M rows \u2502\n",
                "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                "    Core 1     Core 2     Core 3     Core 4\n",
                "    (parallel execution)\n",
                "\n",
                "  Rule of thumb: 2-4 partitions per CPU core\n",
                "  Too few partitions:  underutilized cores\n",
                "  Too many partitions: scheduling overhead\n",
                "```\n",
                "\n",
                "### Narrow vs Wide Transformations\n",
                "\n",
                "```\n",
                "  NARROW (no shuffle):          WIDE (requires shuffle):\n",
                "  filter, select, map           groupBy, join, repartition\n",
                "\n",
                "  \u250c\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2510\n",
                "  \u2502 P0 \u2502  \u2502 P1 \u2502  \u2502 P2 \u2502       \u2502 P0 \u2502  \u2502 P1 \u2502  \u2502 P2 \u2502\n",
                "  \u2514\u2500\u252c\u2500\u2500\u2518  \u2514\u2500\u252c\u2500\u2500\u2518  \u2514\u2500\u252c\u2500\u2500\u2518       \u2514\u2500\u252c\u2500\u2500\u2518  \u2514\u2500\u252c\u2500\u2500\u2518  \u2514\u2500\u252c\u2500\u2500\u2518\n",
                "    \u2502       \u2502       \u2502             \u2502 \u2572  \u2502 /  \u2502\n",
                "    \u2502       \u2502       \u2502             \u2502  \u2572\u2502/   \u2502    SHUFFLE!\n",
                "  \u250c\u2500\u2534\u2500\u2500\u2510  \u250c\u2500\u2534\u2500\u2500\u2510  \u250c\u2500\u2534\u2500\u2500\u2510       \u250c\u2500\u2534\u2500\u2500\u2510  \u250c\u2500\u2534\u2500\u2500\u2510  \u250c\u2500\u2534\u2500\u2500\u2510\n",
                "  \u2502 P0 \u2502  \u2502 P1 \u2502  \u2502 P2 \u2502       \u2502 P0 \u2502  \u2502 P1 \u2502  \u2502 P2 \u2502\n",
                "  \u2514\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2518\n",
                "  Fast! No data movement.       Expensive! Data moves across network.\n",
                "```\n",
                "\n",
                "### Key Optimization Tips\n",
                "\n",
                "| Tip | Description |\n",
                "|-----|-------------|\n",
                "| **Minimize shuffles** | Avoid unnecessary groupBy/join operations |\n",
                "| **Cache wisely** | Cache DataFrames you reuse: `sdf.cache()` |\n",
                "| **Use Parquet** | Enables predicate pushdown and column pruning |\n",
                "| **Broadcast small tables** | For joins with small lookup tables |\n",
                "| **Partition by** | When writing: `sdf.write.partitionBy('date')` |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Optimization Demo: Caching and Partition Management\n",
                "# ============================================================\n",
                "if SPARK_AVAILABLE:\n",
                "    import time\n",
                "    \n",
                "    # Without caching\n",
                "    start = time.time()\n",
                "    sdf.groupBy('category').agg(F.avg('price')).collect()\n",
                "    sdf.groupBy('category').agg(F.sum('quantity')).collect()\n",
                "    no_cache = time.time() - start\n",
                "    \n",
                "    # With caching\n",
                "    sdf.cache()\n",
                "    sdf.count()  # Trigger caching\n",
                "    \n",
                "    start = time.time()\n",
                "    sdf.groupBy('category').agg(F.avg('price')).collect()\n",
                "    sdf.groupBy('category').agg(F.sum('quantity')).collect()\n",
                "    with_cache = time.time() - start\n",
                "    \n",
                "    sdf.unpersist()\n",
                "    \n",
                "    print(f\"Without caching: {no_cache:.3f}s\")\n",
                "    print(f\"With caching:    {with_cache:.3f}s\")\n",
                "    print(f\"Speedup: {no_cache/with_cache:.1f}x\")\n",
                "else:\n",
                "    print(\"Caching keeps data in memory between operations.\")\n",
                "    print(\"Use .cache() when you reuse a DataFrame multiple times.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Spark vs Pandas Comparison <a id='7-spark-vs-pandas'></a>\n",
                "\n",
                "### API Comparison\n",
                "\n",
                "| Operation | Pandas | PySpark |\n",
                "|-----------|--------|----------|\n",
                "| Read CSV | `pd.read_csv('f.csv')` | `spark.read.csv('f.csv')` |\n",
                "| Read Parquet | `pd.read_parquet('f.pq')` | `spark.read.parquet('f.pq')` |\n",
                "| Select cols | `df[['a', 'b']]` | `sdf.select('a', 'b')` |\n",
                "| Filter | `df[df.a > 5]` | `sdf.filter(F.col('a') > 5)` |\n",
                "| New column | `df['c'] = df.a * 2` | `sdf.withColumn('c', F.col('a') * 2)` |\n",
                "| GroupBy | `df.groupby('a').mean()` | `sdf.groupBy('a').agg(F.avg('b'))` |\n",
                "| Sort | `df.sort_values('a')` | `sdf.orderBy('a')` |\n",
                "| Join | `pd.merge(df1, df2, on='id')` | `sdf1.join(sdf2, 'id')` |\n",
                "| Count | `len(df)` | `sdf.count()` |\n",
                "\n",
                "### When to Use Which\n",
                "\n",
                "```\n",
                "  Data fits in RAM?          \u2500\u2500 YES \u2500\u25b6  Pandas (faster for small data)\n",
                "         \u2502\n",
                "        NO\n",
                "         \u2502\n",
                "  Need real-time processing? \u2500\u2500 YES \u2500\u25b6  Spark Streaming\n",
                "         \u2502\n",
                "        NO\n",
                "         \u2502\n",
                "  Batch processing at scale  \u2500\u2500\u2500\u2500\u2500\u25b6  Spark (batch)\n",
                "```\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Exercises <a id='8-exercises'></a>\n",
                "\n",
                "### Exercise 1: PySpark Feature Engineering\n",
                "Using PySpark, create a feature engineering pipeline that: handles nulls, creates a log-transformed price column, creates a price/quantity ratio, and saves as partitioned Parquet.\n",
                "\n",
                "### Exercise 2: Optimization\n",
                "Given a large DataFrame with 50 columns, you only need 5 for your ML model. Write code that reads efficiently (column pruning) and caches for repeated use.\n",
                "\n",
                "### Exercise 3: Broadcast Join\n",
                "Create a small lookup table (category \u2192 department mapping) and demonstrate a broadcast join with a large transactions DataFrame. Compare performance with a regular join.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Interview Preparation <a id='9-interview'></a>\n",
                "\n",
                "### Q1: \"Explain Spark\u2019s lazy evaluation. Why does it matter?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"Spark doesn't execute transformations immediately. Instead it builds a DAG (Directed Acyclic Graph) of operations. Execution only happens when an action like `.count()` or `.collect()` is called.\n",
                "\n",
                "This matters because Spark can **optimize the entire plan** before executing. For example, if I select 3 columns then filter rows, Spark pushes the filter down to read less data. Without lazy evaluation, it would read all data first, then filter \u2014 wasting I/O.\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q2: \"What\u2019s a shuffle in Spark? Why is it expensive?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"A shuffle is a redistribution of data across partitions. It happens during wide transformations like `groupBy`, `join`, and `repartition`.\n",
                "\n",
                "It\u2019s expensive because:\n",
                "1. Data must be **serialized**, sent **over the network**, and **deserialized**\n",
                "2. Intermediate data is written to **disk** for fault tolerance\n",
                "3. It creates a **stage boundary** in the DAG\n",
                "\n",
                "To minimize shuffles: use broadcast joins for small tables, pre-partition data by join keys, and avoid unnecessary `repartition()` calls.\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q3: \"How would you optimize a Spark job that\u2019s running slowly?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"My debugging checklist:\n",
                "1. **Check the Spark UI** for stage timelines, shuffle sizes, and skewed partitions\n",
                "2. **Data skew**: One partition much larger than others? Use salting or repartition\n",
                "3. **Shuffle reduction**: Can I broadcast smaller DataFrames in joins?\n",
                "4. **Partition count**: Too few = underutilized cores. Too many = overhead. Aim for 2-4 per core\n",
                "5. **Caching**: Am I re-computing the same DataFrame? Cache it\n",
                "6. **Predicate pushdown**: Am I filtering early? Read Parquet with column pruning\n",
                "7. **Serialization**: Use Kryo over Java serialization for speed\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q4: \"Explain narrow vs wide transformations.\"\n",
                "\n",
                "**Answer:**  \n",
                "\"**Narrow** transformations (filter, select, map) operate on data within the same partition \u2014 no data movement needed. They\u2019re fast and don\u2019t require a shuffle.\n",
                "\n",
                "**Wide** transformations (groupBy, join, distinct) require data to move between partitions (shuffle). They create stage boundaries and are the primary source of Spark performance issues.\n",
                "\n",
                "Rule of thumb: maximize narrow transformations, minimize wide ones. If you must shuffle, reduce data volume first (filter/select before groupBy).\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q5: \"When would you use Spark vs Pandas? What about Dask or Polars?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"**Pandas**: Data fits in RAM (<10GB). Fastest for small data, richest API.\n",
                "**Spark**: Data doesn't fit in RAM or needs distributed processing (>10GB). Enterprise standard, integrates with data lakes.\n",
                "**Dask**: Pandas-like API that scales out. Good bridge between Pandas and Spark \u2014 great for Pandas users who need to scale without learning Spark.\n",
                "**Polars**: Rust-based, very fast single-machine processing. Handles larger-than-RAM data via lazy evaluation and streaming. Growing fast.\n",
                "\n",
                "My rule: start with Pandas. When it\u2019s too slow, try Polars (same machine). When data is truly distributed, use Spark.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cleanup Spark session\n",
                "if SPARK_AVAILABLE:\n",
                "    spark.stop()\n",
                "    print(\"\u2705 Spark session stopped.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83c\udf93 Key Takeaways\n",
                "\n",
                "1. **Spark solves the scale problem** \u2014 when data doesn\u2019t fit in memory, distribute it\n",
                "2. **Driver + Executors** \u2014 understand the architecture for debugging\n",
                "3. **Lazy evaluation + DAG** \u2014 Spark optimizes your entire plan before executing\n",
                "4. **Minimize shuffles** \u2014 they\u2019re the #1 performance killer\n",
                "5. **Cache wisely** \u2014 for repeated access to the same data\n",
                "6. **Parquet + Spark** \u2014 the combination enables column pruning and predicate pushdown\n",
                "\n",
                "---\n",
                "\n",
                "\u27a1\ufe0f **Next Lesson**: [Lesson 7: Building End-to-End Data Pipelines](./lesson_07_end_to_end_pipeline.ipynb)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}