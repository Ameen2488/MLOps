{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 3: Sampling Strategies & Class Imbalance\n",
                "\n",
                "**Module 3: Data & Pipeline Engineering**  \n",
                "**Estimated Time**: 4-5 hours  \n",
                "**Difficulty**: Intermediate\n",
                "\n",
                "---\n",
                "\n",
                "## \ud83c\udfaf Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "\u2705 Understand why sampling matters at every stage of the ML lifecycle  \n",
                "\u2705 Know all major sampling techniques (non-probability and probability)  \n",
                "\u2705 Handle class imbalance using SMOTE, focal loss, and resampling  \n",
                "\u2705 Implement sampling techniques in Python  \n",
                "\u2705 Answer 5 interview questions on sampling and class imbalance  \n",
                "\n",
                "---\n",
                "\n",
                "## \ud83d\udcda Table of Contents\n",
                "\n",
                "1. [Why Sampling Matters](#1-why-sampling)\n",
                "2. [Non-Probability Sampling](#2-non-probability)\n",
                "3. [Probability Sampling](#3-probability)\n",
                "4. [Practical Implications](#4-implications)\n",
                "5. [Class Imbalance Handling](#5-class-imbalance)\n",
                "6. [Hands-On: Implementing Sampling Techniques](#6-hands-on)\n",
                "7. [Hands-On: SMOTE for Imbalanced Classification](#7-smote)\n",
                "8. [Exercises](#8-exercises)\n",
                "9. [Interview Preparation](#9-interview)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Why Sampling Matters <a id='1-why-sampling'></a>\n",
                "\n",
                "Sampling is the practice of **selecting a subset of data** from a larger pool. In ML, sampling occurs at **every stage** of the workflow:\n",
                "\n",
                "| Stage | Sampling Used | Example |\n",
                "|-------|--------------|----------|\n",
                "| **Data Collection** | Choosing what real-world data to collect | Selecting which user segments to survey |\n",
                "| **Labeling** | Selecting a subset for labeling | Choosing 10K images from 1M for annotation |\n",
                "| **Train/Val/Test Split** | Splitting data into subsets | 80/10/10 split with stratification |\n",
                "| **Training (SGD)** | Mini-batch selection | Sampling 32 records per batch |\n",
                "| **Monitoring** | Logging a fraction of predictions | Logging 1% of inference requests |\n",
                "\n",
                "### The Core Problem\n",
                "\n",
                "In many cases, we **cannot or do not use all available data**:\n",
                "- Data is **too large** (training on trillions of records isn\u2019t feasible)\n",
                "- Obtaining **labels is costly** (so we label a subset)\n",
                "- We intentionally **down-sample for quicker experimentation**\n",
                "\n",
                "**Good sampling** can make model development efficient and ensure the model generalizes.\n",
                "\n",
                "**Poor sampling** can mislead your results \u2014 for example, selecting an unrepresentative subset can cause your model to perform well on that subset but **fail in production**.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Non-Probability Sampling <a id='2-non-probability'></a>\n",
                "\n",
                "Not strictly based on random chance but uses some subjective or practical criteria to pick data.\n",
                "\n",
                "### 2.1 Convenience Sampling\n",
                "\n",
                "Select data that is **easiest to obtain**.\n",
                "\n",
                "**Example:** Using the first 10,000 records from a log because they\u2019re readily at hand.\n",
                "\n",
                "**Implications:**\n",
                "- \u26a0\ufe0f **High risk of bias** \u2014 sample may not represent the overall population\n",
                "- A model trained on data from a single city may not generalize to other regions\n",
                "- Popular because it\u2019s convenient, but can **skew results significantly**\n",
                "\n",
                "### 2.2 Snowball Sampling\n",
                "\n",
                "Use existing sample data to **recruit further data**. Often used in social networks or graphs.\n",
                "\n",
                "**Example:** You have data on some users, then include their friends, then friends-of-friends.\n",
                "\n",
                "**Implications:**\n",
                "- Helpful when you don\u2019t have direct access to data\n",
                "- \u26a0\ufe0f Can **over-represent tightly connected communities** and miss isolated samples\n",
                "\n",
                "### 2.3 Judgment (Purposive) Sampling\n",
                "\n",
                "Rely on **experts** to hand-pick what data to include.\n",
                "\n",
                "**Example:** A domain expert selects \"important\" fraud cases to train on.\n",
                "\n",
                "**Implications:**\n",
                "- Can incorporate valuable **domain knowledge**\n",
                "- \u26a0\ufe0f Subjective and can reflect **expert\u2019s biases**\n",
                "\n",
                "### 2.4 Quota Sampling\n",
                "\n",
                "Ensure certain **predefined quantities** of different sub-groups.\n",
                "\n",
                "**Example:** Include exactly 100 samples of each class in a classification problem.\n",
                "\n",
                "**Implications:**\n",
                "- \u2705 Guarantees representation of all groups\n",
                "- \u26a0\ufe0f Selection within each group might still be non-random (convenience-based)\n",
                "- Can introduce bias if the population within each quota is not homogeneous\n",
                "\n",
                "### When to Use Non-Probability Sampling\n",
                "\n",
                "Non-probability sampling is often a **starting point** (especially in early prototyping or when data access is limited). However, models built on non-random samples may not be reliable for production.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Probability (Random) Sampling <a id='3-probability'></a>\n",
                "\n",
                "Every data point has some **probability** of being selected, typically striving for an unbiased sample. Less prone to bias than non-probabilistic methods.\n",
                "\n",
                "### 3.1 Simple Random Sampling\n",
                "\n",
                "Each data point has an **equal chance** of being selected. Like shuffling your dataset and picking a subset.\n",
                "\n",
                "**When it works:** Homogeneous data with no important subgroups.\n",
                "\n",
                "**\u26a0\ufe0f Problem:** If there are rare but important subgroups, you might pick too few:\n",
                "\n",
                "> You have transactions with **2% fraud**. A simple random sample of 1,000 might contain ~20 fraud cases. Random fluctuation could give you 5 or 50, **skewing the fraud rate**.\n",
                "\n",
                "### 3.2 Weighted Sampling\n",
                "\n",
                "Each sample is given a **weight (probability)** for selection. This allows oversampling certain cases.\n",
                "\n",
                "```python\n",
                "import random\n",
                "# Weighted sampling in Python\n",
                "items = ['fraud', 'legitimate']\n",
                "weights = [0.5, 0.5]  # 50/50, even though fraud is only 2% of data\n",
                "sampled = random.choices(items, weights=weights, k=1000)\n",
                "```\n",
                "\n",
                "**Key distinction:**\n",
                "- **Weighted sampling** \u2192 Physically changes dataset composition (more minority samples)\n",
                "- **Sample weights in training** \u2192 Includes all data but gives more importance in the loss function\n",
                "\n",
                "Both aim to handle imbalance, but weighted sampling changes the dataset while sample weights change the optimization.\n",
                "\n",
                "### 3.3 Stratified Sampling\n",
                "\n",
                "Divide the population into **strata (groups)** and sample from each group separately to ensure representation.\n",
                "\n",
                "**Example:** For an imbalanced classification, stratify by class label so your train/test split has the **same class proportions** as the full dataset.\n",
                "\n",
                "```python\n",
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, stratify=y, random_state=42\n",
                ")\n",
                "```\n",
                "\n",
                "**\u2705 Advantage:** Greatly reduces variance between subgroup representation.\n",
                "\n",
                "**\u26a0\ufe0f Drawback:** Need to know the important grouping variables upfront.\n",
                "\n",
                "### 3.4 Reservoir Sampling\n",
                "\n",
                "An algorithm for sampling from **streaming data of unknown size**, ensuring each item has an equal probability.\n",
                "\n",
                "**Use case:** Maintain a random sample of fixed size from a continuous stream (e.g., user clicks) without storing it all.\n",
                "\n",
                "```python\n",
                "import random\n",
                "\n",
                "def reservoir_sampling(stream, k):\n",
                "    \"\"\"Maintain a random sample of size k from a stream.\"\"\"\n",
                "    reservoir = []\n",
                "    for i, item in enumerate(stream):\n",
                "        if i < k:\n",
                "            reservoir.append(item)\n",
                "        else:\n",
                "            # Replace element at random position with decreasing probability\n",
                "            j = random.randint(0, i)\n",
                "            if j < k:\n",
                "                reservoir[j] = item\n",
                "    return reservoir\n",
                "```\n",
                "\n",
                "This is crucial in **production streaming pipelines** where you can\u2019t store all data.\n",
                "\n",
                "### 3.5 Importance Sampling\n",
                "\n",
                "A more advanced technique used in **statistical estimation and reinforcement learning**.\n",
                "\n",
                "**Idea:** Sample from a different distribution than the target, and correct for the difference using importance weights.\n",
                "\n",
                "**Example in RL:** Sample episodes from a behavior policy but evaluate a target policy. Importance sampling provides a way to correct for the difference.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Practical Implications of Sampling Choices <a id='4-implications'></a>\n",
                "\n",
                "| Implication | Description |\n",
                "|------------|-------------|\n",
                "| **Bias** | Non-representative samples lead to biased models |\n",
                "| **Variance** | Small samples have high variance in estimates |\n",
                "| **Generalization** | Training sample must match production distribution |\n",
                "| **Evaluation** | Test set must be representative for valid metrics |\n",
                "| **Temporal** | For time-series, sample order matters (no future data in training!) |\n",
                "\n",
                "### Golden Rules\n",
                "\n",
                "1. **Always use stratified sampling** for classification train/test splits\n",
                "2. **Never shuffle time-series data** \u2014 use temporal splits\n",
                "3. **Document your sampling strategy** \u2014 it affects reproducibility\n",
                "4. **Monitor for sampling bias** \u2014 compare sample statistics to population\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Class Imbalance Handling <a id='5-class-imbalance'></a>\n",
                "\n",
                "Class imbalance is one of the most common challenges in real-world ML. Most production datasets are heavily imbalanced.\n",
                "\n",
                "### 5.1 The Problem\n",
                "\n",
                "| Domain | Positive Rate | Challenge |\n",
                "|--------|--------------|------------|\n",
                "| Fraud detection | 0.1% - 2% | Missing fraud costs millions |\n",
                "| Medical diagnosis | 1% - 5% | Missing disease can be fatal |\n",
                "| Click prediction | 1% - 5% | 95% of predictions are \"no click\" |\n",
                "| Spam detection | 10% - 20% | Relatively manageable |\n",
                "\n",
                "A naive model can achieve 99.9% accuracy on fraud data by **always predicting legitimate** \u2014 but catches zero fraud!\n",
                "\n",
                "### 5.2 Data-Level Approaches\n",
                "\n",
                "#### Oversampling (Minority Class)\n",
                "- **Random Oversampling**: Duplicate minority samples randomly\n",
                "- **SMOTE**: Generate synthetic minority samples by interpolating between existing ones\n",
                "- **ADASYN**: Like SMOTE but focuses on harder-to-learn samples\n",
                "\n",
                "#### Undersampling (Majority Class)\n",
                "- **Random Undersampling**: Remove majority samples randomly\n",
                "- **Tomek Links**: Remove majority samples that are close to minority samples\n",
                "- **NearMiss**: Select majority samples closest to minority samples\n",
                "\n",
                "#### Combined Approaches\n",
                "- **SMOTE + Tomek Links**: Oversample minority, then clean boundary\n",
                "- **SMOTE + Edited Nearest Neighbors (ENN)**: Oversample + clean noisy samples\n",
                "\n",
                "### 5.3 Algorithm-Level Approaches\n",
                "\n",
                "#### Class Weights\n",
                "Give more importance to minority class in the loss function:\n",
                "```python\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "model = RandomForestClassifier(class_weight='balanced')\n",
                "```\n",
                "\n",
                "#### Focal Loss\n",
                "A modified cross-entropy loss that **down-weights easy examples** and focuses on hard ones:\n",
                "```\n",
                "FL(p_t) = -\u03b1_t * (1 - p_t)^\u03b3 * log(p_t)\n",
                "```\n",
                "- When \u03b3 = 0, focal loss = standard cross-entropy\n",
                "- When \u03b3 > 0, easy examples are down-weighted\n",
                "- Originally proposed for object detection (RetinaNet)\n",
                "\n",
                "#### Ensemble Methods\n",
                "- **BalancedRandomForest**: Random forest with undersampling per tree\n",
                "- **EasyEnsemble**: Multiple undersampled subsets, ensemble their predictions\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Hands-On: Implementing Sampling Techniques <a id='6-hands-on'></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import random\n",
                "from collections import Counter\n",
                "\n",
                "np.random.seed(42)\n",
                "random.seed(42)\n",
                "\n",
                "# ============================================================\n",
                "# Create a synthetic population dataset\n",
                "# ============================================================\n",
                "n = 100000\n",
                "population = pd.DataFrame({\n",
                "    'user_id': range(n),\n",
                "    'region': np.random.choice(['US', 'EU', 'Asia', 'LatAm', 'Africa'], \n",
                "                               n, p=[0.3, 0.25, 0.25, 0.1, 0.1]),\n",
                "    'age': np.random.normal(35, 12, n).astype(int).clip(18, 80),\n",
                "    'purchase_amount': np.random.exponential(50, n).round(2),\n",
                "    'is_fraud': np.random.choice([0, 1], n, p=[0.98, 0.02])  # 2% fraud\n",
                "})\n",
                "\n",
                "print(f\"Population size: {len(population):,}\")\n",
                "print(f\"\\nRegion distribution:\")\n",
                "print(population['region'].value_counts(normalize=True).round(3))\n",
                "print(f\"\\nFraud rate: {population['is_fraud'].mean():.4f} ({population['is_fraud'].sum()} cases)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 1. Simple Random Sampling\n",
                "# ============================================================\n",
                "sample_size = 1000\n",
                "\n",
                "simple_sample = population.sample(n=sample_size, random_state=42)\n",
                "\n",
                "print(\"SIMPLE RANDOM SAMPLING\")\n",
                "print(f\"Sample size: {len(simple_sample)}\")\n",
                "print(f\"\\nRegion distribution (sample vs population):\")\n",
                "comparison = pd.DataFrame({\n",
                "    'Population': population['region'].value_counts(normalize=True).round(3),\n",
                "    'Sample': simple_sample['region'].value_counts(normalize=True).round(3)\n",
                "})\n",
                "print(comparison)\n",
                "print(f\"\\nFraud rate - Population: {population['is_fraud'].mean():.4f}\")\n",
                "print(f\"Fraud rate - Sample:     {simple_sample['is_fraud'].mean():.4f}\")\n",
                "print(f\"Fraud count in sample:   {simple_sample['is_fraud'].sum()} (expected ~{sample_size * 0.02:.0f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 2. Stratified Sampling\n",
                "# ============================================================\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Stratified by region\n",
                "stratified_sample = population.groupby('region', group_keys=False).apply(\n",
                "    lambda x: x.sample(n=min(200, len(x)), random_state=42)\n",
                ")\n",
                "\n",
                "print(\"STRATIFIED SAMPLING (by region)\")\n",
                "print(f\"Sample size: {len(stratified_sample)}\")\n",
                "print(f\"\\nRegion distribution (sample vs population):\")\n",
                "comparison = pd.DataFrame({\n",
                "    'Population': population['region'].value_counts(normalize=True).round(3),\n",
                "    'Stratified': stratified_sample['region'].value_counts(normalize=True).round(3)\n",
                "})\n",
                "print(comparison)\n",
                "print(\"\\n>>> Notice each region is now equally represented (200 each)\")\n",
                "\n",
                "# Stratified train/test split (preserving class proportions)\n",
                "print(\"\\n--- Stratified Train/Test Split (preserving fraud rate) ---\")\n",
                "X = population.drop('is_fraud', axis=1)\n",
                "y = population['is_fraud']\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, stratify=y, random_state=42\n",
                ")\n",
                "print(f\"Train fraud rate: {y_train.mean():.4f}\")\n",
                "print(f\"Test fraud rate:  {y_test.mean():.4f}\")\n",
                "print(f\"Population:       {y.mean():.4f}\")\n",
                "print(\">>> Proportions are preserved!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 3. Weighted Sampling\n",
                "# ============================================================\n",
                "\n",
                "# Give fraud cases 50x more weight\n",
                "weights = population['is_fraud'].map({0: 1, 1: 50})\n",
                "weighted_sample = population.sample(n=1000, weights=weights, random_state=42)\n",
                "\n",
                "print(\"WEIGHTED SAMPLING (fraud upweighted 50x)\")\n",
                "print(f\"Sample size: {len(weighted_sample)}\")\n",
                "print(f\"Fraud rate in sample: {weighted_sample['is_fraud'].mean():.4f}\")\n",
                "print(f\"Fraud cases in sample: {weighted_sample['is_fraud'].sum()}\")\n",
                "print(f\"\\n>>> Population fraud rate was 0.02, now sample has ~{weighted_sample['is_fraud'].mean():.2f}\")\n",
                "print(\">>> This enriches rare events for model training\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 4. Reservoir Sampling (streaming)\n",
                "# ============================================================\n",
                "\n",
                "def reservoir_sampling(stream, k):\n",
                "    \"\"\"Sample k items from a stream of unknown length.\n",
                "    Each item has equal probability of being in the final sample.\"\"\"\n",
                "    reservoir = []\n",
                "    for i, item in enumerate(stream):\n",
                "        if i < k:\n",
                "            reservoir.append(item)\n",
                "        else:\n",
                "            j = random.randint(0, i)\n",
                "            if j < k:\n",
                "                reservoir[j] = item\n",
                "    return reservoir\n",
                "\n",
                "# Simulate a data stream\n",
                "stream = list(range(100000))  # Simulated stream of 100K events\n",
                "sample = reservoir_sampling(stream, k=100)\n",
                "\n",
                "print(\"RESERVOIR SAMPLING\")\n",
                "print(f\"Stream size: {len(stream):,}\")\n",
                "print(f\"Reservoir size: {len(sample)}\")\n",
                "print(f\"Sample mean: {np.mean(sample):.0f} (expected ~{np.mean(stream):.0f})\")\n",
                "print(f\"Sample std:  {np.std(sample):.0f} (expected ~{np.std(stream):.0f})\")\n",
                "print(f\"\\n>>> Reservoir sampling works without knowing stream size in advance!\")\n",
                "print(\">>> Crucial for production streaming pipelines.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Hands-On: SMOTE for Imbalanced Classification <a id='7-smote'></a>\n",
                "\n",
                "SMOTE (Synthetic Minority Oversampling Technique) generates **new synthetic minority samples** by interpolating between existing minority examples."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install if needed: pip install imbalanced-learn\n",
                "# !pip install imbalanced-learn\n",
                "\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import classification_report, f1_score\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "\n",
                "# Prepare data\n",
                "df = population.copy()\n",
                "le = LabelEncoder()\n",
                "df['region_encoded'] = le.fit_transform(df['region'])\n",
                "\n",
                "features = ['age', 'purchase_amount', 'region_encoded']\n",
                "X = df[features]\n",
                "y = df['is_fraud']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, stratify=y, random_state=42\n",
                ")\n",
                "\n",
                "print(f\"Training set class distribution:\")\n",
                "print(f\"  Legitimate: {(y_train == 0).sum():,}\")\n",
                "print(f\"  Fraud:      {(y_train == 1).sum():,}\")\n",
                "print(f\"  Ratio:      {(y_train == 0).sum() / (y_train == 1).sum():.0f}:1\")\n",
                "\n",
                "# --- Baseline: No handling ---\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"BASELINE (no imbalance handling)\")\n",
                "print(\"=\"*60)\n",
                "rf_baseline = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "rf_baseline.fit(X_train, y_train)\n",
                "y_pred_baseline = rf_baseline.predict(X_test)\n",
                "print(classification_report(y_test, y_pred_baseline, target_names=['Legitimate', 'Fraud']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Method 1: Class Weights ---\n",
                "print(\"=\"*60)\n",
                "print(\"METHOD 1: Class Weights (class_weight='balanced')\")\n",
                "print(\"=\"*60)\n",
                "rf_weighted = RandomForestClassifier(\n",
                "    n_estimators=100, class_weight='balanced', random_state=42\n",
                ")\n",
                "rf_weighted.fit(X_train, y_train)\n",
                "y_pred_weighted = rf_weighted.predict(X_test)\n",
                "print(classification_report(y_test, y_pred_weighted, target_names=['Legitimate', 'Fraud']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Method 2: SMOTE ---\n",
                "try:\n",
                "    from imblearn.over_sampling import SMOTE\n",
                "    from imblearn.combine import SMOTETomek\n",
                "    \n",
                "    print(\"=\"*60)\n",
                "    print(\"METHOD 2: SMOTE (Synthetic Minority Oversampling)\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    smote = SMOTE(random_state=42)\n",
                "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
                "    \n",
                "    print(f\"Before SMOTE: {Counter(y_train)}\")\n",
                "    print(f\"After SMOTE:  {Counter(y_train_smote)}\")\n",
                "    \n",
                "    rf_smote = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "    rf_smote.fit(X_train_smote, y_train_smote)\n",
                "    y_pred_smote = rf_smote.predict(X_test)\n",
                "    print(classification_report(y_test, y_pred_smote, target_names=['Legitimate', 'Fraud']))\n",
                "    \n",
                "    # --- Method 3: SMOTE + Tomek Links ---\n",
                "    print(\"=\"*60)\n",
                "    print(\"METHOD 3: SMOTE + Tomek Links (Combined)\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    smt = SMOTETomek(random_state=42)\n",
                "    X_train_smt, y_train_smt = smt.fit_resample(X_train, y_train)\n",
                "    \n",
                "    print(f\"After SMOTE+Tomek: {Counter(y_train_smt)}\")\n",
                "    \n",
                "    rf_smt = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "    rf_smt.fit(X_train_smt, y_train_smt)\n",
                "    y_pred_smt = rf_smt.predict(X_test)\n",
                "    print(classification_report(y_test, y_pred_smt, target_names=['Legitimate', 'Fraud']))\n",
                "    \n",
                "except ImportError:\n",
                "    print(\"\\n\u26a0\ufe0f Install imbalanced-learn: pip install imbalanced-learn\")\n",
                "    print(\"Then re-run this cell.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Compare all methods\n",
                "# ============================================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"COMPARISON: Fraud Detection F1-Scores\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "results = {\n",
                "    'Baseline (no handling)': f1_score(y_test, y_pred_baseline, pos_label=1),\n",
                "    'Class Weights': f1_score(y_test, y_pred_weighted, pos_label=1),\n",
                "}\n",
                "\n",
                "try:\n",
                "    results['SMOTE'] = f1_score(y_test, y_pred_smote, pos_label=1)\n",
                "    results['SMOTE + Tomek'] = f1_score(y_test, y_pred_smt, pos_label=1)\n",
                "except:\n",
                "    pass\n",
                "\n",
                "for method, score in results.items():\n",
                "    bar = '\u2588' * int(score * 50)\n",
                "    print(f\"  {method:<25} F1={score:.4f} {bar}\")\n",
                "\n",
                "print(\"\\n>>> Class weights and SMOTE significantly improve minority class detection!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Exercises <a id='8-exercises'></a>\n",
                "\n",
                "### Exercise 1: Convenience Sampling Bias\n",
                "Take the first 1,000 records from the population (sorted by user_id). Compare the region distribution and fraud rate to the full population. Is it representative? What went wrong?\n",
                "\n",
                "### Exercise 2: Temporal Sampling\n",
                "Add a `timestamp` column to the population. Implement a temporal train/test split where the test set is the most recent 20% of data. Why is this better than random splitting for time-dependent data?\n",
                "\n",
                "### Exercise 3: Custom Focal Loss\n",
                "Implement a focal loss function in Python. Compare its behavior to standard cross-entropy loss for a highly imbalanced dataset.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Interview Preparation <a id='9-interview'></a>\n",
                "\n",
                "### Q1: \"You have a fraud detection dataset with 0.1% positives. How do you handle this?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"I\u2019d use a **multi-layered approach**:\n",
                "\n",
                "1. **Evaluation metrics**: Never use accuracy. Use **Precision, Recall, F1, PR-AUC** (Precision-Recall AUC is better than ROC-AUC for severe imbalance)\n",
                "2. **Stratified splitting**: Ensure train/test/validation all have the same 0.1% positive rate\n",
                "3. **Algorithm-level**: Use `class_weight='balanced'` or focal loss to penalize missing fraud more heavily\n",
                "4. **Data-level**: Apply **SMOTE** on training data only (never on test set!) to generate synthetic fraud examples\n",
                "5. **Ensemble approaches**: BalancedRandomForest or EasyEnsemble for robust predictions\n",
                "6. **Business context**: Set the decision threshold based on the cost ratio (cost of missing fraud vs cost of false alarm)\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q2: \"What is stratified sampling and when would you use it?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"Stratified sampling divides the population into **non-overlapping groups (strata)** and samples from each group proportionally.\n",
                "\n",
                "I use it whenever:\n",
                "- **Class imbalance exists** \u2014 ensures rare classes appear in both train and test sets\n",
                "- **Important subgroups** \u2014 e.g., ensuring each country/demographic is represented\n",
                "- **Train/test/val splits** \u2014 `sklearn.train_test_split(stratify=y)` preserves class proportions\n",
                "\n",
                "Without stratification, a random split on 0.1% positive data could put ALL positive cases in the training set, leaving the test set with zero positive examples \u2014 making evaluation meaningless.\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q3: \"Explain the difference between oversampling and class weights. Which do you prefer?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"**Oversampling** (like SMOTE) physically changes the dataset by adding synthetic minority samples. **Class weights** keep the original data but modify the loss function to penalize minority misclassification more.\n",
                "\n",
                "Trade-offs:\n",
                "- Oversampling **increases training time** (more data) but can capture complex decision boundaries\n",
                "- Class weights are **computationally cheaper** and don\u2019t risk overfitting to synthetic data\n",
                "- SMOTE can **create noisy samples** near the class boundary\n",
                "\n",
                "My preference:\n",
                "- Start with **class weights** (simple, fast, no data augmentation risk)\n",
                "- If that\u2019s insufficient, try **SMOTE + Tomek Links** (oversample + clean boundary)\n",
                "- Always validate on the **original, unmodified test set**\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q4: \"What is reservoir sampling and when is it used in production?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"Reservoir sampling is an **online algorithm** that maintains a random sample of fixed size `k` from a stream of unknown (potentially infinite) length, guaranteeing each item has equal probability `k/n` of being in the sample.\n",
                "\n",
                "Production use cases:\n",
                "1. **Monitoring**: Sampling 1% of model predictions for quality analysis without storing all predictions\n",
                "2. **Log analysis**: Maintaining a representative sample of server logs for debugging\n",
                "3. **A/B testing**: Randomly assigning streaming users to experiment groups\n",
                "4. **Data collection**: Building training datasets from live traffic\n",
                "\n",
                "The key advantage is **constant memory (O(k))** regardless of stream size.\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q5: \"SMOTE generated synthetic samples look good in training, but the model performs poorly in production. What went wrong?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"Several common issues:\n",
                "\n",
                "1. **SMOTE applied before train/test split** \u2014 synthetic samples leaked into the test set, giving overly optimistic evaluation\n",
                "2. **Distribution mismatch** \u2014 SMOTE creates samples that don\u2019t match the true production distribution. The model learns patterns that don\u2019t exist in real data.\n",
                "3. **Feature space not suitable** \u2014 SMOTE interpolates in feature space, which doesn\u2019t make sense for all feature types (categorical, high-dimensional)\n",
                "4. **Threshold not calibrated** \u2014 The decision threshold optimized on SMOTE-augmented data doesn\u2019t translate to production\n",
                "\n",
                "Prevention:\n",
                "- Always apply SMOTE **only on training data**, evaluate on the original test set\n",
                "- Use **probability calibration** after SMOTE\n",
                "- Consider **class weights** as a simpler, more robust alternative\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83c\udf93 Key Takeaways\n",
                "\n",
                "1. **Sampling occurs everywhere** in ML \u2014 collection, labeling, splitting, training, monitoring\n",
                "2. **Non-probability sampling** is quick but biased; use for prototyping only\n",
                "3. **Stratified sampling** should be default for any classification problem\n",
                "4. **Reservoir sampling** enables constant-memory sampling from streams\n",
                "5. **Class imbalance** requires multi-layered handling: metrics, algorithm, data, and business context\n",
                "6. **SMOTE** is powerful but must be applied only on training data\n",
                "\n",
                "---\n",
                "\n",
                "\u27a1\ufe0f **Next Lesson**: [Lesson 4: Data Leakage](./lesson_04_data_leakage.ipynb) \u2014 The most insidious ML bug."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}