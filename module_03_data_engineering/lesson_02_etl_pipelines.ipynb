{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lesson 2: ETL/ELT Pipelines — Theory and Practice\n",
                "\n",
                "**Module 3: Data & Pipeline Engineering**  \n",
                "**Estimated Time**: 4-5 hours  \n",
                "**Difficulty**: Intermediate\n",
                "\n",
                "---\n",
                "\n",
                "## \ud83c\udfaf Learning Objectives\n",
                "\n",
                "By the end of this lesson, you will:\n",
                "\n",
                "\u2705 Understand the ETL (Extract, Transform, Load) pipeline architecture  \n",
                "\u2705 Know when to choose ETL vs ELT and hybrid approaches  \n",
                "\u2705 Build a complete multi-source hybrid ETL/ELT pipeline from scratch  \n",
                "\u2705 Implement data validation, cleaning, and feature engineering in a pipeline  \n",
                "\u2705 Answer 5 interview questions on pipeline design  \n",
                "\n",
                "---\n",
                "\n",
                "## \ud83d\udcda Table of Contents\n",
                "\n",
                "1. [ETL Foundations](#1-etl-foundations)\n",
                "2. [The Extract Phase](#2-extract)\n",
                "3. [The Transform Phase](#3-transform)\n",
                "4. [The Load Phase](#4-load)\n",
                "5. [ETL vs ELT — When to Use Which](#5-etl-vs-elt)\n",
                "6. [Note on Streaming](#6-streaming)\n",
                "7. [Hands-On: Building a Hybrid ETL/ELT Pipeline](#7-hands-on)\n",
                "8. [Exercises](#8-exercises)\n",
                "9. [Interview Preparation](#9-interview)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ETL Foundations <a id='1-etl-foundations'></a>\n",
                "\n",
                "**ETL** stands for **Extract, Transform, Load**. It describes the pipeline of:\n",
                "1. **Getting data** from sources\n",
                "2. **Processing it** into a usable form\n",
                "3. **Loading it** into a storage system for downstream use\n",
                "\n",
                "ETL is often the **first stage** of preparing data for model training or inference.\n",
                "\n",
                "### Why ETL Matters in MLOps\n",
                "\n",
                "In traditional software engineering, the database schema is relatively fixed. In ML:\n",
                "- Data **changes over time** (distribution drift)\n",
                "- Features **evolve** as models improve\n",
                "- Multiple **data sources** must be joined and reconciled\n",
                "- **Reproducibility** requires deterministic pipelines\n",
                "- **Data quality** directly impacts model performance\n",
                "\n",
                "A well-designed ETL pipeline is the **backbone** of any production ML system.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. The Extract Phase <a id='2-extract'></a>\n",
                "\n",
                "Pull data from various sources (databases, APIs, files, logs, etc.).\n",
                "\n",
                "### 2.1 Common Data Sources for ML\n",
                "\n",
                "| Source | Protocol | Example |\n",
                "|--------|----------|----------|\n",
                "| Relational DB | SQL queries | PostgreSQL, MySQL |\n",
                "| NoSQL DB | API/SDK | MongoDB, DynamoDB |\n",
                "| Data Lake | File reads | S3 (Parquet), GCS |\n",
                "| REST APIs | HTTP GET/POST | Third-party data |\n",
                "| Message Queues | Consumer | Kafka, RabbitMQ |\n",
                "| File Systems | File I/O | CSV, JSON uploads |\n",
                "\n",
                "### 2.2 Validation During Extraction\n",
                "\n",
                "**Critical principle:** Reject or quarantine bad data **as early as possible** to save trouble downstream.\n",
                "\n",
                "```python\n",
                "# Example: Early validation during extraction\n",
                "def extract_with_validation(raw_records):\n",
                "    valid = []\n",
                "    quarantined = []\n",
                "    for record in raw_records:\n",
                "        if record.get('user_id') is None:\n",
                "            quarantined.append(record)  # Missing required field\n",
                "        elif not isinstance(record.get('amount'), (int, float)):\n",
                "            quarantined.append(record)  # Invalid type\n",
                "        else:\n",
                "            valid.append(record)\n",
                "    return valid, quarantined\n",
                "```\n",
                "\n",
                "**Best Practices:**\n",
                "- Check for **malformed records** (missing fields, wrong types)\n",
                "- Log or notify about quarantined data (don't silently drop!)\n",
                "- Track **extraction metrics** (rows extracted, rows rejected, latency)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. The Transform Phase <a id='3-transform'></a>\n",
                "\n",
                "This is the **core processing step** \u2014 the \"hefty\" part where most data wrangling happens.\n",
                "\n",
                "### 3.1 Common Transformations\n",
                "\n",
                "| Type | Description | Example |\n",
                "|------|-------------|----------|\n",
                "| **Data Cleaning** | Handle issues in raw data | Fill missing values, remove duplicates |\n",
                "| **Merging** | Join multiple data sources | Users + Orders + Products |\n",
                "| **Standardization** | Consistent formats | Date formats, currency, casing |\n",
                "| **Deduplication** | Remove duplicate records | Same order logged twice |\n",
                "| **Aggregation** | Summarize data | Daily averages, counts |\n",
                "| **Feature Engineering** | Create ML-ready features | `day-of-week`, `time-since-last-event` |\n",
                "| **Encoding** | Convert to numeric | One-hot encoding, label encoding |\n",
                "| **Normalization** | Scale numeric fields | StandardScaler, MinMaxScaler |\n",
                "\n",
                "### 3.2 Feature Engineering in ETL\n",
                "\n",
                "In ML terms, the Transform phase is where **raw data becomes features**:\n",
                "\n",
                "| Raw Data | Engineered Feature |\n",
                "|----------|-------------------|\n",
                "| `timestamp: 2024-01-15 14:30:00` | `day_of_week: 1 (Monday)` |\n",
                "| `last_purchase: 2023-12-20` | `days_since_last_purchase: 26` |\n",
                "| `category: 'electronics'` | One-hot: `[1, 0, 0, 0]` |\n",
                "| `price: 249.99` | `price_normalized: 0.72` |\n",
                "| 5 transactions in 1 hour | `transaction_velocity: 5` |\n",
                "\n",
                "### 3.3 Transform Best Practices\n",
                "\n",
                "1. **Idempotency** \u2014 Running the transform twice should produce the same result\n",
                "2. **Testability** \u2014 Each transform should be a testable function\n",
                "3. **Documentation** \u2014 Document what each transform does and why\n",
                "4. **Data validation** after transforms (not just before!)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. The Load Phase <a id='4-load'></a>\n",
                "\n",
                "Load the transformed data into a **target destination**.\n",
                "\n",
                "### 4.1 Common Destinations in ML\n",
                "\n",
                "| Destination | Use Case |\n",
                "|-------------|----------|\n",
                "| **Data Warehouse** (BigQuery, Redshift) | Analytical queries, dashboards |\n",
                "| **Data Lake** (S3, GCS as Parquet) | ML training data storage |\n",
                "| **Feature Store** (Feast) | Consistent training and serving features |\n",
                "| **Database** (PostgreSQL) | Structured, queryable results |\n",
                "| **File System** (local/cloud) | Training artifacts |\n",
                "\n",
                "### 4.2 Load Strategies\n",
                "\n",
                "| Strategy | Description | When to Use |\n",
                "|----------|-------------|-------------|\n",
                "| **Full Overwrite** | Replace all data | Small, easily reproduced datasets |\n",
                "| **Append** | Add new records | Time-series, log data |\n",
                "| **Upsert** | Insert or update | User profiles, slowly changing dimensions |\n",
                "| **Partition Overwrite** | Replace one partition | Daily/hourly partitioned data |\n",
                "\n",
                "### 4.3 Load Considerations\n",
                "\n",
                "- **How often?** Batch (daily, hourly) vs streaming (real-time)\n",
                "- **What format?** Parquet for analytics, databases for serving\n",
                "- **Partitioning?** By date is most common (`year/month/day`)\n",
                "\n",
                "**Example:** Load aggregated features daily into a warehouse table that the model training job reads:\n",
                "```\n",
                "s3://my-bucket/features/\n",
                "  \u251c\u2500\u2500 year=2024/\n",
                "  \u2502   \u251c\u2500\u2500 month=01/\n",
                "  \u2502   \u2502   \u251c\u2500\u2500 day=14/ features.parquet\n",
                "  \u2502   \u2502   \u2514\u2500\u2500 day=15/ features.parquet\n",
                "```\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. ETL vs ELT \u2014 When to Use Which <a id='5-etl-vs-elt'></a>\n",
                "\n",
                "### 5.1 ELT: Extract, Load, Transform\n",
                "\n",
                "**ELT** is a variant where raw data is **first loaded** into storage (often a data lake) **before transformation**.\n",
                "\n",
                "ELT has become popular with the rise of **inexpensive storage** and **scalable compute**. Organizations dump all raw data into a data lake immediately, and transform it later when needed.\n",
                "\n",
                "### 5.2 Comparison\n",
                "\n",
                "| Aspect | ETL | ELT |\n",
                "|--------|-----|-----|\n",
                "| **Transform timing** | Before loading | After loading |\n",
                "| **Storage** | Only cleaned data stored | Raw + cleaned data stored |\n",
                "| **Flexibility** | Less (schema decided upfront) | More (redefine transforms later) |\n",
                "| **Ingestion speed** | Slower (processing first) | Faster (dump and process later) |\n",
                "| **Risk** | Data loss (if you filter too aggressively) | Data swamp (too much unprocessed data) |\n",
                "| **Cost** | Less storage, more compute upfront | More storage, compute on demand |\n",
                "| **Best for** | Well-defined, stable pipelines | Exploratory, evolving ML pipelines |\n",
                "\n",
                "### 5.3 The Hybrid Approach (Most Common in ML)\n",
                "\n",
                "In practice, most ML teams use a **hybrid**:\n",
                "1. **Light ETL** on extraction \u2014 basic validation, remove garbage\n",
                "2. **Load** into a data lake (raw but validated)\n",
                "3. **Heavy transforms** (feature engineering) later, closer to model training\n",
                "\n",
                "### 5.4 Real-World Example: E-Commerce Recommendation\n",
                "\n",
                "**ETL approach:**\n",
                "An e-commerce company building a recommendation model:\n",
                "- **Extract:** Pull data from production databases (orders, user info, product catalog)\n",
                "- **Transform:** Join tables, clean inactive users, aggregate purchase history, encode categories\n",
                "- **Load:** Save feature table to data warehouse\n",
                "\n",
                "**ELT approach:**\n",
                "- Dump all raw logs and databases into a data lake\n",
                "- ML pipeline transforms raw data on the fly each time\n",
                "- More flexible, but possibly slower\n",
                "\n",
                "**Hybrid (recommended):**\n",
                "- Some ETL to create intermediate validated features\n",
                "- ELT of those into a warehouse\n",
                "- Further transformations close to training time\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Note on Streaming <a id='6-streaming'></a>\n",
                "\n",
                "ETL traditionally implies **batch processing** (periodic loads). If you have **streaming data** (real-time) feeding into an online model, similar principles apply, but with streaming tools:\n",
                "\n",
                "| ETL Stage | Batch Tool | Streaming Tool |\n",
                "|-----------|-----------|----------------|\n",
                "| Extract | SQL query, file read | **Kafka consumer** |\n",
                "| Transform | Pandas, Spark (batch) | **Spark Streaming, Flink** |\n",
                "| Load | Write to DB/file | **Write to Redis, Kafka topic** |\n",
                "\n",
                "The foundational idea of **\u201cget data \u2192 process \u2192 use data\u201d** remains, just with low latency.\n",
                "\n",
                "We\u2019ll touch on streaming in the context of feature stores (Lesson 5) and orchestration (Lesson 7).\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Hands-On: Building a Hybrid ETL/ELT Pipeline <a id='7-hands-on'></a>\n",
                "\n",
                "In this section, we\u2019ll simulate a basic ML data pipeline using Pandas, NumPy, and Scikit-learn.\n",
                "\n",
                "### Objectives:\n",
                "- Generate **synthetic data** to simulate data collection from multiple sources\n",
                "- Explore different file formats\n",
                "- Implement a **hybrid ETL/ELT approach** with practical data transformation workflows\n",
                "- Obtain final processed DataFrames ready for ML\n",
                "\n",
                "### Architecture Overview:\n",
                "```\n",
                "Source 1 (Users DB)  \u2500\u2500\u2500\u2500\u2510\n",
                "                          \u251c\u2500\u25b6 EXTRACT \u2500\u25b6 VALIDATE \u2500\u25b6 LOAD (raw) \u2500\u25b6 TRANSFORM \u2500\u25b6 LOAD (features)\n",
                "Source 2 (Events API) \u2500\u2500\u2500\u2518\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from datetime import datetime, timedelta\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "np.random.seed(42)\n",
                "\n",
                "# ============================================================\n",
                "# STEP 1: SIMULATE DATA SOURCES\n",
                "# Generating synthetic data to mimic real data sources\n",
                "# ============================================================\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"STEP 1: Simulating Data Sources\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# --- Source 1: Users Database (Internal DB) ---\n",
                "n_users = 10000\n",
                "users_df = pd.DataFrame({\n",
                "    'user_id': range(1, n_users + 1),\n",
                "    'name': [f'User_{i}' for i in range(1, n_users + 1)],\n",
                "    'email': [f'user_{i}@example.com' if np.random.random() > 0.05 \n",
                "              else None for i in range(1, n_users + 1)],  # 5% missing emails\n",
                "    'age': np.random.randint(18, 80, n_users),\n",
                "    'country': np.random.choice(['US', 'UK', 'DE', 'FR', 'JP', 'IN', None], \n",
                "                               n_users, p=[0.3, 0.15, 0.1, 0.1, 0.1, 0.15, 0.1]),\n",
                "    'signup_date': pd.date_range('2020-01-01', periods=n_users, freq='30min'),\n",
                "    'subscription_tier': np.random.choice(['free', 'basic', 'premium'], \n",
                "                                          n_users, p=[0.6, 0.25, 0.15])\n",
                "})\n",
                "\n",
                "# Inject some messy data (realistic!)\n",
                "users_df.loc[users_df.sample(50).index, 'age'] = -1  # Invalid ages\n",
                "users_df.loc[users_df.sample(30).index, 'age'] = 150  # Unrealistic ages\n",
                "\n",
                "print(f\"Source 1 (Users DB): {users_df.shape[0]} records, {users_df.shape[1]} columns\")\n",
                "print(f\"  Missing emails: {users_df['email'].isna().sum()}\")\n",
                "print(f\"  Missing countries: {users_df['country'].isna().sum()}\")\n",
                "print(f\"  Invalid ages: {((users_df['age'] < 0) | (users_df['age'] > 120)).sum()}\")\n",
                "\n",
                "# --- Source 2: Events/Transactions (API / Event Stream) ---\n",
                "n_events = 50000\n",
                "events_df = pd.DataFrame({\n",
                "    'event_id': range(1, n_events + 1),\n",
                "    'user_id': np.random.choice(range(1, n_users + 1), n_events),\n",
                "    'event_type': np.random.choice(['purchase', 'view', 'add_to_cart', 'search', 'review'], \n",
                "                                   n_events, p=[0.1, 0.4, 0.2, 0.2, 0.1]),\n",
                "    'product_category': np.random.choice(['electronics', 'clothing', 'food', 'books', 'sports'], \n",
                "                                         n_events),\n",
                "    'amount': np.where(\n",
                "        np.random.choice(['purchase', 'view', 'add_to_cart', 'search', 'review'], \n",
                "                         n_events, p=[0.1, 0.4, 0.2, 0.2, 0.1]) == 'purchase',\n",
                "        np.round(np.random.exponential(50, n_events), 2),\n",
                "        0.0\n",
                "    ),\n",
                "    'timestamp': pd.date_range('2024-01-01', periods=n_events, freq='1min')\n",
                "})\n",
                "\n",
                "# Inject duplicates (realistic in event streams!)\n",
                "duplicates = events_df.sample(200)\n",
                "events_df = pd.concat([events_df, duplicates], ignore_index=True)\n",
                "\n",
                "print(f\"\\nSource 2 (Events API): {events_df.shape[0]} records, {events_df.shape[1]} columns\")\n",
                "print(f\"  (includes {200} duplicate events)\")\n",
                "\n",
                "print(\"\\n\u2705 Data sources simulated successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 2: EXTRACT WITH VALIDATION\n",
                "# Apply early validation to reject/quarantine bad data\n",
                "# ============================================================\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"STEP 2: Extract with Validation\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "def validate_users(df):\n",
                "    \"\"\"Validate user records, quarantine invalid ones.\"\"\"\n",
                "    quarantine = []\n",
                "    \n",
                "    # Rule 1: age must be between 0 and 120\n",
                "    invalid_age = (df['age'] < 0) | (df['age'] > 120)\n",
                "    quarantine.append(('invalid_age', df[invalid_age]))\n",
                "    \n",
                "    # Rule 2: user_id must not be null\n",
                "    invalid_id = df['user_id'].isna()\n",
                "    quarantine.append(('missing_user_id', df[invalid_id]))\n",
                "    \n",
                "    # Remove quarantined records\n",
                "    valid_mask = ~invalid_age & ~invalid_id\n",
                "    valid_df = df[valid_mask].copy()\n",
                "    \n",
                "    return valid_df, quarantine\n",
                "\n",
                "def validate_events(df):\n",
                "    \"\"\"Validate event records, remove duplicates.\"\"\"\n",
                "    # Remove exact duplicates\n",
                "    before = len(df)\n",
                "    df = df.drop_duplicates(subset=['event_id'])\n",
                "    dupes_removed = before - len(df)\n",
                "    \n",
                "    # Validate: amount cannot be negative\n",
                "    invalid_amount = df['amount'] < 0\n",
                "    quarantine = [('negative_amount', df[invalid_amount])]\n",
                "    df = df[~invalid_amount].copy()\n",
                "    \n",
                "    return df, quarantine, dupes_removed\n",
                "\n",
                "# Apply validation\n",
                "users_valid, users_quarantine = validate_users(users_df)\n",
                "events_valid, events_quarantine, dupes = validate_events(events_df)\n",
                "\n",
                "print(f\"Users:  {len(users_df)} \u2192 {len(users_valid)} valid\")\n",
                "for reason, q_df in users_quarantine:\n",
                "    if len(q_df) > 0:\n",
                "        print(f\"  Quarantined ({reason}): {len(q_df)} records\")\n",
                "\n",
                "print(f\"\\nEvents: {len(events_df)} \u2192 {len(events_valid)} valid\")\n",
                "print(f\"  Duplicates removed: {dupes}\")\n",
                "for reason, q_df in events_quarantine:\n",
                "    if len(q_df) > 0:\n",
                "        print(f\"  Quarantined ({reason}): {len(q_df)} records\")\n",
                "\n",
                "print(\"\\n\u2705 Early validation complete! Bad data quarantined before proceeding.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 3: LOAD RAW (ELT style - save validated raw data)\n",
                "# This simulates loading into a data lake before heavy transforms\n",
                "# ============================================================\n",
                "import os\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"STEP 3: Load Raw Data (ELT-style)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Create a simulated \"data lake\" directory structure\n",
                "os.makedirs('data_lake/raw/users', exist_ok=True)\n",
                "os.makedirs('data_lake/raw/events', exist_ok=True)\n",
                "\n",
                "# Save as Parquet (industry standard for data lakes)\n",
                "users_valid.to_parquet('data_lake/raw/users/users_validated.parquet', index=False)\n",
                "events_valid.to_parquet('data_lake/raw/events/events_validated.parquet', index=False)\n",
                "\n",
                "# Check sizes\n",
                "users_size = os.path.getsize('data_lake/raw/users/users_validated.parquet') / 1024\n",
                "events_size = os.path.getsize('data_lake/raw/events/events_validated.parquet') / 1024\n",
                "\n",
                "print(f\"Saved to data lake:\")\n",
                "print(f\"  data_lake/raw/users/  - {users_size:.0f} KB\")\n",
                "print(f\"  data_lake/raw/events/ - {events_size:.0f} KB\")\n",
                "print(\"\\n\u2705 Raw validated data loaded to data lake!\")\n",
                "print(\"\\nThis is the ELT part: Load first, then Transform.\")\n",
                "print(\"Advantage: We can always go back to this raw data if we need different transforms.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 4: TRANSFORM (Feature Engineering)\n",
                "# This is the heavy-duty processing step\n",
                "# ============================================================\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"STEP 4: Transform (Feature Engineering)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Read from the \"data lake\" (simulating a fresh pipeline run)\n",
                "users = pd.read_parquet('data_lake/raw/users/users_validated.parquet')\n",
                "events = pd.read_parquet('data_lake/raw/events/events_validated.parquet')\n",
                "\n",
                "# --- Transform 1: Clean and standardize users ---\n",
                "print(\"\\nTransform 1: Cleaning users...\")\n",
                "\n",
                "# Fill missing countries with 'Unknown'\n",
                "users['country'] = users['country'].fillna('Unknown')\n",
                "\n",
                "# Fill missing emails with placeholder\n",
                "users['email'] = users['email'].fillna('no_email@unknown.com')\n",
                "\n",
                "# Create age bins (feature engineering)\n",
                "users['age_group'] = pd.cut(users['age'], \n",
                "                            bins=[0, 25, 35, 50, 65, 120], \n",
                "                            labels=['18-25', '26-35', '36-50', '51-65', '65+'])\n",
                "\n",
                "# Calculate account age\n",
                "users['account_age_days'] = (pd.Timestamp('2024-06-01') - users['signup_date']).dt.days\n",
                "\n",
                "print(f\"  Added: age_group, account_age_days\")\n",
                "print(f\"  Filled: {(users['country'] == 'Unknown').sum()} missing countries\")\n",
                "\n",
                "# --- Transform 2: Aggregate events per user ---\n",
                "print(\"\\nTransform 2: Aggregating events per user...\")\n",
                "\n",
                "user_event_features = events.groupby('user_id').agg(\n",
                "    total_events=('event_id', 'count'),\n",
                "    total_purchases=('event_type', lambda x: (x == 'purchase').sum()),\n",
                "    total_views=('event_type', lambda x: (x == 'view').sum()),\n",
                "    total_cart_adds=('event_type', lambda x: (x == 'add_to_cart').sum()),\n",
                "    total_spend=('amount', 'sum'),\n",
                "    avg_spend=('amount', 'mean'),\n",
                "    num_categories=('product_category', 'nunique'),\n",
                "    first_event=('timestamp', 'min'),\n",
                "    last_event=('timestamp', 'max')\n",
                ").reset_index()\n",
                "\n",
                "# Derived features\n",
                "user_event_features['purchase_rate'] = (\n",
                "    user_event_features['total_purchases'] / \n",
                "    user_event_features['total_events']\n",
                ").round(4)\n",
                "\n",
                "user_event_features['cart_to_purchase_ratio'] = np.where(\n",
                "    user_event_features['total_cart_adds'] > 0,\n",
                "    user_event_features['total_purchases'] / user_event_features['total_cart_adds'],\n",
                "    0\n",
                ").round(4)\n",
                "\n",
                "user_event_features['days_active'] = (\n",
                "    (user_event_features['last_event'] - user_event_features['first_event']).dt.days\n",
                ")\n",
                "\n",
                "print(f\"  Created {len(user_event_features.columns)} features per user\")\n",
                "print(f\"  Users with events: {len(user_event_features)}\")\n",
                "\n",
                "# --- Transform 3: Join users + event features ---\n",
                "print(\"\\nTransform 3: Joining user profiles + event features...\")\n",
                "\n",
                "final_features = users.merge(user_event_features, on='user_id', how='left')\n",
                "\n",
                "# Fill NaN for users with no events\n",
                "event_cols = user_event_features.columns.drop('user_id')\n",
                "for col in event_cols:\n",
                "    if final_features[col].dtype in ['float64', 'int64']:\n",
                "        final_features[col] = final_features[col].fillna(0)\n",
                "\n",
                "print(f\"  Final feature table: {final_features.shape}\")\n",
                "print(f\"  Users with events: {(final_features['total_events'] > 0).sum()}\")\n",
                "print(f\"  Users without events: {(final_features['total_events'] == 0).sum()}\")\n",
                "\n",
                "print(\"\\n\u2705 Transformation complete!\")\n",
                "final_features.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 5: LOAD FEATURES (Final load for ML consumption)\n",
                "# ============================================================\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"STEP 5: Load Features (ML-ready)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "os.makedirs('data_lake/features', exist_ok=True)\n",
                "\n",
                "# Save the feature table\n",
                "final_features.to_parquet('data_lake/features/user_features.parquet', index=False)\n",
                "features_size = os.path.getsize('data_lake/features/user_features.parquet') / 1024\n",
                "\n",
                "print(f\"\\nFeature table saved: {features_size:.0f} KB\")\n",
                "print(f\"Shape: {final_features.shape}\")\n",
                "print(f\"Columns: {list(final_features.columns)}\")\n",
                "\n",
                "# Pipeline summary\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"PIPELINE SUMMARY\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Input:     {len(users_df):>6} users + {len(events_df):>6} events\")\n",
                "print(f\"After validation: {len(users_valid):>6} users + {len(events_valid):>6} events\")\n",
                "print(f\"Output:    {final_features.shape[0]:>6} rows x {final_features.shape[1]} columns\")\n",
                "print(f\"\\nPipeline stages: EXTRACT \u2192 VALIDATE \u2192 LOAD(raw) \u2192 TRANSFORM \u2192 LOAD(features)\")\n",
                "print(\"\\n\u2705 Hybrid ETL/ELT pipeline complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 6: POST-LOAD VALIDATION\n",
                "# Always validate your output!\n",
                "# ============================================================\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"STEP 6: Post-Load Data Quality Checks\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Read back from storage to verify\n",
                "loaded_features = pd.read_parquet('data_lake/features/user_features.parquet')\n",
                "\n",
                "checks = [\n",
                "    ('No null user_ids', loaded_features['user_id'].isna().sum() == 0),\n",
                "    ('No negative ages', (loaded_features['age'] >= 0).all()),\n",
                "    ('Total spend >= 0', (loaded_features['total_spend'] >= 0).all()),\n",
                "    ('Purchase rate in [0,1]', loaded_features['purchase_rate'].between(0, 1).all()),\n",
                "    ('Expected row count', len(loaded_features) == len(users_valid)),\n",
                "    ('No duplicate user_ids', loaded_features['user_id'].nunique() == len(loaded_features)),\n",
                "]\n",
                "\n",
                "all_passed = True\n",
                "for check_name, result in checks:\n",
                "    status = '\u2705' if result else '\u274c'\n",
                "    print(f\"  {status} {check_name}\")\n",
                "    if not result:\n",
                "        all_passed = False\n",
                "\n",
                "print(f\"\\n{'All quality checks passed!' if all_passed else 'SOME CHECKS FAILED!'}\")\n",
                "\n",
                "# Cleanup simulated data lake\n",
                "import shutil\n",
                "shutil.rmtree('data_lake', ignore_errors=True)\n",
                "print(\"\\n(Cleaned up temporary files)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Exercises <a id='8-exercises'></a>\n",
                "\n",
                "### Exercise 1: Add a Third Data Source\n",
                "Add a simulated \"Product Catalog\" data source with columns: `product_id, name, category, price, stock_count`. Integrate it into the pipeline by joining products with events.\n",
                "\n",
                "### Exercise 2: Implement Idempotency\n",
                "Modify the pipeline so that running it twice produces the same result. Hint: use timestamps or checksums to detect already-processed data.\n",
                "\n",
                "### Exercise 3: Add Data Quality Metrics\n",
                "Create a `pipeline_metrics` dictionary that tracks: total rows in, rows quarantined, duplicates removed, rows out, execution time per stage. Print a summary report.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Interview Preparation <a id='9-interview'></a>\n",
                "\n",
                "### Q1: \"Design an ETL pipeline for processing daily user activity data. How do you handle late-arriving data?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"I\u2019d design a pipeline with these stages:\n",
                "\n",
                "1. **Extract**: Pull data from the activity log database daily at 2 AM UTC\n",
                "2. **Transform**: Clean, deduplicate, compute daily aggregates per user\n",
                "3. **Load**: Write to a partitioned Parquet table (partitioned by date)\n",
                "\n",
                "For **late-arriving data**, I\u2019d use a **lookback window** pattern:\n",
                "- Each run processes data from the target day + a configurable lookback (e.g., 3 days)\n",
                "- Use **partition overwrite** (not append) so reprocessing doesn\u2019t create duplicates\n",
                "- Track a `processing_timestamp` column separate from the event timestamp\n",
                "- Flag rows that arrived late for monitoring\n",
                "\n",
                "This makes the pipeline **idempotent** \u2014 running it multiple times produces the same result.\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q2: \"What\u2019s the difference between ETL and ELT? When would you choose each?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"In ETL, data is transformed before loading (clean \u2192 store). In ELT, raw data is loaded first, then transformed (store \u2192 clean).\n",
                "\n",
                "I choose **ETL** when:\n",
                "- Schema is well-defined and stable\n",
                "- Storage is expensive (only store what you need)\n",
                "- Compliance requires not storing raw PII\n",
                "\n",
                "I choose **ELT** when:\n",
                "- Exploration phase (don\u2019t know what transforms needed yet)\n",
                "- Storage is cheap (cloud data lakes)\n",
                "- Multiple teams need different views of the same data\n",
                "\n",
                "In practice, most ML teams use a **hybrid**: light validation during extraction, load raw to a data lake, then heavy feature engineering later. This preserves flexibility while maintaining data quality.\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q3: \"How do you ensure data quality in a production ETL pipeline?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"I implement data quality at every stage:\n",
                "\n",
                "1. **Extract**: Schema validation, null checks on required fields, type checking\n",
                "2. **Transform**: Business rule validation (e.g., amount > 0), statistical checks (e.g., row count within expected range)\n",
                "3. **Load**: Post-load verification (re-read and validate), row count comparison, checksum verification\n",
                "\n",
                "Specific tools and patterns:\n",
                "- **Great Expectations** for declarative data validation\n",
                "- **Quarantine table** for invalid records (don\u2019t silently drop!)\n",
                "- **Data quality dashboards** with alerts for anomalies\n",
                "- **Automated tests** that run before each pipeline stage proceeds\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q4: \"How do you make a data pipeline idempotent? Why is this important?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"An idempotent pipeline produces the same result regardless of how many times it runs.\n",
                "\n",
                "Implementation strategies:\n",
                "1. **Partition overwrite** instead of append (overwrite `date=2024-01-15` partition)\n",
                "2. **Deduplication** using natural keys (event_id, user_id + timestamp)\n",
                "3. **Deterministic transforms** \u2014 no `random()` without seeds, no `datetime.now()` in logic\n",
                "4. **Upsert** operations for dimension tables\n",
                "\n",
                "Why important:\n",
                "- Failed jobs can be safely **retried** without data corruption\n",
                "- **Backfill** operations (reprocessing historical data) just work\n",
                "- Debugging is easier when you can reproduce results\"\n",
                "\n",
                "---\n",
                "\n",
                "### Q5: \"How would you debug a failed ETL job in production?\"\n",
                "\n",
                "**Answer:**  \n",
                "\"My debugging workflow:\n",
                "\n",
                "1. **Check the logs** \u2014 identify which stage failed (extract, transform, or load)\n",
                "2. **Check input data** \u2014 has the source schema changed? Any corrupt records?\n",
                "3. **Check environment** \u2014 disk space, memory, network connectivity, credentials\n",
                "4. **Reproduce locally** \u2014 pull a sample of the failing data and run the pipeline locally\n",
                "5. **Root cause analysis** \u2014 is it a data issue, code bug, or infrastructure issue?\n",
                "6. **Fix and test** \u2014 add a test case for this exact failure mode\n",
                "7. **Backfill** \u2014 reprocess the failed data (idempotent pipeline makes this safe)\n",
                "\n",
                "Prevention:\n",
                "- **Structured logging** with correlation IDs\n",
                "- **Alerting** on pipeline failures with context (not just \u2018pipeline failed\u2019)\n",
                "- **Dead letter queues** for records that fail transformation\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## \ud83c\udf93 Key Takeaways\n",
                "\n",
                "1. **ETL is Extract \u2192 Transform \u2192 Load** \u2014 the backbone of ML data pipelines\n",
                "2. **Validate early** \u2014 reject bad data at extraction, not after expensive transforms\n",
                "3. **ETL vs ELT** \u2014 most ML teams use a hybrid approach\n",
                "4. **Idempotency** is non-negotiable for production pipelines\n",
                "5. **Data quality checks** should exist at every stage, not just at the end\n",
                "6. The **Transform phase** is where raw data becomes ML features\n",
                "\n",
                "---\n",
                "\n",
                "\u27a1\ufe0f **Next Lesson**: [Lesson 3: Sampling Strategies](./lesson_03_sampling_strategies.ipynb) \u2014 Learn how sampling choices affect your models."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}